2023-10-30 12:29:41,343 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpu0qcs47b
2023-10-30 12:29:41,343 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpu0qcs47b/_remote_module_non_scriptable.py
2023-10-30 12:29:41,771 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-30 12:29:41,826 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 12:29:43,249 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-30 12:29:43,488 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 12:29:43,488 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 12:29:43,488 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 12:29:43,488 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 12:29:44,234 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 12:29:44,311 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-30 12:29:44,311 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-30 12:29:44,351 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 12:29:44,434 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-30 12:29:44,438 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 1. 0.], size_todo: 86630400
2023-10-30 12:29:44,438 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 1. 0.], size_todo: 85056000
2023-10-30 12:29:44,439 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0. 1. 0.], size_todo: 85054464
2023-10-30 12:29:44,440 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0. 1. 0.], size_todo: 77966592
2023-10-30 12:29:44,441 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0. 1. 0.], size_todo: 70878720
2023-10-30 12:29:44,442 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0. 1. 0.], size_todo: 63790848
2023-10-30 12:29:44,443 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0. 1. 0.], size_todo: 56702976
2023-10-30 12:29:44,443 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0. 1. 0.], size_todo: 49615104
2023-10-30 12:29:44,444 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0. 1. 0.], size_todo: 42527232
2023-10-30 12:29:44,445 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0. 1. 0.], size_todo: 35439360
2023-10-30 12:29:44,446 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0. 1. 0.], size_todo: 28351488
2023-10-30 12:29:44,447 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0. 1. 0.], size_todo: 21263616
2023-10-30 12:29:44,448 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0. 1. 0.], size_todo: 14175744
2023-10-30 12:29:44,449 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0. 1. 0.], size_todo: 7087872
2023-10-30 12:29:44,450 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0. 1. 0.], size_todo: 0
2023-10-30 12:29:44,450 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0. 1. 0.], size_todo: 0
2023-10-30 12:29:44,450 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-30 12:29:44,452 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.23 GiB (100.00%), Disk Mem 0.00 Gib (0.00%)
2023-10-30 12:29:44,497 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 12:29:44,668 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-30 12:29:44,668 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-30 12:29:44,668 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-30 12:29:44,668 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-30 12:29:44,668 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-30 12:29:44,669 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-30 12:29:44,670 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-30 12:29:44,670 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-30 12:29:44,670 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-30 12:29:44,673 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:44,674 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-30 12:29:44,676 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:44,676 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-30 12:29:44,676 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:44,690 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-30 12:29:44,693 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:44,699 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-30 12:29:44,701 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:44,708 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-30 12:29:44,711 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:44,717 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-30 12:29:44,720 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:44,725 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-30 12:29:44,728 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:44,734 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-30 12:29:44,738 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:44,744 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-30 12:29:44,747 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:44,753 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-30 12:29:44,756 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:44,762 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-30 12:29:44,764 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:44,769 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-30 12:29:44,772 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:44,778 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-30 12:29:44,781 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:44,788 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-30 12:29:44,790 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:44,791 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-30 12:29:44,792 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:44,800 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-30 12:29:44,807 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-30 12:29:44,808 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-30 12:29:44,809 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-30 12:29:44,817 [model.py:410 in init_all_weights] DEBUG - init all weights...
2023-10-30 12:29:44,857 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-30 12:29:44,857 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-30 12:29:44,857 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-30 12:29:44,858 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-30 12:29:44,858 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-30 12:29:44,858 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-30 12:29:44,858 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-30 12:29:44,859 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-30 12:29:44,859 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-30 12:29:44,859 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-30 12:29:44,859 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-30 12:29:44,860 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-30 12:29:44,860 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-30 12:29:44,860 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-30 12:29:44,860 [block.py:237 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-30 12:29:44,860 [block.py:237 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-30 12:29:44,903 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 12:29:45,041 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:45,041 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,042 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,042 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-30 12:29:45,042 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,043 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:45,043 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs_k: {}
2023-10-30 12:29:45,043 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c370>
2023-10-30 12:29:45,043 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:45,043 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs_k: {}
2023-10-30 12:29:45,044 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c4f0>
2023-10-30 12:29:45,044 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:45,044 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs_k: {}
2023-10-30 12:29:45,044 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c5b0>
2023-10-30 12:29:45,044 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:45,045 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs_k: {}
2023-10-30 12:29:45,045 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c610>
2023-10-30 12:29:45,045 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 9, 768])
2023-10-30 12:29:45,046 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:45,046 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,049 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,050 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-30 12:29:45,050 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,050 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:45,050 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs_k: {}
2023-10-30 12:29:45,050 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:45,051 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:45,051 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs_k: {}
2023-10-30 12:29:45,051 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c700>
2023-10-30 12:29:45,051 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:45,051 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs_k: {}
2023-10-30 12:29:45,052 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c640>
2023-10-30 12:29:45,052 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:45,052 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs_k: {}
2023-10-30 12:29:45,052 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c2e0>
2023-10-30 12:29:45,052 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 9, 768])
2023-10-30 12:29:45,053 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:45,053 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,056 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,059 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,060 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,060 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:45,060 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,075 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fac0>, (<mixtensor.MixTensor object at 0x7f0f8572fb80>, <mixtensor.MixTensor object at 0x7f0f8572f5e0>))
2023-10-30 12:29:45,076 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:45,076 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,078 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f5b0>, (<mixtensor.MixTensor object at 0x7f0f8572f610>, <mixtensor.MixTensor object at 0x7f0f8572fa90>))
2023-10-30 12:29:45,078 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:45,079 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,081 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ff40>, (<mixtensor.MixTensor object at 0x7f0f8572ff70>, <mixtensor.MixTensor object at 0x7f0f8572fc10>))
2023-10-30 12:29:45,081 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:45,082 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,084 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa60>, (<mixtensor.MixTensor object at 0x7f0f8572fc40>, <mixtensor.MixTensor object at 0x7f0f8572fc70>))
2023-10-30 12:29:45,084 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,084 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:45,084 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,088 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,091 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,091 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,091 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:45,091 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,096 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c7c0>, (<mixtensor.MixTensor object at 0x7f0f84d4c760>, <mixtensor.MixTensor object at 0x7f0f84d4c610>))
2023-10-30 12:29:45,096 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:45,097 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,099 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c820>, (<mixtensor.MixTensor object at 0x7f0f84d4c850>, <mixtensor.MixTensor object at 0x7f0f84d4c880>))
2023-10-30 12:29:45,099 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:45,099 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,101 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c940>, (<mixtensor.MixTensor object at 0x7f0f84d4c970>, <mixtensor.MixTensor object at 0x7f0f84d4c9a0>))
2023-10-30 12:29:45,101 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:45,101 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,103 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c910>, (<mixtensor.MixTensor object at 0x7f0f84d4c9d0>, <mixtensor.MixTensor object at 0x7f0f84d4ca00>))
2023-10-30 12:29:45,104 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,104 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:45,104 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,107 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,110 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,111 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:45,111 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,115 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cc10>, (<mixtensor.MixTensor object at 0x7f0f84d4cc40>, <mixtensor.MixTensor object at 0x7f0f84d4cc70>))
2023-10-30 12:29:45,115 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:45,115 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,118 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, <mixtensor.MixTensor object at 0x7f0f84d4cd90>))
2023-10-30 12:29:45,118 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:45,118 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,121 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, <mixtensor.MixTensor object at 0x7f0f84d4ceb0>))
2023-10-30 12:29:45,121 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:45,121 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,123 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce20>, (<mixtensor.MixTensor object at 0x7f0f84d4cee0>, <mixtensor.MixTensor object at 0x7f0f84d4cf10>))
2023-10-30 12:29:45,123 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,123 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:45,124 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,127 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,130 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,130 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,130 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:45,130 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,133 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d120>, (<mixtensor.MixTensor object at 0x7f0f84d4d150>, <mixtensor.MixTensor object at 0x7f0f84d4d180>))
2023-10-30 12:29:45,134 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:45,134 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,136 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d240>, (<mixtensor.MixTensor object at 0x7f0f84d4d270>, <mixtensor.MixTensor object at 0x7f0f84d4d2a0>))
2023-10-30 12:29:45,136 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:45,136 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,138 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d360>, (<mixtensor.MixTensor object at 0x7f0f84d4d390>, <mixtensor.MixTensor object at 0x7f0f84d4d3c0>))
2023-10-30 12:29:45,138 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:45,139 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,143 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d330>, (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, <mixtensor.MixTensor object at 0x7f0f84d4d420>))
2023-10-30 12:29:45,143 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,143 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:45,144 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,147 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,150 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,150 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,150 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:45,150 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,153 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d630>, (<mixtensor.MixTensor object at 0x7f0f84d4d660>, <mixtensor.MixTensor object at 0x7f0f84d4d690>))
2023-10-30 12:29:45,154 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:45,154 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,156 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d750>, (<mixtensor.MixTensor object at 0x7f0f84d4d780>, <mixtensor.MixTensor object at 0x7f0f84d4d7b0>))
2023-10-30 12:29:45,156 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:45,157 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,159 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d870>, (<mixtensor.MixTensor object at 0x7f0f84d4d8a0>, <mixtensor.MixTensor object at 0x7f0f84d4d8d0>))
2023-10-30 12:29:45,159 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:45,159 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,163 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d840>, (<mixtensor.MixTensor object at 0x7f0f84d4d900>, <mixtensor.MixTensor object at 0x7f0f84d4d930>))
2023-10-30 12:29:45,164 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,164 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:45,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,167 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,171 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,171 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,171 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:45,171 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4db40>, (<mixtensor.MixTensor object at 0x7f0f84d4db70>, <mixtensor.MixTensor object at 0x7f0f84d4dba0>))
2023-10-30 12:29:45,175 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:45,175 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,178 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dc60>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4dcc0>))
2023-10-30 12:29:45,178 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:45,178 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,180 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dd80>, (<mixtensor.MixTensor object at 0x7f0f84d4ddb0>, <mixtensor.MixTensor object at 0x7f0f84d4dde0>))
2023-10-30 12:29:45,180 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:45,181 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,183 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dd50>, (<mixtensor.MixTensor object at 0x7f0f84d4de10>, <mixtensor.MixTensor object at 0x7f0f84d4de40>))
2023-10-30 12:29:45,183 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,183 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:45,184 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,187 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,190 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,190 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,191 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:45,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,194 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d150>, (<mixtensor.MixTensor object at 0x7f0f8572fac0>, <mixtensor.MixTensor object at 0x7f0f8572fee0>))
2023-10-30 12:29:45,194 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:45,195 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,197 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4e0b0>, <mixtensor.MixTensor object at 0x7f0f84d4e0e0>))
2023-10-30 12:29:45,197 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:45,197 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,200 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e1d0>, <mixtensor.MixTensor object at 0x7f0f84d4e200>))
2023-10-30 12:29:45,200 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:45,200 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,202 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e170>, (<mixtensor.MixTensor object at 0x7f0f84d4e230>, <mixtensor.MixTensor object at 0x7f0f84d4e260>))
2023-10-30 12:29:45,202 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,203 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:45,203 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,206 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,209 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,209 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,210 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:45,210 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,214 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e470>, (<mixtensor.MixTensor object at 0x7f0f84d4e4a0>, <mixtensor.MixTensor object at 0x7f0f84d4e4d0>))
2023-10-30 12:29:45,214 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:45,214 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,216 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e590>, (<mixtensor.MixTensor object at 0x7f0f84d4e5c0>, <mixtensor.MixTensor object at 0x7f0f84d4e5f0>))
2023-10-30 12:29:45,216 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:45,217 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,219 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e6b0>, (<mixtensor.MixTensor object at 0x7f0f84d4e6e0>, <mixtensor.MixTensor object at 0x7f0f84d4e710>))
2023-10-30 12:29:45,219 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:45,219 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,222 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e680>, (<mixtensor.MixTensor object at 0x7f0f84d4e740>, <mixtensor.MixTensor object at 0x7f0f84d4e770>))
2023-10-30 12:29:45,222 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,222 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:45,222 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,225 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,229 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,229 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,229 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:45,229 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,233 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e980>, (<mixtensor.MixTensor object at 0x7f0f84d4e9b0>, <mixtensor.MixTensor object at 0x7f0f84d4e9e0>))
2023-10-30 12:29:45,233 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:45,234 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,236 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eaa0>, (<mixtensor.MixTensor object at 0x7f0f84d4ead0>, <mixtensor.MixTensor object at 0x7f0f84d4eb00>))
2023-10-30 12:29:45,236 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:45,236 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,238 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ebc0>, (<mixtensor.MixTensor object at 0x7f0f84d4ebf0>, <mixtensor.MixTensor object at 0x7f0f84d4ec20>))
2023-10-30 12:29:45,238 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:45,239 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,242 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb90>, (<mixtensor.MixTensor object at 0x7f0f84d4ec50>, <mixtensor.MixTensor object at 0x7f0f84d4ec80>))
2023-10-30 12:29:45,242 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,242 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:45,242 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,246 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,248 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,249 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:45,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,253 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, (<mixtensor.MixTensor object at 0x7f0f84d4eec0>, <mixtensor.MixTensor object at 0x7f0f84d4eef0>))
2023-10-30 12:29:45,253 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:45,253 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,256 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efb0>, (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, <mixtensor.MixTensor object at 0x7f0f84d4f010>))
2023-10-30 12:29:45,256 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:45,256 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,258 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f100>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:45,258 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:45,258 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,261 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f160>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:45,261 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,261 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:45,262 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,265 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,268 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,268 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,268 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:45,269 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,272 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f3a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f3d0>, <mixtensor.MixTensor object at 0x7f0f84d4f400>))
2023-10-30 12:29:45,272 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:45,273 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,275 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f4f0>, <mixtensor.MixTensor object at 0x7f0f84d4f520>))
2023-10-30 12:29:45,275 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:45,275 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,278 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f610>, <mixtensor.MixTensor object at 0x7f0f84d4f640>))
2023-10-30 12:29:45,278 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:45,278 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,281 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4f670>, <mixtensor.MixTensor object at 0x7f0f84d4f6a0>))
2023-10-30 12:29:45,281 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,281 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:45,282 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,282 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,286 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,286 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,286 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:45,286 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,291 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f400>, (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, <mixtensor.MixTensor object at 0x7f0f84d4f700>))
2023-10-30 12:29:45,291 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:45,292 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,294 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f760>, (<mixtensor.MixTensor object at 0x7f0f84d4f790>, <mixtensor.MixTensor object at 0x7f0f84d4f880>))
2023-10-30 12:29:45,294 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:45,294 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,297 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:45,297 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:45,297 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,299 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f430>, (<mixtensor.MixTensor object at 0x7f0f84d4f3a0>, <mixtensor.MixTensor object at 0x7f0f84d4f730>))
2023-10-30 12:29:45,300 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 12:29:45,300 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:45,300 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,301 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,301 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,301 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:45,302 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,302 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e980>
2023-10-30 12:29:45,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:45,302 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,303 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f0d0>
2023-10-30 12:29:45,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:45,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,304 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f400>
2023-10-30 12:29:45,304 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:45,304 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,305 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c070>
2023-10-30 12:29:45,305 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 9, 768])
2023-10-30 12:29:45,305 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:45,306 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,306 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,307 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 9, 768])",)
2023-10-30 12:29:45,307 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,307 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:45,308 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,317 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f430>
2023-10-30 12:29:45,318 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:45,318 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,328 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f040>
2023-10-30 12:29:45,328 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:45,328 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,339 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c2e0>
2023-10-30 12:29:45,340 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:45,340 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs_k: {}
2023-10-30 12:29:45,348 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c1f0>
2023-10-30 12:29:45,351 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-30 12:29:45,352 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:45,352 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,353 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,354 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:45,354 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,354 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:45,355 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,355 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c370>
2023-10-30 12:29:45,355 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:45,355 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,357 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f760>
2023-10-30 12:29:45,357 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:45,357 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,358 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c1c0>
2023-10-30 12:29:45,358 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:45,358 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,358 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c0d0>
2023-10-30 12:29:45,358 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,359 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:45,359 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,363 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,363 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-30 12:29:45,363 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,364 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:45,364 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs_k: {}
2023-10-30 12:29:45,364 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4efb0>
2023-10-30 12:29:45,364 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:45,364 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs_k: {}
2023-10-30 12:29:45,365 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f580>
2023-10-30 12:29:45,365 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:45,365 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs_k: {}
2023-10-30 12:29:45,366 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:45,366 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:45,366 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs_k: {}
2023-10-30 12:29:45,366 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f8b0>
2023-10-30 12:29:45,366 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,367 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:45,367 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,371 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,374 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,374 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,374 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:45,375 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,382 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, <mixtensor.MixTensor object at 0x7f0f84d4f700>))
2023-10-30 12:29:45,383 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:45,383 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,384 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4fa00>, <mixtensor.MixTensor object at 0x7f0f84d4faf0>))
2023-10-30 12:29:45,384 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:45,385 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,386 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb20>, (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, <mixtensor.MixTensor object at 0x7f0f84d4fb80>))
2023-10-30 12:29:45,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:45,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,388 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f490>, (<mixtensor.MixTensor object at 0x7f0f84d4f970>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:45,388 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,388 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:45,388 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,396 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,396 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,396 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:45,396 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,399 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, (<mixtensor.MixTensor object at 0x7f0f84d4fd90>, <mixtensor.MixTensor object at 0x7f0f84d4fdc0>))
2023-10-30 12:29:45,399 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:45,399 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,401 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, (<mixtensor.MixTensor object at 0x7f0f84d4fe80>, <mixtensor.MixTensor object at 0x7f0f84d4feb0>))
2023-10-30 12:29:45,401 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:45,401 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,403 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fee0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, <mixtensor.MixTensor object at 0x7f0f84d4ff40>))
2023-10-30 12:29:45,403 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:45,403 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,404 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f280>, (<mixtensor.MixTensor object at 0x7f0f84d4fdf0>, <mixtensor.MixTensor object at 0x7f0f84d4fe20>))
2023-10-30 12:29:45,404 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,405 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:45,405 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,409 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,412 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,412 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,412 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:45,413 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,416 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4fd90>, <mixtensor.MixTensor object at 0x7f0f84d4fdc0>))
2023-10-30 12:29:45,416 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:45,416 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,418 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd80>, (<mixtensor.MixTensor object at 0x7f0f8572c1f0>, <mixtensor.MixTensor object at 0x7f0f8572dd20>))
2023-10-30 12:29:45,418 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:45,418 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,419 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, (<mixtensor.MixTensor object at 0x7f0f8572dd50>, <mixtensor.MixTensor object at 0x7f0f8572c0a0>))
2023-10-30 12:29:45,420 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:45,420 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,421 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572da50>, (<mixtensor.MixTensor object at 0x7f0f8572c070>, <mixtensor.MixTensor object at 0x7f0f8572d750>))
2023-10-30 12:29:45,421 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,422 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:45,422 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,426 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,429 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,429 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,429 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:45,430 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,432 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f700>, (<mixtensor.MixTensor object at 0x7f0f84d4fb20>, <mixtensor.MixTensor object at 0x7f0f84d4eec0>))
2023-10-30 12:29:45,432 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:45,433 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,434 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd90>, (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, <mixtensor.MixTensor object at 0x7f0f84d4eef0>))
2023-10-30 12:29:45,434 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:45,434 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,436 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc10>, (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, <mixtensor.MixTensor object at 0x7f0f84d4f0a0>))
2023-10-30 12:29:45,436 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:45,436 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,437 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fee0>, (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, <mixtensor.MixTensor object at 0x7f0f84d4fc70>))
2023-10-30 12:29:45,438 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,438 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:45,438 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,442 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,445 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,445 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,446 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:45,446 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,449 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ef50>, (<mixtensor.MixTensor object at 0x7f0f84d4ea40>, <mixtensor.MixTensor object at 0x7f0f84d4e530>))
2023-10-30 12:29:45,449 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:45,449 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,450 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44070>, (<mixtensor.MixTensor object at 0x7f0f84d44160>, <mixtensor.MixTensor object at 0x7f0f84d44190>))
2023-10-30 12:29:45,450 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:45,451 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,452 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d441f0>, <mixtensor.MixTensor object at 0x7f0f84d44220>))
2023-10-30 12:29:45,452 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:45,452 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,454 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb20>, (<mixtensor.MixTensor object at 0x7f0f84d4f460>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:45,454 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,454 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:45,454 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,458 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,461 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,461 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,462 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:45,462 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,464 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ea40>, (<mixtensor.MixTensor object at 0x7f0f84d4e530>, <mixtensor.MixTensor object at 0x7f0f84d44280>))
2023-10-30 12:29:45,464 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:45,464 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,466 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d442e0>, (<mixtensor.MixTensor object at 0x7f0f84d44310>, <mixtensor.MixTensor object at 0x7f0f84d44400>))
2023-10-30 12:29:45,466 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:45,466 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,468 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44430>, (<mixtensor.MixTensor object at 0x7f0f84d44460>, <mixtensor.MixTensor object at 0x7f0f84d44490>))
2023-10-30 12:29:45,468 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:45,468 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,469 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cf40>, (<mixtensor.MixTensor object at 0x7f0f84d44280>, <mixtensor.MixTensor object at 0x7f0f84d442b0>))
2023-10-30 12:29:45,469 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,470 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:45,470 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,474 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,479 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,480 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,481 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:45,481 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,508 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f84d44520>))
2023-10-30 12:29:45,509 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:45,509 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,511 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44580>, (<mixtensor.MixTensor object at 0x7f0f84d44670>, <mixtensor.MixTensor object at 0x7f0f84d446a0>))
2023-10-30 12:29:45,511 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:45,512 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,513 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446d0>, (<mixtensor.MixTensor object at 0x7f0f84d44700>, <mixtensor.MixTensor object at 0x7f0f84d44730>))
2023-10-30 12:29:45,513 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:45,514 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,516 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44250>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d44550>))
2023-10-30 12:29:45,516 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,517 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:45,517 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,524 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,525 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,525 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:45,525 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,528 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f84d44790>))
2023-10-30 12:29:45,528 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:45,529 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,531 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447f0>, (<mixtensor.MixTensor object at 0x7f0f84d44820>, <mixtensor.MixTensor object at 0x7f0f84d44910>))
2023-10-30 12:29:45,531 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:45,531 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,533 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44940>, (<mixtensor.MixTensor object at 0x7f0f84d44970>, <mixtensor.MixTensor object at 0x7f0f84d449a0>))
2023-10-30 12:29:45,533 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:45,533 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,535 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d442e0>, (<mixtensor.MixTensor object at 0x7f0f84d44430>, <mixtensor.MixTensor object at 0x7f0f84d447c0>))
2023-10-30 12:29:45,535 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,535 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:45,536 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,540 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,543 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,543 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,543 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:45,544 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,546 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f84d44a00>))
2023-10-30 12:29:45,547 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:45,547 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,548 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a60>, (<mixtensor.MixTensor object at 0x7f0f84d44a90>, <mixtensor.MixTensor object at 0x7f0f84d44b80>))
2023-10-30 12:29:45,549 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:45,549 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,550 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d44c10>))
2023-10-30 12:29:45,550 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:45,550 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,552 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44250>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44a30>))
2023-10-30 12:29:45,552 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,552 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:45,553 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,556 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,559 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,560 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,560 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:45,560 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,562 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f84d44c70>))
2023-10-30 12:29:45,563 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:45,563 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,566 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44cd0>, (<mixtensor.MixTensor object at 0x7f0f84d44d00>, <mixtensor.MixTensor object at 0x7f0f84d44df0>))
2023-10-30 12:29:45,566 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:45,566 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,567 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44e20>, (<mixtensor.MixTensor object at 0x7f0f84d44e50>, <mixtensor.MixTensor object at 0x7f0f84d44e80>))
2023-10-30 12:29:45,567 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:45,568 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,569 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44520>, (<mixtensor.MixTensor object at 0x7f0f84d446d0>, <mixtensor.MixTensor object at 0x7f0f84d44ca0>))
2023-10-30 12:29:45,569 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,569 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:45,570 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,574 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,577 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,577 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,577 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:45,577 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,580 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d444f0>, (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, <mixtensor.MixTensor object at 0x7f0f84d44f10>))
2023-10-30 12:29:45,580 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:45,580 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,581 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44f70>, (<mixtensor.MixTensor object at 0x7f0f84d45060>, <mixtensor.MixTensor object at 0x7f0f84d45090>))
2023-10-30 12:29:45,582 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:45,582 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,583 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d450c0>, (<mixtensor.MixTensor object at 0x7f0f84d450f0>, <mixtensor.MixTensor object at 0x7f0f84d45120>))
2023-10-30 12:29:45,583 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:45,583 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,585 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d449d0>, (<mixtensor.MixTensor object at 0x7f0f84d441c0>, <mixtensor.MixTensor object at 0x7f0f84d44f40>))
2023-10-30 12:29:45,585 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,585 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:45,586 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,586 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,589 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,589 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,590 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:45,590 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,594 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, (<mixtensor.MixTensor object at 0x7f0f84d44f10>, <mixtensor.MixTensor object at 0x7f0f84d45180>))
2023-10-30 12:29:45,594 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:45,594 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,597 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d451e0>, (<mixtensor.MixTensor object at 0x7f0f84d45210>, <mixtensor.MixTensor object at 0x7f0f84d45300>))
2023-10-30 12:29:45,597 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:45,597 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,599 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45330>, (<mixtensor.MixTensor object at 0x7f0f84d45360>, <mixtensor.MixTensor object at 0x7f0f84d45390>))
2023-10-30 12:29:45,599 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:45,600 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,601 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a60>, (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, <mixtensor.MixTensor object at 0x7f0f84d451b0>))
2023-10-30 12:29:45,602 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 12:29:45,603 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:45,603 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,607 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,607 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,607 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,607 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:45,608 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,608 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44580>
2023-10-30 12:29:45,608 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:45,608 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,608 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44e20>
2023-10-30 12:29:45,608 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:45,609 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,609 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44ee0>
2023-10-30 12:29:45,609 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:45,609 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,609 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d447f0>
2023-10-30 12:29:45,609 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,610 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:45,610 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,610 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,611 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,611 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,611 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:45,611 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,619 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c490>
2023-10-30 12:29:45,620 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:45,620 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,627 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d444f0>
2023-10-30 12:29:45,627 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:45,627 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,633 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d453c0>
2023-10-30 12:29:45,633 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:45,633 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,639 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d449d0>
2023-10-30 12:29:45,640 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:45,641 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:45,642 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,643 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,643 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:45,643 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:45,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,644 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c5b0>
2023-10-30 12:29:45,645 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:45,645 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,645 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c5e0>
2023-10-30 12:29:45,645 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:45,646 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,646 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cb20>
2023-10-30 12:29:45,646 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:45,646 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,649 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fd90>
2023-10-30 12:29:45,649 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,649 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:45,650 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,654 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,655 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-30 12:29:45,655 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,655 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:45,656 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs_k: {}
2023-10-30 12:29:45,656 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fa90>
2023-10-30 12:29:45,656 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:45,656 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs_k: {}
2023-10-30 12:29:45,657 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c1c0>
2023-10-30 12:29:45,657 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:45,657 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs_k: {}
2023-10-30 12:29:45,657 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c640>
2023-10-30 12:29:45,657 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:45,658 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs_k: {}
2023-10-30 12:29:45,658 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fcd0>
2023-10-30 12:29:45,658 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,658 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:45,659 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,663 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,666 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,667 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,667 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:45,667 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,670 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, <mixtensor.MixTensor object at 0x7f0f84d4cee0>))
2023-10-30 12:29:45,670 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:45,670 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,672 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, (<mixtensor.MixTensor object at 0x7f0f84d4ceb0>, <mixtensor.MixTensor object at 0x7f0f84d4cf10>))
2023-10-30 12:29:45,672 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:45,672 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,674 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d540>, (<mixtensor.MixTensor object at 0x7f0f84d4d570>, <mixtensor.MixTensor object at 0x7f0f84d4d270>))
2023-10-30 12:29:45,674 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:45,674 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,675 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c850>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4d0c0>))
2023-10-30 12:29:45,675 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,675 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:45,676 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,681 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,684 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,684 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,684 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:45,685 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,687 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d2a0>, (<mixtensor.MixTensor object at 0x7f0f84d4d780>, <mixtensor.MixTensor object at 0x7f0f84d4d8a0>))
2023-10-30 12:29:45,688 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:45,688 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,689 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dae0>, (<mixtensor.MixTensor object at 0x7f0f84d4d7b0>, <mixtensor.MixTensor object at 0x7f0f84d4d8d0>))
2023-10-30 12:29:45,689 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:45,690 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,691 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d930>, (<mixtensor.MixTensor object at 0x7f0f84d4df60>, <mixtensor.MixTensor object at 0x7f0f84d4df90>))
2023-10-30 12:29:45,691 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:45,691 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,693 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c070>, (<mixtensor.MixTensor object at 0x7f0f84d4d900>, <mixtensor.MixTensor object at 0x7f0f84d4dab0>))
2023-10-30 12:29:45,693 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,693 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:45,693 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,698 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,701 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,702 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,702 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:45,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,705 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d5d0>, (<mixtensor.MixTensor object at 0x7f0f84d4d780>, <mixtensor.MixTensor object at 0x7f0f84d4d8a0>))
2023-10-30 12:29:45,705 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:45,706 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,707 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4de10>, (<mixtensor.MixTensor object at 0x7f0f84d4dfc0>, <mixtensor.MixTensor object at 0x7f0f84d4dff0>))
2023-10-30 12:29:45,707 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:45,707 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,709 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e3b0>, (<mixtensor.MixTensor object at 0x7f0f84d4e0b0>, <mixtensor.MixTensor object at 0x7f0f84d4e1d0>))
2023-10-30 12:29:45,709 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:45,709 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,710 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, (<mixtensor.MixTensor object at 0x7f0f84d4d2a0>, <mixtensor.MixTensor object at 0x7f0f84d4ddb0>))
2023-10-30 12:29:45,711 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,711 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:45,711 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,716 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,719 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,720 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,720 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:45,720 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,724 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d780>, (<mixtensor.MixTensor object at 0x7f0f84d4d8a0>, <mixtensor.MixTensor object at 0x7f0f84d4e3e0>))
2023-10-30 12:29:45,724 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:45,724 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,726 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, (<mixtensor.MixTensor object at 0x7f0f84d4e200>, <mixtensor.MixTensor object at 0x7f0f84d4e6e0>))
2023-10-30 12:29:45,726 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:45,727 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,728 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e740>, (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, <mixtensor.MixTensor object at 0x7f0f84d4e920>))
2023-10-30 12:29:45,729 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:45,729 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,732 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, (<mixtensor.MixTensor object at 0x7f0f84d4c9d0>, <mixtensor.MixTensor object at 0x7f0f84d4e410>))
2023-10-30 12:29:45,733 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,733 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:45,733 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,737 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:45,741 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,741 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,741 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:45,742 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,752 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d8a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e3e0>, <mixtensor.MixTensor object at 0x7f0f84d4e710>))
2023-10-30 12:29:45,752 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:45,752 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,754 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eda0>, (<mixtensor.MixTensor object at 0x7f0f84d4edd0>, <mixtensor.MixTensor object at 0x7f0f84d4ee30>))
2023-10-30 12:29:45,754 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:45,754 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,755 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb00>, (<mixtensor.MixTensor object at 0x7f0f84d4ec20>, <mixtensor.MixTensor object at 0x7f0f84d4ec80>))
2023-10-30 12:29:45,756 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:45,756 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,757 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, (<mixtensor.MixTensor object at 0x7f0f84d4c070>, <mixtensor.MixTensor object at 0x7f0f84d4e770>))
2023-10-30 12:29:45,757 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,758 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:45,758 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,762 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:45,766 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,766 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,766 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:45,766 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,769 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e3e0>, (<mixtensor.MixTensor object at 0x7f0f84d4e710>, <mixtensor.MixTensor object at 0x7f0f84d4f2e0>))
2023-10-30 12:29:45,769 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:45,769 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,771 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f100>, (<mixtensor.MixTensor object at 0x7f0f84d4f160>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:45,771 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:45,771 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,780 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f7c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f7f0>, <mixtensor.MixTensor object at 0x7f0f84d4f4f0>))
2023-10-30 12:29:45,781 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:45,781 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,783 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, (<mixtensor.MixTensor object at 0x7f0f84d4d060>, <mixtensor.MixTensor object at 0x7f0f84d4efe0>))
2023-10-30 12:29:45,783 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,784 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:45,785 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,790 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:45,793 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,793 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,794 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:45,794 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,797 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e710>, (<mixtensor.MixTensor object at 0x7f0f84d4f2e0>, <mixtensor.MixTensor object at 0x7f0f84d4f670>))
2023-10-30 12:29:45,797 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:45,797 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,799 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f850>, (<mixtensor.MixTensor object at 0x7f0f84d4f520>, <mixtensor.MixTensor object at 0x7f0f84d4f790>))
2023-10-30 12:29:45,799 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:45,799 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,801 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f3a0>, <mixtensor.MixTensor object at 0x7f0f84d4fa90>))
2023-10-30 12:29:45,801 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:45,801 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,803 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e740>, (<mixtensor.MixTensor object at 0x7f0f84d4c850>, <mixtensor.MixTensor object at 0x7f0f84d4f820>))
2023-10-30 12:29:45,803 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,803 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:45,804 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,809 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:45,813 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,813 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,814 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:45,814 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,817 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f670>, <mixtensor.MixTensor object at 0x7f0f84d4f880>))
2023-10-30 12:29:45,817 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:45,817 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,819 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f730>, (<mixtensor.MixTensor object at 0x7f0f84d4f580>, <mixtensor.MixTensor object at 0x7f0f84d44940>))
2023-10-30 12:29:45,819 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:45,819 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,826 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44e20>, (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, <mixtensor.MixTensor object at 0x7f0f84d447f0>))
2023-10-30 12:29:45,826 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:45,826 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,828 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658c40>, (<mixtensor.MixTensor object at 0x7f0f8565b130>, <mixtensor.MixTensor object at 0x7f0f85658d00>))
2023-10-30 12:29:45,829 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,829 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:45,829 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,834 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:45,837 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,837 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,838 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:45,838 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,840 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c040>, (<mixtensor.MixTensor object at 0x7f0f8572fa30>, <mixtensor.MixTensor object at 0x7f0f8572fee0>))
2023-10-30 12:29:45,842 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:45,842 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,844 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d060>, (<mixtensor.MixTensor object at 0x7f0f8572dd80>, <mixtensor.MixTensor object at 0x7f0f8572d450>))
2023-10-30 12:29:45,844 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:45,844 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,845 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd60>, (<mixtensor.MixTensor object at 0x7f0f8572c790>, <mixtensor.MixTensor object at 0x7f0f8572ca60>))
2023-10-30 12:29:45,845 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:45,846 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,847 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d4e0>, (<mixtensor.MixTensor object at 0x7f0f8572d4b0>, <mixtensor.MixTensor object at 0x7f0f8572d180>))
2023-10-30 12:29:45,847 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,847 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:45,848 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,852 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:45,856 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,856 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,856 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:45,857 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,864 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, (<mixtensor.MixTensor object at 0x7f0f84d4cdf0>, <mixtensor.MixTensor object at 0x7f0f84d4e1a0>))
2023-10-30 12:29:45,864 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:45,864 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,866 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e140>, (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, <mixtensor.MixTensor object at 0x7f0f84d4e170>))
2023-10-30 12:29:45,866 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:45,866 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,867 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4e650>, <mixtensor.MixTensor object at 0x7f0f84d4e530>))
2023-10-30 12:29:45,867 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:45,868 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,869 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f730>, (<mixtensor.MixTensor object at 0x7f0f84d4eb00>, <mixtensor.MixTensor object at 0x7f0f84d4dd20>))
2023-10-30 12:29:45,869 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,869 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:45,870 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,874 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:45,877 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,878 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,878 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:45,878 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,881 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cdf0>, (<mixtensor.MixTensor object at 0x7f0f84d4e3e0>, <mixtensor.MixTensor object at 0x7f0f84d4fac0>))
2023-10-30 12:29:45,881 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:45,881 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,882 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, <mixtensor.MixTensor object at 0x7f0f84d4cbb0>))
2023-10-30 12:29:45,883 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:45,883 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,884 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f100>, (<mixtensor.MixTensor object at 0x7f0f84d4e3b0>, <mixtensor.MixTensor object at 0x7f0f84d4f2e0>))
2023-10-30 12:29:45,884 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:45,884 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,886 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, <mixtensor.MixTensor object at 0x7f0f84d4f850>))
2023-10-30 12:29:45,886 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,886 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:45,887 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,887 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:45,891 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,891 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,891 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:45,891 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,894 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f880>, (<mixtensor.MixTensor object at 0x7f0f84d4e710>, <mixtensor.MixTensor object at 0x7f0f84d4dc90>))
2023-10-30 12:29:45,894 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:45,894 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,896 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f700>, (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, <mixtensor.MixTensor object at 0x7f0f84d4d930>))
2023-10-30 12:29:45,896 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:45,896 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,897 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f910>, (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, <mixtensor.MixTensor object at 0x7f0f84d44790>))
2023-10-30 12:29:45,898 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:45,898 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,899 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eaa0>, (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, <mixtensor.MixTensor object at 0x7f0f84d4d3f0>))
2023-10-30 12:29:45,899 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 12:29:45,900 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:45,900 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,904 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:45,904 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,904 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,905 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:45,905 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,905 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ecb0>
2023-10-30 12:29:45,905 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:45,905 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,906 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e140>
2023-10-30 12:29:45,906 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:45,906 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,906 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f880>
2023-10-30 12:29:45,906 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:45,906 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,907 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4eb60>
2023-10-30 12:29:45,907 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,907 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:45,907 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,908 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:45,908 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,908 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,908 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:45,908 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,915 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565b070>
2023-10-30 12:29:45,916 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:45,916 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,922 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f910>
2023-10-30 12:29:45,922 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:45,922 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,928 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4eaa0>
2023-10-30 12:29:45,928 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:45,928 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:45,934 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e3e0>
2023-10-30 12:29:45,935 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:45,936 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:45,936 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,937 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:45,937 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:45,938 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,938 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:45,938 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,939 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572f610>
2023-10-30 12:29:45,939 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:45,939 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,941 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d960>
2023-10-30 12:29:45,941 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:45,941 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,941 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fcd0>
2023-10-30 12:29:45,941 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:45,942 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:45,943 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fca0>
2023-10-30 12:29:45,943 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,944 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:45,944 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:45,949 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-30 12:29:45,949 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:45,950 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:45,950 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs_k: {}
2023-10-30 12:29:45,950 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c0a0>
2023-10-30 12:29:45,950 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:45,950 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs_k: {}
2023-10-30 12:29:45,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cb20>
2023-10-30 12:29:45,951 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:45,951 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs_k: {}
2023-10-30 12:29:45,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:45,952 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:45,952 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs_k: {}
2023-10-30 12:29:45,952 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fc10>
2023-10-30 12:29:45,952 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:45,953 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:45,953 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,958 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:45,962 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,962 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,962 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:45,963 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,965 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4feb0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, <mixtensor.MixTensor object at 0x7f0f84d4fe20>))
2023-10-30 12:29:45,966 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:45,966 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,967 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f940>, (<mixtensor.MixTensor object at 0x7f0f84d4e050>, <mixtensor.MixTensor object at 0x7f0f84d4fd60>))
2023-10-30 12:29:45,968 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:45,968 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,969 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, <mixtensor.MixTensor object at 0x7f0f84d4f5e0>))
2023-10-30 12:29:45,969 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:45,970 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,971 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd30>, (<mixtensor.MixTensor object at 0x7f0f84d4f490>, <mixtensor.MixTensor object at 0x7f0f84d4f550>))
2023-10-30 12:29:45,971 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:45,971 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:45,972 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,977 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:45,980 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,981 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,981 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:45,981 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,984 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4f880>, <mixtensor.MixTensor object at 0x7f0f84d4eb60>))
2023-10-30 12:29:45,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:45,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,986 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ef50>, (<mixtensor.MixTensor object at 0x7f0f84d4c1c0>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:45,986 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:45,986 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,988 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eaa0>, (<mixtensor.MixTensor object at 0x7f0f84d4e3e0>, <mixtensor.MixTensor object at 0x7f0f84d4ea40>))
2023-10-30 12:29:45,988 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:45,988 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:45,989 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, (<mixtensor.MixTensor object at 0x7f0f84d4e680>, <mixtensor.MixTensor object at 0x7f0f84d4e590>))
2023-10-30 12:29:45,990 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:45,990 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:45,990 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:45,995 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:45,999 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:45,999 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,165 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:46,166 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,169 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f880>, <mixtensor.MixTensor object at 0x7f0f84d4eb60>))
2023-10-30 12:29:46,169 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:46,170 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,173 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cdf0>, (<mixtensor.MixTensor object at 0x7f0f84d4e710>, <mixtensor.MixTensor object at 0x7f0f84d4f670>))
2023-10-30 12:29:46,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:46,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d440d0>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:46,175 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:46,176 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,177 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4feb0>, (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, <mixtensor.MixTensor object at 0x7f0f84d4eb90>))
2023-10-30 12:29:46,178 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,178 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:46,179 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:46,183 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:46,187 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,187 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,187 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:46,188 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,191 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f880>, (<mixtensor.MixTensor object at 0x7f0f84d4eb60>, <mixtensor.MixTensor object at 0x7f0f84d44340>))
2023-10-30 12:29:46,191 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:46,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,193 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44160>, (<mixtensor.MixTensor object at 0x7f0f84d441f0>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:46,193 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:46,193 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,194 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445b0>, (<mixtensor.MixTensor object at 0x7f0f84d445e0>, <mixtensor.MixTensor object at 0x7f0f84d44310>))
2023-10-30 12:29:46,195 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:46,195 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,197 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fa00>, (<mixtensor.MixTensor object at 0x7f0f84d44340>, <mixtensor.MixTensor object at 0x7f0f84d44370>))
2023-10-30 12:29:46,197 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,197 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:46,197 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:46,202 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:46,206 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,206 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,206 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:46,206 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,209 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44130>, (<mixtensor.MixTensor object at 0x7f0f84d440a0>, <mixtensor.MixTensor object at 0x7f0f84d44280>))
2023-10-30 12:29:46,209 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:46,210 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,212 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44640>, (<mixtensor.MixTensor object at 0x7f0f84d44400>, <mixtensor.MixTensor object at 0x7f0f84d44670>))
2023-10-30 12:29:46,212 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:46,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,214 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44700>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d448b0>))
2023-10-30 12:29:46,214 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:46,214 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,216 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d44280>, <mixtensor.MixTensor object at 0x7f0f84d44610>))
2023-10-30 12:29:46,216 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,216 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:46,217 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:46,221 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:46,225 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,225 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,225 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:46,225 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44160>, (<mixtensor.MixTensor object at 0x7f0f84d445b0>, <mixtensor.MixTensor object at 0x7f0f84d446a0>))
2023-10-30 12:29:46,229 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:46,229 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,230 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44550>, (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, <mixtensor.MixTensor object at 0x7f0f84d44b20>))
2023-10-30 12:29:46,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:46,231 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,232 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b50>, (<mixtensor.MixTensor object at 0x7f0f84d44910>, <mixtensor.MixTensor object at 0x7f0f84d449a0>))
2023-10-30 12:29:46,233 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:46,233 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,235 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446a0>, (<mixtensor.MixTensor object at 0x7f0f84d44460>, <mixtensor.MixTensor object at 0x7f0f84d44730>))
2023-10-30 12:29:46,235 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,235 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:46,235 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:46,240 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:46,243 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,244 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,244 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:46,244 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,247 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d44160>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:46,247 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:46,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,250 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a90>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d44c10>))
2023-10-30 12:29:46,250 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:46,250 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,252 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a30>, (<mixtensor.MixTensor object at 0x7f0f84d44fa0>, <mixtensor.MixTensor object at 0x7f0f84d44fd0>))
2023-10-30 12:29:46,252 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:46,252 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,253 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44640>, (<mixtensor.MixTensor object at 0x7f0f84d44700>, <mixtensor.MixTensor object at 0x7f0f84d44d60>))
2023-10-30 12:29:46,254 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,254 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:46,254 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:46,258 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:46,262 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,262 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,262 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:46,263 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,266 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d44160>, <mixtensor.MixTensor object at 0x7f0f84d44e50>))
2023-10-30 12:29:46,266 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:46,266 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,268 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45000>, (<mixtensor.MixTensor object at 0x7f0f84d45030>, <mixtensor.MixTensor object at 0x7f0f84d45270>))
2023-10-30 12:29:46,268 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:46,268 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,270 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45060>, (<mixtensor.MixTensor object at 0x7f0f84d450f0>, <mixtensor.MixTensor object at 0x7f0f84d441c0>))
2023-10-30 12:29:46,270 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:46,270 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,271 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446a0>, (<mixtensor.MixTensor object at 0x7f0f84d448e0>, <mixtensor.MixTensor object at 0x7f0f84d446d0>))
2023-10-30 12:29:46,272 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,272 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:46,272 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:46,277 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:46,280 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,280 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,281 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:46,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,284 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d44160>, <mixtensor.MixTensor object at 0x7f0f84d452d0>))
2023-10-30 12:29:46,284 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:46,284 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,286 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45120>, (<mixtensor.MixTensor object at 0x7f0f84d44f40>, <mixtensor.MixTensor object at 0x7f0f84d4ffd0>))
2023-10-30 12:29:46,287 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:46,287 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,288 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bb50>, (<mixtensor.MixTensor object at 0x7f0f8565bb80>, <mixtensor.MixTensor object at 0x7f0f8565b0d0>))
2023-10-30 12:29:46,288 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:46,288 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,290 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, (<mixtensor.MixTensor object at 0x7f0f8565b6d0>, <mixtensor.MixTensor object at 0x7f0f8565ba00>))
2023-10-30 12:29:46,290 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,290 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:46,291 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:46,295 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:46,298 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,298 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,299 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:46,299 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,301 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f7c0>, (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, <mixtensor.MixTensor object at 0x7f0f8572d2d0>))
2023-10-30 12:29:46,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:46,302 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,304 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cca0>, (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, <mixtensor.MixTensor object at 0x7f0f8572d030>))
2023-10-30 12:29:46,304 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:46,304 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,305 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c9a0>, (<mixtensor.MixTensor object at 0x7f0f8572fc40>, <mixtensor.MixTensor object at 0x7f0f8572c6d0>))
2023-10-30 12:29:46,306 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:46,306 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,307 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d1e0>, (<mixtensor.MixTensor object at 0x7f0f8572c370>, <mixtensor.MixTensor object at 0x7f0f8572c0d0>))
2023-10-30 12:29:46,307 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,308 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:46,308 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:46,312 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:46,316 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,316 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,316 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:46,316 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,320 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca30>, (<mixtensor.MixTensor object at 0x7f0f8572d750>, <mixtensor.MixTensor object at 0x7f0f8572fcd0>))
2023-10-30 12:29:46,320 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:46,321 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,322 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd90>, (<mixtensor.MixTensor object at 0x7f0f84d45120>, <mixtensor.MixTensor object at 0x7f0f84d44a30>))
2023-10-30 12:29:46,322 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:46,322 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,324 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44640>, (<mixtensor.MixTensor object at 0x7f0f84d44e50>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:46,324 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:46,324 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,326 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, (<mixtensor.MixTensor object at 0x7f0f8572fa90>, <mixtensor.MixTensor object at 0x7f0f8572cd30>))
2023-10-30 12:29:46,326 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,326 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:46,327 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:46,327 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:46,331 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,331 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,331 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:46,331 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,334 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d750>, (<mixtensor.MixTensor object at 0x7f0f8572fcd0>, <mixtensor.MixTensor object at 0x7f0f84d45060>))
2023-10-30 12:29:46,334 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:46,334 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,336 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447c0>, (<mixtensor.MixTensor object at 0x7f0f84d44550>, <mixtensor.MixTensor object at 0x7f0f84d44a90>))
2023-10-30 12:29:46,336 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:46,336 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,338 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445b0>, (<mixtensor.MixTensor object at 0x7f0f84d44130>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:46,338 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:46,338 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,339 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c730>, (<mixtensor.MixTensor object at 0x7f0f84d45060>, <mixtensor.MixTensor object at 0x7f0f84d446a0>))
2023-10-30 12:29:46,340 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 12:29:46,340 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:46,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:46,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:46,345 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,345 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,345 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:46,345 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,345 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dc00>
2023-10-30 12:29:46,346 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:46,346 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,346 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c9a0>
2023-10-30 12:29:46,346 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:46,346 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fcd0>
2023-10-30 12:29:46,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:46,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d45000>
2023-10-30 12:29:46,347 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,348 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:46,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:46,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:46,349 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,349 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,349 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:46,349 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,359 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c730>
2023-10-30 12:29:46,360 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:46,360 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,366 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fee0>
2023-10-30 12:29:46,367 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:46,367 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,374 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572f610>
2023-10-30 12:29:46,374 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:46,374 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,380 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d960>
2023-10-30 12:29:46,381 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:46,383 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:46,383 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:46,384 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:46,384 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:46,384 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,385 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:46,385 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,385 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cf10>
2023-10-30 12:29:46,385 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:46,385 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,386 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e1a0>
2023-10-30 12:29:46,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:46,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,386 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d7b0>
2023-10-30 12:29:46,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:46,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,387 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4df60>
2023-10-30 12:29:46,387 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,387 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:46,387 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:46,391 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:46,392 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-30 12:29:46,392 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,392 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:46,392 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs_k: {}
2023-10-30 12:29:46,393 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d090>
2023-10-30 12:29:46,393 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:46,393 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs_k: {}
2023-10-30 12:29:46,393 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4dab0>
2023-10-30 12:29:46,393 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:46,394 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs_k: {}
2023-10-30 12:29:46,394 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e260>
2023-10-30 12:29:46,394 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:46,394 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs_k: {}
2023-10-30 12:29:46,394 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cb20>
2023-10-30 12:29:46,395 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,395 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:46,395 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:46,400 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:46,403 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,403 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,403 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:46,404 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,407 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4edd0>, (<mixtensor.MixTensor object at 0x7f0f84d4ec20>, <mixtensor.MixTensor object at 0x7f0f84d4c070>))
2023-10-30 12:29:46,407 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:46,407 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,409 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ee30>, (<mixtensor.MixTensor object at 0x7f0f84d4ec80>, <mixtensor.MixTensor object at 0x7f0f84d4e770>))
2023-10-30 12:29:46,409 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:46,409 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,411 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, <mixtensor.MixTensor object at 0x7f0f84d4f160>))
2023-10-30 12:29:46,411 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:46,411 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,412 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d8d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:46,413 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,413 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:46,413 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:46,418 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:46,421 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,422 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,422 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:46,422 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,425 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f820>, (<mixtensor.MixTensor object at 0x7f0f84d4f580>, <mixtensor.MixTensor object at 0x7f0f84d4cf40>))
2023-10-30 12:29:46,425 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:46,425 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,427 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc10>, (<mixtensor.MixTensor object at 0x7f0f84d4e740>, <mixtensor.MixTensor object at 0x7f0f84d4e0e0>))
2023-10-30 12:29:46,427 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:46,427 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,429 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e650>, <mixtensor.MixTensor object at 0x7f0f84d4eb00>))
2023-10-30 12:29:46,429 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:46,429 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,430 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ebf0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, <mixtensor.MixTensor object at 0x7f0f84d4f610>))
2023-10-30 12:29:46,431 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,431 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:46,431 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:46,436 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:46,440 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,440 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,440 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:46,441 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,444 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4cbb0>, <mixtensor.MixTensor object at 0x7f0f84d4f2e0>))
2023-10-30 12:29:46,444 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:46,444 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,446 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, <mixtensor.MixTensor object at 0x7f0f84d4d930>))
2023-10-30 12:29:46,446 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:46,446 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,447 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, (<mixtensor.MixTensor object at 0x7f0f84d4c640>, <mixtensor.MixTensor object at 0x7f0f84d4fe80>))
2023-10-30 12:29:46,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:46,448 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,449 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dd20>, (<mixtensor.MixTensor object at 0x7f0f84d4f6d0>, <mixtensor.MixTensor object at 0x7f0f84d4f850>))
2023-10-30 12:29:46,449 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,450 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:46,450 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:46,454 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:46,458 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,458 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,458 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:46,459 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,461 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c4f0>, (<mixtensor.MixTensor object at 0x7f0f8572caf0>, <mixtensor.MixTensor object at 0x7f0f8572cc40>))
2023-10-30 12:29:46,461 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:46,462 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,463 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cbe0>, (<mixtensor.MixTensor object at 0x7f0f8572cbb0>, <mixtensor.MixTensor object at 0x7f0f8572c970>))
2023-10-30 12:29:46,463 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:46,463 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,465 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c940>, (<mixtensor.MixTensor object at 0x7f0f8572c910>, <mixtensor.MixTensor object at 0x7f0f8572c8e0>))
2023-10-30 12:29:46,465 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:46,465 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,466 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ceb0>, (<mixtensor.MixTensor object at 0x7f0f8572ce80>, <mixtensor.MixTensor object at 0x7f0f8572cc10>))
2023-10-30 12:29:46,467 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,467 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:46,467 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:46,472 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:46,475 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,475 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,476 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:46,476 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,478 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c4f0>, (<mixtensor.MixTensor object at 0x7f0f8572caf0>, <mixtensor.MixTensor object at 0x7f0f8572c880>))
2023-10-30 12:29:46,478 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:46,478 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,480 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c670>, (<mixtensor.MixTensor object at 0x7f0f8572c610>, <mixtensor.MixTensor object at 0x7f0f8572c9d0>))
2023-10-30 12:29:46,480 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:46,480 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,482 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c520>, (<mixtensor.MixTensor object at 0x7f0f8572c580>, <mixtensor.MixTensor object at 0x7f0f8572c220>))
2023-10-30 12:29:46,482 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:46,482 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,483 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf70>, (<mixtensor.MixTensor object at 0x7f0f8572cee0>, <mixtensor.MixTensor object at 0x7f0f8572c850>))
2023-10-30 12:29:46,483 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,484 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:46,484 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:46,489 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:46,492 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,492 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,492 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:46,492 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,495 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c4f0>, (<mixtensor.MixTensor object at 0x7f0f8572caf0>, <mixtensor.MixTensor object at 0x7f0f8572c160>))
2023-10-30 12:29:46,495 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:46,495 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,497 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd50>, (<mixtensor.MixTensor object at 0x7f0f8572c5b0>, <mixtensor.MixTensor object at 0x7f0f8572d000>))
2023-10-30 12:29:46,497 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:46,497 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,499 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d330>, (<mixtensor.MixTensor object at 0x7f0f8572d390>, <mixtensor.MixTensor object at 0x7f0f8572dd80>))
2023-10-30 12:29:46,499 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:46,499 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,501 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf10>, (<mixtensor.MixTensor object at 0x7f0f8572cc70>, <mixtensor.MixTensor object at 0x7f0f8572fc70>))
2023-10-30 12:29:46,501 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,501 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:46,501 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:46,506 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:46,510 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,510 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,510 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:46,510 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,513 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572caf0>, (<mixtensor.MixTensor object at 0x7f0f8572c160>, <mixtensor.MixTensor object at 0x7f0f8572d4b0>))
2023-10-30 12:29:46,513 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:46,513 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,515 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca60>, (<mixtensor.MixTensor object at 0x7f0f8572d180>, <mixtensor.MixTensor object at 0x7f0f8572fd00>))
2023-10-30 12:29:46,515 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:46,515 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,516 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d1e0>, (<mixtensor.MixTensor object at 0x7f0f8572cca0>, <mixtensor.MixTensor object at 0x7f0f8572d900>))
2023-10-30 12:29:46,517 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:46,517 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,518 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cc40>, (<mixtensor.MixTensor object at 0x7f0f8572c8b0>, <mixtensor.MixTensor object at 0x7f0f8572d450>))
2023-10-30 12:29:46,518 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,518 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:46,519 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:46,524 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:46,527 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,527 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,528 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:46,528 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,530 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c160>, (<mixtensor.MixTensor object at 0x7f0f8572d4b0>, <mixtensor.MixTensor object at 0x7f0f8572f610>))
2023-10-30 12:29:46,531 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:46,531 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,532 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d6c0>, (<mixtensor.MixTensor object at 0x7f0f8572c1c0>, <mixtensor.MixTensor object at 0x7f0f8572dcf0>))
2023-10-30 12:29:46,532 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:46,533 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,534 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d750>, (<mixtensor.MixTensor object at 0x7f0f8572cd90>, <mixtensor.MixTensor object at 0x7f0f8572d2a0>))
2023-10-30 12:29:46,534 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:46,534 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,536 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c250>, (<mixtensor.MixTensor object at 0x7f0f8572cf70>, <mixtensor.MixTensor object at 0x7f0f8572d960>))
2023-10-30 12:29:46,536 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,536 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:46,536 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:46,541 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:46,544 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,544 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,545 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:46,545 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,548 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d4b0>, (<mixtensor.MixTensor object at 0x7f0f8572f610>, <mixtensor.MixTensor object at 0x7f0f8572f7c0>))
2023-10-30 12:29:46,548 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:46,548 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,550 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, (<mixtensor.MixTensor object at 0x7f0f84d44940>, <mixtensor.MixTensor object at 0x7f0f8572dc90>))
2023-10-30 12:29:46,550 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:46,550 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,552 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d9c0>, (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, <mixtensor.MixTensor object at 0x7f0f856581f0>))
2023-10-30 12:29:46,552 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:46,552 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,553 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd50>, (<mixtensor.MixTensor object at 0x7f0f8572ceb0>, <mixtensor.MixTensor object at 0x7f0f8572d600>))
2023-10-30 12:29:46,553 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,554 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:46,554 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:46,559 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:46,562 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,563 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,563 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:46,563 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,566 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bc40>, (<mixtensor.MixTensor object at 0x7f0f8565ba60>, <mixtensor.MixTensor object at 0x7f0f85658c40>))
2023-10-30 12:29:46,566 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:46,566 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,568 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4dd20>, <mixtensor.MixTensor object at 0x7f0f84d4fe50>))
2023-10-30 12:29:46,568 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:46,568 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,570 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, (<mixtensor.MixTensor object at 0x7f0f84d4eb60>, <mixtensor.MixTensor object at 0x7f0f84d4d060>))
2023-10-30 12:29:46,570 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:46,570 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,571 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, (<mixtensor.MixTensor object at 0x7f0f8565b100>, <mixtensor.MixTensor object at 0x7f0f84d4f040>))
2023-10-30 12:29:46,572 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,572 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:46,572 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:46,576 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:46,580 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,580 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,580 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:46,580 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,583 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, (<mixtensor.MixTensor object at 0x7f0f84d4e530>, <mixtensor.MixTensor object at 0x7f0f84d4ce80>))
2023-10-30 12:29:46,583 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:46,583 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,585 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4fa90>, <mixtensor.MixTensor object at 0x7f0f84d4f580>))
2023-10-30 12:29:46,585 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:46,585 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,587 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f7f0>, (<mixtensor.MixTensor object at 0x7f0f84d4fa30>, <mixtensor.MixTensor object at 0x7f0f84d4f8b0>))
2023-10-30 12:29:46,587 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:46,587 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,875 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f730>, (<mixtensor.MixTensor object at 0x7f0f84d4e080>, <mixtensor.MixTensor object at 0x7f0f84d4f2e0>))
2023-10-30 12:29:46,876 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,876 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:46,876 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:46,877 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:46,880 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,880 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,881 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:46,881 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,884 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, (<mixtensor.MixTensor object at 0x7f0f84d4e530>, <mixtensor.MixTensor object at 0x7f0f84d4e8f0>))
2023-10-30 12:29:46,884 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:46,884 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,885 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d7b0>, (<mixtensor.MixTensor object at 0x7f0f84d4da80>, <mixtensor.MixTensor object at 0x7f0f84d453c0>))
2023-10-30 12:29:46,886 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:46,886 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,887 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d449d0>, (<mixtensor.MixTensor object at 0x7f0f84d44790>, <mixtensor.MixTensor object at 0x7f0f84d45000>))
2023-10-30 12:29:46,887 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:46,887 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,889 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, <mixtensor.MixTensor object at 0x7f0f84d4e410>))
2023-10-30 12:29:46,889 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 12:29:46,889 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:46,889 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:46,893 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:46,893 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,893 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,894 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:46,894 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,894 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:46,894 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:46,894 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,895 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f730>
2023-10-30 12:29:46,895 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:46,895 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,895 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fc10>
2023-10-30 12:29:46,895 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:46,895 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,896 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:46,896 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,896 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:46,896 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:46,897 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:46,897 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,897 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,897 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:46,897 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,904 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565b7c0>
2023-10-30 12:29:46,905 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:46,905 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,911 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f640>
2023-10-30 12:29:46,911 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:46,911 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,918 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4df90>
2023-10-30 12:29:46,918 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:46,918 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:46,924 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e8f0>
2023-10-30 12:29:46,925 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:46,926 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:46,926 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:46,927 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:46,927 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:46,927 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,927 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:46,928 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,928 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c0d0>
2023-10-30 12:29:46,928 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:46,928 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,928 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c6a0>
2023-10-30 12:29:46,929 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:46,929 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,930 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565bb80>
2023-10-30 12:29:46,931 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:46,931 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:46,931 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f85658c40>
2023-10-30 12:29:46,931 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,931 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:46,932 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:46,936 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:46,936 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-30 12:29:46,937 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:46,937 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:46,937 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs_k: {}
2023-10-30 12:29:46,937 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d6f0>
2023-10-30 12:29:46,937 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:46,938 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs_k: {}
2023-10-30 12:29:46,938 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4de40>
2023-10-30 12:29:46,938 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:46,938 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs_k: {}
2023-10-30 12:29:46,938 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e380>
2023-10-30 12:29:46,939 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:46,939 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs_k: {}
2023-10-30 12:29:46,939 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d060>
2023-10-30 12:29:46,939 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:46,940 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:46,940 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:46,944 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:46,947 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,947 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,948 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:46,948 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,961 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ddb0>, (<mixtensor.MixTensor object at 0x7f0f84d4da50>, <mixtensor.MixTensor object at 0x7f0f84d4d7b0>))
2023-10-30 12:29:46,961 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:46,961 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,964 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, (<mixtensor.MixTensor object at 0x7f0f84d4e050>, <mixtensor.MixTensor object at 0x7f0f84d4ee90>))
2023-10-30 12:29:46,964 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:46,964 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,966 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f490>, (<mixtensor.MixTensor object at 0x7f0f84d4f7c0>, <mixtensor.MixTensor object at 0x7f0f84d4e140>))
2023-10-30 12:29:46,967 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:46,967 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,969 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c730>, (<mixtensor.MixTensor object at 0x7f0f84d4e890>, <mixtensor.MixTensor object at 0x7f0f84d4f460>))
2023-10-30 12:29:46,969 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:46,970 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:46,970 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:46,975 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:46,979 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:46,980 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,980 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:46,980 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,984 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:46,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:46,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,987 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e710>, (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, <mixtensor.MixTensor object at 0x7f0f84d4f670>))
2023-10-30 12:29:46,987 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:46,987 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,989 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb90>, (<mixtensor.MixTensor object at 0x7f0f84d4ffd0>, <mixtensor.MixTensor object at 0x7f0f84d4f1c0>))
2023-10-30 12:29:46,989 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:46,989 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:46,991 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c700>, (<mixtensor.MixTensor object at 0x7f0f84d4ea40>, <mixtensor.MixTensor object at 0x7f0f84d4e590>))
2023-10-30 12:29:46,992 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:46,992 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:46,992 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:46,999 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,003 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,004 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,004 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:47,004 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,007 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f070>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:47,007 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:47,007 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,009 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c310>, (<mixtensor.MixTensor object at 0x7f0f84d4dde0>, <mixtensor.MixTensor object at 0x7f0f84d4f7f0>))
2023-10-30 12:29:47,010 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:47,010 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,012 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dab0>, (<mixtensor.MixTensor object at 0x7f0f84d4e260>, <mixtensor.MixTensor object at 0x7f0f84d4cb20>))
2023-10-30 12:29:47,012 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:47,012 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,014 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ddb0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:47,014 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,015 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:47,015 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,021 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,025 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,025 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,025 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:47,026 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,029 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f910>, (<mixtensor.MixTensor object at 0x7f0f84d4dcc0>, <mixtensor.MixTensor object at 0x7f0f84d4d3f0>))
2023-10-30 12:29:47,029 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:47,029 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,030 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e530>, (<mixtensor.MixTensor object at 0x7f0f84d44220>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:47,031 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:47,031 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,032 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44490>, (<mixtensor.MixTensor object at 0x7f0f84d442b0>, <mixtensor.MixTensor object at 0x7f0f84d441f0>))
2023-10-30 12:29:47,032 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:47,032 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,034 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f070>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4ce80>))
2023-10-30 12:29:47,034 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,034 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:47,035 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,039 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,042 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,042 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,043 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:47,043 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,046 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dcc0>, (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, <mixtensor.MixTensor object at 0x7f0f84d44340>))
2023-10-30 12:29:47,046 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:47,046 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,048 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44880>, (<mixtensor.MixTensor object at 0x7f0f84d44040>, <mixtensor.MixTensor object at 0x7f0f84d44400>))
2023-10-30 12:29:47,048 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:47,048 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,050 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44070>, (<mixtensor.MixTensor object at 0x7f0f84d44280>, <mixtensor.MixTensor object at 0x7f0f84d44970>))
2023-10-30 12:29:47,050 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:47,050 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,051 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f520>, (<mixtensor.MixTensor object at 0x7f0f84d44340>, <mixtensor.MixTensor object at 0x7f0f84d44850>))
2023-10-30 12:29:47,052 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,052 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:47,052 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,056 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,059 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,060 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,060 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:47,060 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,063 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445e0>, (<mixtensor.MixTensor object at 0x7f0f84d44490>, <mixtensor.MixTensor object at 0x7f0f84d44670>))
2023-10-30 12:29:47,063 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:47,063 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,065 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44610>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44dc0>))
2023-10-30 12:29:47,065 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:47,065 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,066 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b80>, (<mixtensor.MixTensor object at 0x7f0f84d44b20>, <mixtensor.MixTensor object at 0x7f0f84d449a0>))
2023-10-30 12:29:47,067 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:47,067 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,068 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44490>, (<mixtensor.MixTensor object at 0x7f0f84d44670>, <mixtensor.MixTensor object at 0x7f0f84d448b0>))
2023-10-30 12:29:47,068 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,068 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:47,069 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,073 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,076 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,076 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,076 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:47,076 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,079 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44880>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d44df0>))
2023-10-30 12:29:47,079 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:47,080 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,081 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44be0>, (<mixtensor.MixTensor object at 0x7f0f84d44fa0>, <mixtensor.MixTensor object at 0x7f0f84d44fd0>))
2023-10-30 12:29:47,081 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:47,082 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,083 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d60>, (<mixtensor.MixTensor object at 0x7f0f84d454b0>, <mixtensor.MixTensor object at 0x7f0f84d454e0>))
2023-10-30 12:29:47,083 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:47,083 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,085 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44df0>, (<mixtensor.MixTensor object at 0x7f0f84d44430>, <mixtensor.MixTensor object at 0x7f0f84d44e80>))
2023-10-30 12:29:47,085 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,085 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:47,085 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,089 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,093 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,093 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,093 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:47,093 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,096 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44490>, (<mixtensor.MixTensor object at 0x7f0f84d44880>, <mixtensor.MixTensor object at 0x7f0f84d450f0>))
2023-10-30 12:29:47,096 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:47,096 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,098 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45210>, (<mixtensor.MixTensor object at 0x7f0f84d45360>, <mixtensor.MixTensor object at 0x7f0f84d44d00>))
2023-10-30 12:29:47,098 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:47,098 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,100 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d45120>, <mixtensor.MixTensor object at 0x7f0f84d44e50>))
2023-10-30 12:29:47,100 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:47,100 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,102 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44610>, (<mixtensor.MixTensor object at 0x7f0f84d44b80>, <mixtensor.MixTensor object at 0x7f0f84d448e0>))
2023-10-30 12:29:47,102 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,102 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:47,103 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,107 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,111 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,111 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:47,111 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,114 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44490>, (<mixtensor.MixTensor object at 0x7f0f84d44880>, <mixtensor.MixTensor object at 0x7f0f84d452d0>))
2023-10-30 12:29:47,114 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:47,114 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,116 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d45300>, <mixtensor.MixTensor object at 0x7f0f84d4df60>))
2023-10-30 12:29:47,117 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:47,117 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,118 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1d0>, (<mixtensor.MixTensor object at 0x7f0f84d4c700>, <mixtensor.MixTensor object at 0x7f0f84d4d420>))
2023-10-30 12:29:47,118 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:47,118 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,120 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, (<mixtensor.MixTensor object at 0x7f0f84d4c4f0>, <mixtensor.MixTensor object at 0x7f0f84d4f5e0>))
2023-10-30 12:29:47,120 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,120 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:47,121 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,125 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,129 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,129 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,129 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:47,129 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,132 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dc60>, (<mixtensor.MixTensor object at 0x7f0f8572d690>, <mixtensor.MixTensor object at 0x7f0f8572d9c0>))
2023-10-30 12:29:47,132 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:47,132 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,134 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c6d0>, (<mixtensor.MixTensor object at 0x7f0f8572c1f0>, <mixtensor.MixTensor object at 0x7f0f8572cf40>))
2023-10-30 12:29:47,134 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:47,135 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,136 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd50>, (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, <mixtensor.MixTensor object at 0x7f0f8572dba0>))
2023-10-30 12:29:47,136 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:47,136 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,139 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d030>, (<mixtensor.MixTensor object at 0x7f0f8572d8d0>, <mixtensor.MixTensor object at 0x7f0f8572fa90>))
2023-10-30 12:29:47,140 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,140 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:47,140 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,145 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,148 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,148 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,148 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:47,149 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,153 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d8a0>, (<mixtensor.MixTensor object at 0x7f0f8572cf10>, <mixtensor.MixTensor object at 0x7f0f8572f7c0>))
2023-10-30 12:29:47,153 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:47,153 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,157 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45030>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d44160>))
2023-10-30 12:29:47,157 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:47,158 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,160 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45210>, (<mixtensor.MixTensor object at 0x7f0f84d440a0>, <mixtensor.MixTensor object at 0x7f0f84d44610>))
2023-10-30 12:29:47,161 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:47,161 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,163 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d690>, (<mixtensor.MixTensor object at 0x7f0f8572d750>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:47,164 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,164 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:47,165 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,165 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,170 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,170 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,170 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:47,171 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,174 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf10>, (<mixtensor.MixTensor object at 0x7f0f8572f7c0>, <mixtensor.MixTensor object at 0x7f0f84d44490>))
2023-10-30 12:29:47,174 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:47,174 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,177 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452d0>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d451b0>))
2023-10-30 12:29:47,177 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:47,177 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,181 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44c70>, (<mixtensor.MixTensor object at 0x7f0f84d44a90>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:47,182 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:47,182 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,183 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ccd0>, (<mixtensor.MixTensor object at 0x7f0f84d44490>, <mixtensor.MixTensor object at 0x7f0f84d44880>))
2023-10-30 12:29:47,184 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 12:29:47,184 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:47,184 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,188 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,188 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,189 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,189 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:47,189 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,189 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dbd0>
2023-10-30 12:29:47,189 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:47,189 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,190 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d030>
2023-10-30 12:29:47,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:47,190 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,190 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d45030>
2023-10-30 12:29:47,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:47,190 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,191 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d445e0>
2023-10-30 12:29:47,191 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,191 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:47,191 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,192 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,192 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,192 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,192 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:47,192 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,200 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ccd0>
2023-10-30 12:29:47,200 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:47,201 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,206 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d990>
2023-10-30 12:29:47,207 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:47,207 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,213 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c0d0>
2023-10-30 12:29:47,213 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:47,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,219 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c6a0>
2023-10-30 12:29:47,220 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:47,221 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:47,221 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,222 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,222 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:47,222 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,223 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:47,223 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,223 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f010>
2023-10-30 12:29:47,223 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:47,223 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,223 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f4f0>
2023-10-30 12:29:47,224 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:47,224 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,224 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f130>
2023-10-30 12:29:47,224 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:47,224 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,224 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ca00>
2023-10-30 12:29:47,225 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,225 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:47,225 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,229 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,230 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-30 12:29:47,230 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,230 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:47,230 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs_k: {}
2023-10-30 12:29:47,231 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565bb50>
2023-10-30 12:29:47,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:47,231 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs_k: {}
2023-10-30 12:29:47,231 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e740>
2023-10-30 12:29:47,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:47,231 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs_k: {}
2023-10-30 12:29:47,232 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4efb0>
2023-10-30 12:29:47,232 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:47,232 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs_k: {}
2023-10-30 12:29:47,232 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565bac0>
2023-10-30 12:29:47,232 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,233 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:47,233 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,237 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,241 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,241 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,241 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:47,241 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,244 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f84d4f5b0>))
2023-10-30 12:29:47,245 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:47,245 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,246 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb60>, (<mixtensor.MixTensor object at 0x7f0f84d4cbb0>, <mixtensor.MixTensor object at 0x7f0f84d4c880>))
2023-10-30 12:29:47,246 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:47,247 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,248 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, (<mixtensor.MixTensor object at 0x7f0f84d4d060>, <mixtensor.MixTensor object at 0x7f0f84d4f040>))
2023-10-30 12:29:47,248 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:47,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,250 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f4f0>, (<mixtensor.MixTensor object at 0x7f0f84d4f340>, <mixtensor.MixTensor object at 0x7f0f84d4dd20>))
2023-10-30 12:29:47,250 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,250 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:47,251 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,255 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,259 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,259 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,259 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:47,259 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,263 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, <mixtensor.MixTensor object at 0x7f0f84d4e410>))
2023-10-30 12:29:47,263 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:47,264 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,265 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c370>, (<mixtensor.MixTensor object at 0x7f0f8572dc30>, <mixtensor.MixTensor object at 0x7f0f8572d6f0>))
2023-10-30 12:29:47,265 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:47,266 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,267 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572faf0>, (<mixtensor.MixTensor object at 0x7f0f8572fc40>, <mixtensor.MixTensor object at 0x7f0f8572cb80>))
2023-10-30 12:29:47,267 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:47,267 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,269 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, <mixtensor.MixTensor object at 0x7f0f84d4e710>))
2023-10-30 12:29:47,269 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,269 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:47,270 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,274 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,277 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,277 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,277 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:47,278 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,280 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e410>, <mixtensor.MixTensor object at 0x7f0f8572d630>))
2023-10-30 12:29:47,280 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:47,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,282 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd20>, (<mixtensor.MixTensor object at 0x7f0f8572c640>, <mixtensor.MixTensor object at 0x7f0f8572c5e0>))
2023-10-30 12:29:47,282 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:47,283 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,284 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572c970>, <mixtensor.MixTensor object at 0x7f0f8572c8e0>))
2023-10-30 12:29:47,284 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:47,285 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,286 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fa30>, (<mixtensor.MixTensor object at 0x7f0f8572d630>, <mixtensor.MixTensor object at 0x7f0f8572cfd0>))
2023-10-30 12:29:47,286 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,286 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:47,287 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,291 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,295 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,295 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,295 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:47,296 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,299 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c370>, (<mixtensor.MixTensor object at 0x7f0f8572faf0>, <mixtensor.MixTensor object at 0x7f0f8572c0a0>))
2023-10-30 12:29:47,299 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:47,299 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,301 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c610>, (<mixtensor.MixTensor object at 0x7f0f8572c580>, <mixtensor.MixTensor object at 0x7f0f8572c220>))
2023-10-30 12:29:47,301 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:47,301 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,302 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c850>, (<mixtensor.MixTensor object at 0x7f0f8572ca30>, <mixtensor.MixTensor object at 0x7f0f8572dc00>))
2023-10-30 12:29:47,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:47,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,304 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c0a0>, (<mixtensor.MixTensor object at 0x7f0f8572cb50>, <mixtensor.MixTensor object at 0x7f0f8572d660>))
2023-10-30 12:29:47,304 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,305 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:47,305 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,310 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,313 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,313 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,314 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:47,314 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,317 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572c370>, <mixtensor.MixTensor object at 0x7f0f8572d390>))
2023-10-30 12:29:47,317 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:47,317 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,319 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c9a0>, (<mixtensor.MixTensor object at 0x7f0f8572fcd0>, <mixtensor.MixTensor object at 0x7f0f8572d2d0>))
2023-10-30 12:29:47,319 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:47,319 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,321 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d180>, (<mixtensor.MixTensor object at 0x7f0f8572cca0>, <mixtensor.MixTensor object at 0x7f0f8572c8b0>))
2023-10-30 12:29:47,321 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:47,321 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,323 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cc10>, (<mixtensor.MixTensor object at 0x7f0f8572dd20>, <mixtensor.MixTensor object at 0x7f0f8572cc70>))
2023-10-30 12:29:47,323 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,323 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:47,323 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,328 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,331 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,332 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,332 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:47,332 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,335 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572c370>, <mixtensor.MixTensor object at 0x7f0f8572fc10>))
2023-10-30 12:29:47,335 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:47,335 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,337 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d900>, (<mixtensor.MixTensor object at 0x7f0f8572d450>, <mixtensor.MixTensor object at 0x7f0f8572d2a0>))
2023-10-30 12:29:47,337 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:47,337 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,338 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d960>, (<mixtensor.MixTensor object at 0x7f0f8572ceb0>, <mixtensor.MixTensor object at 0x7f0f8572dc90>))
2023-10-30 12:29:47,339 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:47,339 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,340 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c850>, (<mixtensor.MixTensor object at 0x7f0f8572c0a0>, <mixtensor.MixTensor object at 0x7f0f8572fd00>))
2023-10-30 12:29:47,340 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,340 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:47,341 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,345 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,348 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,348 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,349 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:47,349 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,351 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572c370>, <mixtensor.MixTensor object at 0x7f0f8572c250>))
2023-10-30 12:29:47,352 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:47,352 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,353 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dbd0>, (<mixtensor.MixTensor object at 0x7f0f8572d030>, <mixtensor.MixTensor object at 0x7f0f8572d990>))
2023-10-30 12:29:47,354 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:47,354 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,355 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c0d0>, (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, <mixtensor.MixTensor object at 0x7f0f8572d240>))
2023-10-30 12:29:47,355 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:47,356 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,357 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cc10>, (<mixtensor.MixTensor object at 0x7f0f8572c610>, <mixtensor.MixTensor object at 0x7f0f8572d8a0>))
2023-10-30 12:29:47,357 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,357 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:47,358 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,362 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,365 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,365 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,366 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:47,366 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,368 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572c370>, <mixtensor.MixTensor object at 0x7f0f8572fa30>))
2023-10-30 12:29:47,369 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:47,369 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,370 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d060>, (<mixtensor.MixTensor object at 0x7f0f8572ca60>, <mixtensor.MixTensor object at 0x7f0f8572cbe0>))
2023-10-30 12:29:47,371 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:47,371 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,372 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447f0>, (<mixtensor.MixTensor object at 0x7f0f84d451e0>, <mixtensor.MixTensor object at 0x7f0f84d444f0>))
2023-10-30 12:29:47,372 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:47,373 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,374 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d390>, (<mixtensor.MixTensor object at 0x7f0f8572d180>, <mixtensor.MixTensor object at 0x7f0f8572ccd0>))
2023-10-30 12:29:47,374 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,374 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:47,375 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,379 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,382 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,383 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,383 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:47,383 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,386 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c370>, (<mixtensor.MixTensor object at 0x7f0f8572fa30>, <mixtensor.MixTensor object at 0x7f0f84d44940>))
2023-10-30 12:29:47,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:47,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,388 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44580>, (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, <mixtensor.MixTensor object at 0x7f0f8565b1f0>))
2023-10-30 12:29:47,388 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:47,388 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,390 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c850>, (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, <mixtensor.MixTensor object at 0x7f0f8572d600>))
2023-10-30 12:29:47,390 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:47,390 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,392 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c370>, (<mixtensor.MixTensor object at 0x7f0f8572fa30>, <mixtensor.MixTensor object at 0x7f0f8572d510>))
2023-10-30 12:29:47,392 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,392 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:47,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,397 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,400 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,400 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,401 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:47,401 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,404 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cf10>, (<mixtensor.MixTensor object at 0x7f0f84d4c910>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:47,404 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:47,404 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,406 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc40>, (<mixtensor.MixTensor object at 0x7f0f84d4fa30>, <mixtensor.MixTensor object at 0x7f0f84d4eb00>))
2023-10-30 12:29:47,406 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:47,406 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,408 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb60>, (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, <mixtensor.MixTensor object at 0x7f0f84d4c370>))
2023-10-30 12:29:47,408 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:47,408 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,410 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e200>, (<mixtensor.MixTensor object at 0x7f0f84d4f550>, <mixtensor.MixTensor object at 0x7f0f84d4e080>))
2023-10-30 12:29:47,410 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,410 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:47,410 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,415 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,418 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,419 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,419 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:47,419 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,423 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c910>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f84d4c640>))
2023-10-30 12:29:47,423 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:47,423 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,425 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, (<mixtensor.MixTensor object at 0x7f0f84d4d930>, <mixtensor.MixTensor object at 0x7f0f84d4f6a0>))
2023-10-30 12:29:47,425 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:47,425 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,426 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f520>, (<mixtensor.MixTensor object at 0x7f0f84d4c9a0>, <mixtensor.MixTensor object at 0x7f0f84d4f970>))
2023-10-30 12:29:47,426 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:47,427 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,428 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d900>, (<mixtensor.MixTensor object at 0x7f0f84d4fa90>, <mixtensor.MixTensor object at 0x7f0f84d4f4f0>))
2023-10-30 12:29:47,428 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,428 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:47,429 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,429 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,433 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,433 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,433 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:47,434 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,436 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c910>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f84d4f490>))
2023-10-30 12:29:47,436 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:47,437 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,438 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44580>, (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, <mixtensor.MixTensor object at 0x7f0f84d45000>))
2023-10-30 12:29:47,438 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:47,439 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,440 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45030>, (<mixtensor.MixTensor object at 0x7f0f84d445e0>, <mixtensor.MixTensor object at 0x7f0f84d446a0>))
2023-10-30 12:29:47,440 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:47,440 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,442 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4cf10>, <mixtensor.MixTensor object at 0x7f0f84d4d570>))
2023-10-30 12:29:47,442 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 12:29:47,442 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:47,443 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,446 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,446 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,446 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,447 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:47,447 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,447 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f520>
2023-10-30 12:29:47,447 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:47,447 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,447 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e0e0>
2023-10-30 12:29:47,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:47,448 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,448 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fcd0>
2023-10-30 12:29:47,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:47,448 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,448 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f130>
2023-10-30 12:29:47,449 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,449 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:47,449 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,449 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,450 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,450 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,450 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:47,450 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,457 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572db10>
2023-10-30 12:29:47,458 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:47,458 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,463 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4efb0>
2023-10-30 12:29:47,463 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:47,463 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,469 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f010>
2023-10-30 12:29:47,469 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:47,469 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,474 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f5b0>
2023-10-30 12:29:47,475 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:47,476 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:47,477 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,477 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,478 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:47,478 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,478 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:47,478 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,478 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d750>
2023-10-30 12:29:47,478 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:47,479 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,479 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565bb80>
2023-10-30 12:29:47,479 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:47,479 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,479 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565ba60>
2023-10-30 12:29:47,479 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:47,479 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,480 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565ada0>
2023-10-30 12:29:47,480 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,480 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:47,480 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,485 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,485 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-30 12:29:47,485 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,485 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:47,486 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs_k: {}
2023-10-30 12:29:47,486 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d4b0>
2023-10-30 12:29:47,486 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:47,486 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs_k: {}
2023-10-30 12:29:47,486 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cfa0>
2023-10-30 12:29:47,487 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:47,487 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs_k: {}
2023-10-30 12:29:47,487 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e3b0>
2023-10-30 12:29:47,487 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:47,487 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs_k: {}
2023-10-30 12:29:47,488 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d5a0>
2023-10-30 12:29:47,488 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,488 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:47,488 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,492 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,496 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,496 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,496 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:47,497 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,500 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e890>, (<mixtensor.MixTensor object at 0x7f0f84d4e680>, <mixtensor.MixTensor object at 0x7f0f84d4f100>))
2023-10-30 12:29:47,500 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:47,500 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,502 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f460>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4f640>))
2023-10-30 12:29:47,502 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:47,502 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,517 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4ffd0>, <mixtensor.MixTensor object at 0x7f0f84d4ea40>))
2023-10-30 12:29:47,517 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:47,517 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,519 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c670>, (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, <mixtensor.MixTensor object at 0x7f0f84d4e140>))
2023-10-30 12:29:47,519 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,520 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:47,520 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,525 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,528 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,528 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,528 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:47,528 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,531 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e590>, (<mixtensor.MixTensor object at 0x7f0f84d4cb20>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:47,532 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:47,532 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,534 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c700>, (<mixtensor.MixTensor object at 0x7f0f84d4c4f0>, <mixtensor.MixTensor object at 0x7f0f84d4df60>))
2023-10-30 12:29:47,534 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:47,534 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,536 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d420>, (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, <mixtensor.MixTensor object at 0x7f0f84d4f520>))
2023-10-30 12:29:47,536 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:47,536 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,539 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4de40>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4ce80>))
2023-10-30 12:29:47,539 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,539 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:47,540 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,544 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,548 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,548 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,548 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:47,548 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,551 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4cb20>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:47,552 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:47,552 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,553 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fcd0>, (<mixtensor.MixTensor object at 0x7f0f84d4f130>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:47,554 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:47,554 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,555 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4ec80>, <mixtensor.MixTensor object at 0x7f0f84d4eb60>))
2023-10-30 12:29:47,555 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:47,555 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,557 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e890>, (<mixtensor.MixTensor object at 0x7f0f84d4e590>, <mixtensor.MixTensor object at 0x7f0f84d4e0e0>))
2023-10-30 12:29:47,557 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,557 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:47,558 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,562 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,566 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,566 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,566 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:47,567 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,569 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cb20>, (<mixtensor.MixTensor object at 0x7f0f84d4fc10>, <mixtensor.MixTensor object at 0x7f0f84d4c910>))
2023-10-30 12:29:47,570 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:47,570 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,571 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d900>, (<mixtensor.MixTensor object at 0x7f0f84d4c640>, <mixtensor.MixTensor object at 0x7f0f84d44190>))
2023-10-30 12:29:47,572 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:47,572 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,573 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44310>, (<mixtensor.MixTensor object at 0x7f0f84d44370>, <mixtensor.MixTensor object at 0x7f0f84d44220>))
2023-10-30 12:29:47,574 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:47,574 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,576 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, <mixtensor.MixTensor object at 0x7f0f84d4ddb0>))
2023-10-30 12:29:47,576 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,576 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:47,577 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,581 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,585 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,585 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,585 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:47,585 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,589 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc10>, (<mixtensor.MixTensor object at 0x7f0f84d4c910>, <mixtensor.MixTensor object at 0x7f0f84d44af0>))
2023-10-30 12:29:47,589 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:47,589 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,591 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44100>, (<mixtensor.MixTensor object at 0x7f0f84d441f0>, <mixtensor.MixTensor object at 0x7f0f84d44340>))
2023-10-30 12:29:47,591 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:47,592 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,593 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44910>, (<mixtensor.MixTensor object at 0x7f0f84d44460>, <mixtensor.MixTensor object at 0x7f0f84d44400>))
2023-10-30 12:29:47,593 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:47,594 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,596 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9d0>, (<mixtensor.MixTensor object at 0x7f0f84d44af0>, <mixtensor.MixTensor object at 0x7f0f84d44820>))
2023-10-30 12:29:47,596 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,597 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:47,597 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,602 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:47,605 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,605 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,606 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:47,606 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,610 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d442b0>, (<mixtensor.MixTensor object at 0x7f0f84d44310>, <mixtensor.MixTensor object at 0x7f0f84d44850>))
2023-10-30 12:29:47,610 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:47,611 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,613 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ca0>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44dc0>))
2023-10-30 12:29:47,613 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:47,613 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,614 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d449a0>, (<mixtensor.MixTensor object at 0x7f0f84d448b0>, <mixtensor.MixTensor object at 0x7f0f84d45270>))
2023-10-30 12:29:47,615 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:47,615 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,617 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44310>, (<mixtensor.MixTensor object at 0x7f0f84d44850>, <mixtensor.MixTensor object at 0x7f0f84d44700>))
2023-10-30 12:29:47,617 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,617 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:47,618 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,622 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:47,625 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,626 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,626 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:47,626 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,629 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44100>, (<mixtensor.MixTensor object at 0x7f0f84d44910>, <mixtensor.MixTensor object at 0x7f0f84d44fa0>))
2023-10-30 12:29:47,629 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:47,629 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,631 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44430>, (<mixtensor.MixTensor object at 0x7f0f84d446d0>, <mixtensor.MixTensor object at 0x7f0f84d45390>))
2023-10-30 12:29:47,631 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:47,631 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,633 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44550>, (<mixtensor.MixTensor object at 0x7f0f84d45360>, <mixtensor.MixTensor object at 0x7f0f84d45120>))
2023-10-30 12:29:47,633 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:47,633 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,634 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44fa0>, (<mixtensor.MixTensor object at 0x7f0f84d44970>, <mixtensor.MixTensor object at 0x7f0f84d454b0>))
2023-10-30 12:29:47,635 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,635 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:47,635 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,640 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:47,643 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,643 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:47,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,647 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44310>, (<mixtensor.MixTensor object at 0x7f0f84d44100>, <mixtensor.MixTensor object at 0x7f0f84d44130>))
2023-10-30 12:29:47,647 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:47,647 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,648 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d00>, (<mixtensor.MixTensor object at 0x7f0f84d44e50>, <mixtensor.MixTensor object at 0x7f0f84d44d60>))
2023-10-30 12:29:47,649 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:47,649 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,650 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44be0>, (<mixtensor.MixTensor object at 0x7f0f84d440a0>, <mixtensor.MixTensor object at 0x7f0f84d44730>))
2023-10-30 12:29:47,650 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:47,650 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,652 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ca0>, (<mixtensor.MixTensor object at 0x7f0f84d449a0>, <mixtensor.MixTensor object at 0x7f0f84d45060>))
2023-10-30 12:29:47,652 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,652 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:47,653 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,657 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:47,660 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,660 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,661 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:47,661 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,663 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44310>, (<mixtensor.MixTensor object at 0x7f0f84d44100>, <mixtensor.MixTensor object at 0x7f0f84d44160>))
2023-10-30 12:29:47,663 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:47,664 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,666 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d44640>, <mixtensor.MixTensor object at 0x7f0f84d4f9d0>))
2023-10-30 12:29:47,666 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:47,666 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,667 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e200>, (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, <mixtensor.MixTensor object at 0x7f0f84d4e890>))
2023-10-30 12:29:47,668 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:47,668 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,669 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c970>, (<mixtensor.MixTensor object at 0x7f0f84d4f490>, <mixtensor.MixTensor object at 0x7f0f84d4d420>))
2023-10-30 12:29:47,669 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,670 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:47,670 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,674 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:47,678 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,678 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,678 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:47,678 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,681 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d210>, (<mixtensor.MixTensor object at 0x7f0f8572fc10>, <mixtensor.MixTensor object at 0x7f0f8572c760>))
2023-10-30 12:29:47,681 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:47,681 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,683 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, (<mixtensor.MixTensor object at 0x7f0f8565b820>, <mixtensor.MixTensor object at 0x7f0f8565b220>))
2023-10-30 12:29:47,683 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:47,683 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,685 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658c40>, (<mixtensor.MixTensor object at 0x7f0f8565b310>, <mixtensor.MixTensor object at 0x7f0f8565ada0>))
2023-10-30 12:29:47,685 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:47,685 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,686 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa90>, (<mixtensor.MixTensor object at 0x7f0f8572c730>, <mixtensor.MixTensor object at 0x7f0f8572c9a0>))
2023-10-30 12:29:47,687 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,687 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:47,687 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,691 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:47,695 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,695 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,695 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:47,696 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,698 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, (<mixtensor.MixTensor object at 0x7f0f8565b130>, <mixtensor.MixTensor object at 0x7f0f8565b790>))
2023-10-30 12:29:47,698 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:47,699 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,700 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b80>, (<mixtensor.MixTensor object at 0x7f0f84d44430>, <mixtensor.MixTensor object at 0x7f0f84d44a30>))
2023-10-30 12:29:47,700 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:47,700 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,702 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d00>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d44ca0>))
2023-10-30 12:29:47,702 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:47,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,704 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d900>, (<mixtensor.MixTensor object at 0x7f0f8572fc10>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:47,704 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,704 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:47,704 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,705 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:47,709 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,709 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,709 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:47,709 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,719 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, (<mixtensor.MixTensor object at 0x7f0f8565b790>, <mixtensor.MixTensor object at 0x7f0f84d44310>))
2023-10-30 12:29:47,719 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:47,719 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,721 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44160>, (<mixtensor.MixTensor object at 0x7f0f84d44910>, <mixtensor.MixTensor object at 0x7f0f84d44760>))
2023-10-30 12:29:47,721 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:47,722 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,723 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44cd0>, (<mixtensor.MixTensor object at 0x7f0f84d451b0>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:47,723 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:47,724 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,725 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d210>, (<mixtensor.MixTensor object at 0x7f0f84d44310>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:47,725 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 12:29:47,726 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:47,726 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,730 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:47,731 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,731 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,731 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:47,731 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,732 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c760>
2023-10-30 12:29:47,732 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:47,732 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,732 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d900>
2023-10-30 12:29:47,733 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:47,733 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,733 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44b80>
2023-10-30 12:29:47,733 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:47,734 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,734 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d442b0>
2023-10-30 12:29:47,734 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,734 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:47,735 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,735 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:47,736 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,736 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,736 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:47,736 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,744 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e3b0>
2023-10-30 12:29:47,745 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:47,745 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,751 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cf40>
2023-10-30 12:29:47,752 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:47,752 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,758 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f85658190>
2023-10-30 12:29:47,758 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:47,758 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:47,763 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f85658c40>
2023-10-30 12:29:47,764 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:47,765 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:47,765 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,766 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:47,767 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:47,767 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,767 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:47,767 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,767 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fee0>
2023-10-30 12:29:47,767 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:47,768 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,768 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4da80>
2023-10-30 12:29:47,768 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:47,768 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,768 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fa30>
2023-10-30 12:29:47,768 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:47,769 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:47,769 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f9a0>
2023-10-30 12:29:47,769 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,769 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:47,769 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,774 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:47,774 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-30 12:29:47,774 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:47,774 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:47,775 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs_k: {}
2023-10-30 12:29:47,775 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4dd20>
2023-10-30 12:29:47,775 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:47,775 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs_k: {}
2023-10-30 12:29:47,776 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f4f0>
2023-10-30 12:29:47,776 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:47,776 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs_k: {}
2023-10-30 12:29:47,776 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cf10>
2023-10-30 12:29:47,776 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:47,777 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs_k: {}
2023-10-30 12:29:47,777 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cbb0>
2023-10-30 12:29:47,777 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:47,777 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:47,778 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,782 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:47,785 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,785 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,786 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:47,786 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,789 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c430>, (<mixtensor.MixTensor object at 0x7f0f8572d6f0>, <mixtensor.MixTensor object at 0x7f0f8572cb80>))
2023-10-30 12:29:47,789 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:47,789 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,791 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c640>, (<mixtensor.MixTensor object at 0x7f0f8572c970>, <mixtensor.MixTensor object at 0x7f0f8572d630>))
2023-10-30 12:29:47,791 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:47,791 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,792 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cdf0>, (<mixtensor.MixTensor object at 0x7f0f8572c9d0>, <mixtensor.MixTensor object at 0x7f0f8572c5e0>))
2023-10-30 12:29:47,793 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:47,793 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,943 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, (<mixtensor.MixTensor object at 0x7f0f8572cee0>, <mixtensor.MixTensor object at 0x7f0f8572c340>))
2023-10-30 12:29:47,944 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:47,944 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:47,944 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:47,952 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,953 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,953 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:47,953 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,956 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c580>, (<mixtensor.MixTensor object at 0x7f0f8572c220>, <mixtensor.MixTensor object at 0x7f0f8572dc00>))
2023-10-30 12:29:47,956 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:47,956 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,958 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd90>, (<mixtensor.MixTensor object at 0x7f0f8572fcd0>, <mixtensor.MixTensor object at 0x7f0f8572cca0>))
2023-10-30 12:29:47,958 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:47,958 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,959 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd20>, (<mixtensor.MixTensor object at 0x7f0f8572cf70>, <mixtensor.MixTensor object at 0x7f0f8572dcf0>))
2023-10-30 12:29:47,960 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:47,960 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,962 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d810>, (<mixtensor.MixTensor object at 0x7f0f8572d660>, <mixtensor.MixTensor object at 0x7f0f8572c1c0>))
2023-10-30 12:29:47,962 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:47,962 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:47,962 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,967 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:47,970 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,971 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,971 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:47,971 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,975 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd80>, (<mixtensor.MixTensor object at 0x7f0f8572c220>, <mixtensor.MixTensor object at 0x7f0f8572dc00>))
2023-10-30 12:29:47,975 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:47,975 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,977 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cc70>, (<mixtensor.MixTensor object at 0x7f0f8572db40>, <mixtensor.MixTensor object at 0x7f0f8572cf10>))
2023-10-30 12:29:47,977 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:47,978 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,980 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d840>, (<mixtensor.MixTensor object at 0x7f0f8572d2a0>, <mixtensor.MixTensor object at 0x7f0f8572dc90>))
2023-10-30 12:29:47,980 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:47,980 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,982 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c430>, (<mixtensor.MixTensor object at 0x7f0f8572c580>, <mixtensor.MixTensor object at 0x7f0f8572c8b0>))
2023-10-30 12:29:47,983 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:47,983 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:47,983 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:47,988 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:47,991 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:47,991 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:47,992 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,994 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c220>, (<mixtensor.MixTensor object at 0x7f0f8572dc00>, <mixtensor.MixTensor object at 0x7f0f8572d570>))
2023-10-30 12:29:47,994 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:47,995 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,996 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d030>, (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, <mixtensor.MixTensor object at 0x7f0f8572d240>))
2023-10-30 12:29:47,996 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:47,997 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:47,998 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d8a0>, (<mixtensor.MixTensor object at 0x7f0f8572ca60>, <mixtensor.MixTensor object at 0x7f0f8572d180>))
2023-10-30 12:29:47,998 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:47,998 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,000 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c8e0>, (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, <mixtensor.MixTensor object at 0x7f0f8572d690>))
2023-10-30 12:29:48,000 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,000 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:48,000 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,005 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,008 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,008 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,009 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:48,009 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,011 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dc00>, (<mixtensor.MixTensor object at 0x7f0f8572d570>, <mixtensor.MixTensor object at 0x7f0f8572ccd0>))
2023-10-30 12:29:48,011 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:48,012 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,013 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa30>, (<mixtensor.MixTensor object at 0x7f0f8572d600>, <mixtensor.MixTensor object at 0x7f0f8572d900>))
2023-10-30 12:29:48,013 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:48,014 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,015 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d7e0>, (<mixtensor.MixTensor object at 0x7f0f8572cd30>, <mixtensor.MixTensor object at 0x7f0f8572d9f0>))
2023-10-30 12:29:48,015 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:48,015 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,017 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d2d0>, (<mixtensor.MixTensor object at 0x7f0f8572d810>, <mixtensor.MixTensor object at 0x7f0f8572c2e0>))
2023-10-30 12:29:48,017 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,017 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:48,018 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,022 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,025 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,026 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,026 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:48,026 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,029 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d570>, (<mixtensor.MixTensor object at 0x7f0f8572ccd0>, <mixtensor.MixTensor object at 0x7f0f8572d270>))
2023-10-30 12:29:48,029 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:48,029 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,031 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf40>, (<mixtensor.MixTensor object at 0x7f0f8572d750>, <mixtensor.MixTensor object at 0x7f0f8565ad70>))
2023-10-30 12:29:48,031 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:48,031 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,033 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba30>, (<mixtensor.MixTensor object at 0x7f0f8565ba00>, <mixtensor.MixTensor object at 0x7f0f8565b0d0>))
2023-10-30 12:29:48,033 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:48,033 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,034 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d840>, (<mixtensor.MixTensor object at 0x7f0f8572ce80>, <mixtensor.MixTensor object at 0x7f0f8572d4b0>))
2023-10-30 12:29:48,035 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,035 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:48,035 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,039 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,043 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,043 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,043 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:48,043 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,047 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ccd0>, (<mixtensor.MixTensor object at 0x7f0f8572d270>, <mixtensor.MixTensor object at 0x7f0f8565b100>))
2023-10-30 12:29:48,047 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:48,047 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,049 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bac0>, (<mixtensor.MixTensor object at 0x7f0f8565b9a0>, <mixtensor.MixTensor object at 0x7f0f85658190>))
2023-10-30 12:29:48,049 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:48,049 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,050 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658c40>, (<mixtensor.MixTensor object at 0x7f0f8565b610>, <mixtensor.MixTensor object at 0x7f0f8565b2e0>))
2023-10-30 12:29:48,051 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:48,051 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,052 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cdf0>, (<mixtensor.MixTensor object at 0x7f0f8565b100>, <mixtensor.MixTensor object at 0x7f0f8565bc40>))
2023-10-30 12:29:48,052 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,052 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:48,053 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,057 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,060 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,061 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,061 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:48,061 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,064 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b6d0>, (<mixtensor.MixTensor object at 0x7f0f8565ba30>, <mixtensor.MixTensor object at 0x7f0f8565ba60>))
2023-10-30 12:29:48,064 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:48,064 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,066 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, <mixtensor.MixTensor object at 0x7f0f84d44e20>))
2023-10-30 12:29:48,066 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:48,066 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,067 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d444f0>, (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, <mixtensor.MixTensor object at 0x7f0f84d447f0>))
2023-10-30 12:29:48,068 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:48,068 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,069 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d270>, (<mixtensor.MixTensor object at 0x7f0f8572c910>, <mixtensor.MixTensor object at 0x7f0f8572d570>))
2023-10-30 12:29:48,069 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,069 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:48,070 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,074 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,078 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,078 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,078 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:48,078 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,082 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b6d0>, (<mixtensor.MixTensor object at 0x7f0f8565ba60>, <mixtensor.MixTensor object at 0x7f0f84d44ee0>))
2023-10-30 12:29:48,082 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:48,082 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,084 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447c0>, (<mixtensor.MixTensor object at 0x7f0f84d445b0>, <mixtensor.MixTensor object at 0x7f0f8565ba30>))
2023-10-30 12:29:48,084 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:48,085 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,086 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d8d0>, (<mixtensor.MixTensor object at 0x7f0f8572dd20>, <mixtensor.MixTensor object at 0x7f0f8572cfd0>))
2023-10-30 12:29:48,086 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:48,086 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,088 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d2d0>, (<mixtensor.MixTensor object at 0x7f0f8572cbb0>, <mixtensor.MixTensor object at 0x7f0f8572dc30>))
2023-10-30 12:29:48,088 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,088 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:48,089 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,093 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,096 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,097 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,097 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:48,097 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,100 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, (<mixtensor.MixTensor object at 0x7f0f84d4c5b0>, <mixtensor.MixTensor object at 0x7f0f84d4c820>))
2023-10-30 12:29:48,100 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:48,100 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,101 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d300>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4c700>))
2023-10-30 12:29:48,102 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:48,102 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,103 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eaa0>, (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, <mixtensor.MixTensor object at 0x7f0f84d4d450>))
2023-10-30 12:29:48,103 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:48,103 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,105 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d240>, (<mixtensor.MixTensor object at 0x7f0f84d4f850>, <mixtensor.MixTensor object at 0x7f0f84d4cc40>))
2023-10-30 12:29:48,105 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,105 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:48,106 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,110 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,113 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,113 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,113 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:48,113 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,116 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c820>, (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, <mixtensor.MixTensor object at 0x7f0f84d4f3a0>))
2023-10-30 12:29:48,118 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:48,118 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,120 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe80>, (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, <mixtensor.MixTensor object at 0x7f0f84d4d060>))
2023-10-30 12:29:48,120 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:48,120 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,121 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f460>, (<mixtensor.MixTensor object at 0x7f0f84d4e380>, <mixtensor.MixTensor object at 0x7f0f84d4dfc0>))
2023-10-30 12:29:48,122 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:48,122 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,123 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, (<mixtensor.MixTensor object at 0x7f0f84d4c5b0>, <mixtensor.MixTensor object at 0x7f0f84d4f760>))
2023-10-30 12:29:48,123 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,123 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:48,124 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,125 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,128 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,128 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,128 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:48,128 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,131 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f3a0>, <mixtensor.MixTensor object at 0x7f0f84d4e3e0>))
2023-10-30 12:29:48,131 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:48,131 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,133 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c1c0>, (<mixtensor.MixTensor object at 0x7f0f84d447c0>, <mixtensor.MixTensor object at 0x7f0f84d45000>))
2023-10-30 12:29:48,133 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:48,133 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,134 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446a0>, (<mixtensor.MixTensor object at 0x7f0f84d44b80>, <mixtensor.MixTensor object at 0x7f0f84d442b0>))
2023-10-30 12:29:48,134 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:48,135 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,136 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cb20>, (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:48,136 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 12:29:48,136 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:48,137 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,140 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,141 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,141 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,141 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:48,141 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,141 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c670>
2023-10-30 12:29:48,142 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:48,142 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,142 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d240>
2023-10-30 12:29:48,142 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:48,142 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,142 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f6a0>
2023-10-30 12:29:48,142 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:48,143 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,143 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d330>
2023-10-30 12:29:48,143 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,143 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:48,143 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,144 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,144 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,144 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,144 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:48,144 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,151 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f4f0>
2023-10-30 12:29:48,151 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:48,151 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,156 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4efe0>
2023-10-30 12:29:48,157 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:48,157 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,163 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4db70>
2023-10-30 12:29:48,163 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:48,163 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,169 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4da80>
2023-10-30 12:29:48,169 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:48,171 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:48,171 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,171 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,172 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:48,172 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,172 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:48,172 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,172 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c9a0>
2023-10-30 12:29:48,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:48,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,173 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c250>
2023-10-30 12:29:48,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:48,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,173 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d570>
2023-10-30 12:29:48,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:48,174 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,175 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f550>
2023-10-30 12:29:48,175 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,176 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:48,176 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,180 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-30 12:29:48,181 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,181 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:48,181 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs_k: {}
2023-10-30 12:29:48,181 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565ada0>
2023-10-30 12:29:48,181 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:48,181 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs_k: {}
2023-10-30 12:29:48,182 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4dd20>
2023-10-30 12:29:48,182 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:48,182 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs_k: {}
2023-10-30 12:29:48,182 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fa90>
2023-10-30 12:29:48,183 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:48,183 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs_k: {}
2023-10-30 12:29:48,183 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565ba90>
2023-10-30 12:29:48,183 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,184 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:48,184 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,188 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,191 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,191 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,191 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:48,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,194 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e770>, (<mixtensor.MixTensor object at 0x7f0f84d4c4f0>, <mixtensor.MixTensor object at 0x7f0f84d4f5e0>))
2023-10-30 12:29:48,194 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:48,194 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,196 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f010>, (<mixtensor.MixTensor object at 0x7f0f84d4df60>, <mixtensor.MixTensor object at 0x7f0f84d4f520>))
2023-10-30 12:29:48,196 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:48,196 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,197 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:48,197 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:48,198 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,199 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f6d0>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f84d4efb0>))
2023-10-30 12:29:48,199 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,199 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:48,200 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,204 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,207 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,207 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,208 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:48,208 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,210 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f490>, <mixtensor.MixTensor object at 0x7f0f84d4f9d0>))
2023-10-30 12:29:48,211 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:48,211 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,212 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f100>, (<mixtensor.MixTensor object at 0x7f0f84d4c670>, <mixtensor.MixTensor object at 0x7f0f84d4d870>))
2023-10-30 12:29:48,212 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:48,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,216 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d240>, (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, <mixtensor.MixTensor object at 0x7f0f84d4d330>))
2023-10-30 12:29:48,216 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:48,216 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,218 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, (<mixtensor.MixTensor object at 0x7f0f84d4e890>, <mixtensor.MixTensor object at 0x7f0f84d4d420>))
2023-10-30 12:29:48,218 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,218 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:48,218 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,223 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,226 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,226 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,226 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:48,226 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,229 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb60>, (<mixtensor.MixTensor object at 0x7f0f84d4f490>, <mixtensor.MixTensor object at 0x7f0f84d4f9d0>))
2023-10-30 12:29:48,230 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:48,230 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,231 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4fa30>, <mixtensor.MixTensor object at 0x7f0f84d4efe0>))
2023-10-30 12:29:48,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:48,232 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,233 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c820>, (<mixtensor.MixTensor object at 0x7f0f84d4f4f0>, <mixtensor.MixTensor object at 0x7f0f84d4fee0>))
2023-10-30 12:29:48,233 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:48,233 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,234 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e770>, (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, <mixtensor.MixTensor object at 0x7f0f84d4ce20>))
2023-10-30 12:29:48,235 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,235 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:48,235 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,239 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,243 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,243 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,243 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:48,243 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,246 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f490>, (<mixtensor.MixTensor object at 0x7f0f84d4f9d0>, <mixtensor.MixTensor object at 0x7f0f84d4e8f0>))
2023-10-30 12:29:48,246 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:48,246 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,248 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e3e0>, (<mixtensor.MixTensor object at 0x7f0f84d4e200>, <mixtensor.MixTensor object at 0x7f0f84d443d0>))
2023-10-30 12:29:48,248 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:48,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,263 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440d0>, (<mixtensor.MixTensor object at 0x7f0f84d44d90>, <mixtensor.MixTensor object at 0x7f0f84d44ac0>))
2023-10-30 12:29:48,263 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:48,264 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,265 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, <mixtensor.MixTensor object at 0x7f0f84d4f670>))
2023-10-30 12:29:48,265 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,266 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:48,266 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,272 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,277 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,277 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,278 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:48,278 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,281 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:48,281 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:48,282 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,284 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44190>, (<mixtensor.MixTensor object at 0x7f0f84d44220>, <mixtensor.MixTensor object at 0x7f0f84d44af0>))
2023-10-30 12:29:48,284 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:48,284 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,286 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45240>, (<mixtensor.MixTensor object at 0x7f0f84d44c10>, <mixtensor.MixTensor object at 0x7f0f84d44340>))
2023-10-30 12:29:48,287 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:48,287 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,289 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f910>, (<mixtensor.MixTensor object at 0x7f0f84d44040>, <mixtensor.MixTensor object at 0x7f0f84d44280>))
2023-10-30 12:29:48,289 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,289 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:48,290 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,297 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,300 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,300 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,300 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:48,301 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,303 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44370>, (<mixtensor.MixTensor object at 0x7f0f84d440d0>, <mixtensor.MixTensor object at 0x7f0f84d44820>))
2023-10-30 12:29:48,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:48,304 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,305 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44fd0>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44dc0>))
2023-10-30 12:29:48,305 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:48,306 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,307 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45270>, (<mixtensor.MixTensor object at 0x7f0f84d44700>, <mixtensor.MixTensor object at 0x7f0f84d448e0>))
2023-10-30 12:29:48,307 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:48,307 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,309 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440d0>, (<mixtensor.MixTensor object at 0x7f0f84d44820>, <mixtensor.MixTensor object at 0x7f0f84d44f40>))
2023-10-30 12:29:48,309 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,309 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:48,309 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,314 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,318 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,319 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,319 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:48,319 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,322 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44190>, (<mixtensor.MixTensor object at 0x7f0f84d45240>, <mixtensor.MixTensor object at 0x7f0f84d446d0>))
2023-10-30 12:29:48,322 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:48,323 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,325 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44970>, (<mixtensor.MixTensor object at 0x7f0f84d450f0>, <mixtensor.MixTensor object at 0x7f0f84d45090>))
2023-10-30 12:29:48,325 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:48,325 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,327 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44070>, (<mixtensor.MixTensor object at 0x7f0f84d44e50>, <mixtensor.MixTensor object at 0x7f0f84d440a0>))
2023-10-30 12:29:48,328 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:48,328 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,330 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446d0>, (<mixtensor.MixTensor object at 0x7f0f84d44400>, <mixtensor.MixTensor object at 0x7f0f84d45360>))
2023-10-30 12:29:48,330 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,330 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:48,331 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,337 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,342 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,343 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,343 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:48,343 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,346 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440d0>, (<mixtensor.MixTensor object at 0x7f0f84d44190>, <mixtensor.MixTensor object at 0x7f0f84d44a90>))
2023-10-30 12:29:48,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:48,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,348 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d60>, (<mixtensor.MixTensor object at 0x7f0f84d44730>, <mixtensor.MixTensor object at 0x7f0f84d44550>))
2023-10-30 12:29:48,348 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:48,349 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,350 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44430>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d441c0>))
2023-10-30 12:29:48,350 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:48,350 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,352 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44fd0>, (<mixtensor.MixTensor object at 0x7f0f84d45270>, <mixtensor.MixTensor object at 0x7f0f84d44490>))
2023-10-30 12:29:48,352 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,352 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:48,352 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,357 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,360 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,360 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,361 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:48,361 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,363 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440d0>, (<mixtensor.MixTensor object at 0x7f0f84d44190>, <mixtensor.MixTensor object at 0x7f0f84d44a30>))
2023-10-30 12:29:48,364 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:48,364 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,366 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d45210>, <mixtensor.MixTensor object at 0x7f0f84d4d540>))
2023-10-30 12:29:48,366 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:48,366 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,368 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e260>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4ffd0>))
2023-10-30 12:29:48,368 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:48,368 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,369 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e590>, (<mixtensor.MixTensor object at 0x7f0f84d4f580>, <mixtensor.MixTensor object at 0x7f0f84d4ea40>))
2023-10-30 12:29:48,369 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,370 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:48,370 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,374 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,378 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,378 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:48,379 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,381 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dab0>, (<mixtensor.MixTensor object at 0x7f0f8572fd60>, <mixtensor.MixTensor object at 0x7f0f8572ccd0>))
2023-10-30 12:29:48,381 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:48,381 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,470 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bb50>, (<mixtensor.MixTensor object at 0x7f0f8565bac0>, <mixtensor.MixTensor object at 0x7f0f8565ba60>))
2023-10-30 12:29:48,470 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:48,470 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,472 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b130>, (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, <mixtensor.MixTensor object at 0x7f0f8565b790>))
2023-10-30 12:29:48,472 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:48,472 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,475 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c820>, (<mixtensor.MixTensor object at 0x7f0f8572fe80>, <mixtensor.MixTensor object at 0x7f0f8565b160>))
2023-10-30 12:29:48,475 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,476 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:48,476 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,481 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,484 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,484 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,485 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:48,485 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,489 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85659c00>, (<mixtensor.MixTensor object at 0x7f0f856581f0>, <mixtensor.MixTensor object at 0x7f0f8565b310>))
2023-10-30 12:29:48,490 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:48,490 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,491 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d449a0>, (<mixtensor.MixTensor object at 0x7f0f84d44970>, <mixtensor.MixTensor object at 0x7f0f84d44610>))
2023-10-30 12:29:48,491 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:48,492 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,493 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d60>, (<mixtensor.MixTensor object at 0x7f0f84d44430>, <mixtensor.MixTensor object at 0x7f0f84d44fd0>))
2023-10-30 12:29:48,493 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:48,493 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,495 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fd60>, (<mixtensor.MixTensor object at 0x7f0f8572c8e0>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:48,495 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,495 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:48,496 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,496 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,499 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,499 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,500 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:48,500 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,504 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca00>, (<mixtensor.MixTensor object at 0x7f0f8572d1b0>, <mixtensor.MixTensor object at 0x7f0f84d440d0>))
2023-10-30 12:29:48,505 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:48,505 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,507 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a30>, (<mixtensor.MixTensor object at 0x7f0f84d45240>, <mixtensor.MixTensor object at 0x7f0f84d449d0>))
2023-10-30 12:29:48,508 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:48,508 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,510 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a60>, (<mixtensor.MixTensor object at 0x7f0f84d44760>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:48,510 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:48,510 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,512 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d210>, (<mixtensor.MixTensor object at 0x7f0f84d440d0>, <mixtensor.MixTensor object at 0x7f0f84d44190>))
2023-10-30 12:29:48,513 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 12:29:48,513 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:48,514 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,517 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,518 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,518 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,518 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:48,518 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,519 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c820>
2023-10-30 12:29:48,519 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:48,519 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,519 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d4e0>
2023-10-30 12:29:48,519 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:48,519 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,519 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44d60>
2023-10-30 12:29:48,520 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:48,520 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,520 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44370>
2023-10-30 12:29:48,520 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,520 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:48,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,521 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,521 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,522 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:48,522 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,528 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dab0>
2023-10-30 12:29:48,528 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:48,529 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,535 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e260>
2023-10-30 12:29:48,535 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:48,535 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,542 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d7b0>
2023-10-30 12:29:48,542 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:48,543 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,549 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ce80>
2023-10-30 12:29:48,550 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:48,551 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:48,551 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,552 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,552 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:48,553 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,553 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:48,553 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,553 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fcd0>
2023-10-30 12:29:48,553 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:48,553 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,554 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572da50>
2023-10-30 12:29:48,554 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:48,554 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,554 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cca0>
2023-10-30 12:29:48,554 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:48,554 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,556 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dcf0>
2023-10-30 12:29:48,556 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,557 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:48,557 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,561 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,562 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-30 12:29:48,562 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,562 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:48,563 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs_k: {}
2023-10-30 12:29:48,563 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c340>
2023-10-30 12:29:48,563 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:48,563 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs_k: {}
2023-10-30 12:29:48,564 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c6a0>
2023-10-30 12:29:48,564 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:48,564 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs_k: {}
2023-10-30 12:29:48,564 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ca60>
2023-10-30 12:29:48,564 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:48,564 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs_k: {}
2023-10-30 12:29:48,565 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c1c0>
2023-10-30 12:29:48,565 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,565 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:48,566 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,569 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,572 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,573 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,573 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:48,573 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,576 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d210>, (<mixtensor.MixTensor object at 0x7f0f8572c9a0>, <mixtensor.MixTensor object at 0x7f0f8572d7b0>))
2023-10-30 12:29:48,576 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:48,576 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,577 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d840>, (<mixtensor.MixTensor object at 0x7f0f8572dab0>, <mixtensor.MixTensor object at 0x7f0f84d4cb50>))
2023-10-30 12:29:48,578 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:48,578 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,579 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4f700>, <mixtensor.MixTensor object at 0x7f0f84d4e8c0>))
2023-10-30 12:29:48,579 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:48,579 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,581 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d510>, (<mixtensor.MixTensor object at 0x7f0f8572d9c0>, <mixtensor.MixTensor object at 0x7f0f8572c250>))
2023-10-30 12:29:48,581 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,581 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:48,582 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,585 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,589 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,589 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,589 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:48,589 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,592 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, <mixtensor.MixTensor object at 0x7f0f84d4dde0>))
2023-10-30 12:29:48,592 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:48,592 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,594 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4c370>, <mixtensor.MixTensor object at 0x7f0f84d4eb00>))
2023-10-30 12:29:48,594 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:48,594 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,595 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, <mixtensor.MixTensor object at 0x7f0f84d4f040>))
2023-10-30 12:29:48,596 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:48,596 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,597 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e920>, (<mixtensor.MixTensor object at 0x7f0f84d4c0d0>, <mixtensor.MixTensor object at 0x7f0f84d4d930>))
2023-10-30 12:29:48,597 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,597 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:48,598 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,602 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,605 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,605 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,606 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:48,606 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,609 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4edd0>, (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, <mixtensor.MixTensor object at 0x7f0f84d4dde0>))
2023-10-30 12:29:48,609 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:48,609 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,611 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c880>, (<mixtensor.MixTensor object at 0x7f0f84d4e1d0>, <mixtensor.MixTensor object at 0x7f0f84d4e5c0>))
2023-10-30 12:29:48,611 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:48,611 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,612 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fa90>, (<mixtensor.MixTensor object at 0x7f0f84d4c2e0>, <mixtensor.MixTensor object at 0x7f0f84d4cbb0>))
2023-10-30 12:29:48,612 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:48,613 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,614 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, <mixtensor.MixTensor object at 0x7f0f84d4e710>))
2023-10-30 12:29:48,614 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,614 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:48,615 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,619 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,623 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,623 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,623 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:48,624 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,627 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, (<mixtensor.MixTensor object at 0x7f0f84d4dde0>, <mixtensor.MixTensor object at 0x7f0f84d4d810>))
2023-10-30 12:29:48,627 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:48,627 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,629 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eda0>, (<mixtensor.MixTensor object at 0x7f0f84d4f970>, <mixtensor.MixTensor object at 0x7f0f84d4e3b0>))
2023-10-30 12:29:48,629 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:48,629 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,630 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, (<mixtensor.MixTensor object at 0x7f0f84d4c700>, <mixtensor.MixTensor object at 0x7f0f84d4d450>))
2023-10-30 12:29:48,631 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:48,631 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,632 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ec80>, (<mixtensor.MixTensor object at 0x7f0f84d4fe20>, <mixtensor.MixTensor object at 0x7f0f84d4fc40>))
2023-10-30 12:29:48,632 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,633 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:48,633 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,637 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,640 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,641 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,641 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:48,641 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,644 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dde0>, (<mixtensor.MixTensor object at 0x7f0f84d4d810>, <mixtensor.MixTensor object at 0x7f0f84d4dff0>))
2023-10-30 12:29:48,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:48,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,646 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4d060>, <mixtensor.MixTensor object at 0x7f0f84d4de40>))
2023-10-30 12:29:48,646 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:48,646 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,647 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e260>, (<mixtensor.MixTensor object at 0x7f0f84d4ce80>, <mixtensor.MixTensor object at 0x7f0f84d4dd20>))
2023-10-30 12:29:48,647 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:48,648 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,649 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4cdf0>, <mixtensor.MixTensor object at 0x7f0f84d4e380>))
2023-10-30 12:29:48,649 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,649 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:48,650 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,654 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,657 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,657 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,658 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:48,658 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,660 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d810>, (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, <mixtensor.MixTensor object at 0x7f0f84d4dae0>))
2023-10-30 12:29:48,661 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:48,661 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,662 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b910>, (<mixtensor.MixTensor object at 0x7f0f8565aef0>, <mixtensor.MixTensor object at 0x7f0f8565ad70>))
2023-10-30 12:29:48,662 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:48,663 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,664 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b0d0>, (<mixtensor.MixTensor object at 0x7f0f8565b9a0>, <mixtensor.MixTensor object at 0x7f0f8565b610>))
2023-10-30 12:29:48,664 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:48,664 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,666 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4de10>, (<mixtensor.MixTensor object at 0x7f0f84d4e740>, <mixtensor.MixTensor object at 0x7f0f85658d00>))
2023-10-30 12:29:48,666 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,666 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:48,666 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,670 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,674 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,674 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,674 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:48,674 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,677 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, (<mixtensor.MixTensor object at 0x7f0f84d4dae0>, <mixtensor.MixTensor object at 0x7f0f85658190>))
2023-10-30 12:29:48,677 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:48,677 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,679 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bc40>, (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, <mixtensor.MixTensor object at 0x7f0f8565ada0>))
2023-10-30 12:29:48,679 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:48,679 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,681 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba90>, (<mixtensor.MixTensor object at 0x7f0f8565b130>, <mixtensor.MixTensor object at 0x7f0f84d45510>))
2023-10-30 12:29:48,681 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:48,681 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,682 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4e5f0>, <mixtensor.MixTensor object at 0x7f0f84d4d5d0>))
2023-10-30 12:29:48,683 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,683 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:48,683 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,687 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,690 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,690 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,691 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:48,691 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,693 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d780>, (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, <mixtensor.MixTensor object at 0x7f0f84d451e0>))
2023-10-30 12:29:48,694 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:48,694 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,695 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, (<mixtensor.MixTensor object at 0x7f0f84d445e0>, <mixtensor.MixTensor object at 0x7f0f84d447f0>))
2023-10-30 12:29:48,696 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:48,696 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,702 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445b0>, (<mixtensor.MixTensor object at 0x7f0f84d44940>, <mixtensor.MixTensor object at 0x7f0f84d444f0>))
2023-10-30 12:29:48,702 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:48,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,703 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c5b0>, (<mixtensor.MixTensor object at 0x7f0f84d451e0>, <mixtensor.MixTensor object at 0x7f0f84d45540>))
2023-10-30 12:29:48,704 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,704 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:48,704 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,708 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,711 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,712 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,712 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:48,712 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,715 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, (<mixtensor.MixTensor object at 0x7f0f84d44790>, <mixtensor.MixTensor object at 0x7f0f84d453c0>))
2023-10-30 12:29:48,715 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:48,715 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,717 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45030>, (<mixtensor.MixTensor object at 0x7f0f84d447c0>, <mixtensor.MixTensor object at 0x7f0f8565b0d0>))
2023-10-30 12:29:48,717 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:48,717 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,719 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd90>, (<mixtensor.MixTensor object at 0x7f0f8572fc10>, <mixtensor.MixTensor object at 0x7f0f8572cb50>))
2023-10-30 12:29:48,719 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:48,719 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,720 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d840>, (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, <mixtensor.MixTensor object at 0x7f0f8572fc70>))
2023-10-30 12:29:48,721 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,721 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:48,721 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,726 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:48,729 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,729 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,729 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:48,729 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,732 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4fb80>, <mixtensor.MixTensor object at 0x7f0f84d4c9d0>))
2023-10-30 12:29:48,732 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:48,733 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,734 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, (<mixtensor.MixTensor object at 0x7f0f84d4d780>, <mixtensor.MixTensor object at 0x7f0f84d4eda0>))
2023-10-30 12:29:48,734 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:48,734 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,736 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, <mixtensor.MixTensor object at 0x7f0f84d4e0b0>))
2023-10-30 12:29:48,736 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:48,736 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,737 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4d810>, <mixtensor.MixTensor object at 0x7f0f84d4d8a0>))
2023-10-30 12:29:48,738 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,738 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:48,738 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,743 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:48,746 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,746 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,747 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:48,747 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,749 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, <mixtensor.MixTensor object at 0x7f0f84d4f8b0>))
2023-10-30 12:29:48,749 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:48,750 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,751 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4e590>, <mixtensor.MixTensor object at 0x7f0f84d4d300>))
2023-10-30 12:29:48,751 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:48,752 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,753 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d960>, (<mixtensor.MixTensor object at 0x7f0f84d4f550>, <mixtensor.MixTensor object at 0x7f0f84d4d240>))
2023-10-30 12:29:48,753 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:48,753 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,755 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e080>, (<mixtensor.MixTensor object at 0x7f0f84d4fb80>, <mixtensor.MixTensor object at 0x7f0f84d4d030>))
2023-10-30 12:29:48,755 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,755 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:48,755 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,756 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:48,759 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,759 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,760 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:48,760 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,762 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, <mixtensor.MixTensor object at 0x7f0f84d4f640>))
2023-10-30 12:29:48,762 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:48,763 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,764 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, <mixtensor.MixTensor object at 0x7f0f84d442b0>))
2023-10-30 12:29:48,764 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:48,765 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,766 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d60>, (<mixtensor.MixTensor object at 0x7f0f84d44370>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:48,766 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:48,767 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,768 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c5e0>, (<mixtensor.MixTensor object at 0x7f0f84d4dde0>, <mixtensor.MixTensor object at 0x7f0f84d45030>))
2023-10-30 12:29:48,768 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 12:29:48,768 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:48,769 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,772 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:48,773 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,773 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,773 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:48,773 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,773 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c2b0>
2023-10-30 12:29:48,774 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:48,774 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,774 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fb50>
2023-10-30 12:29:48,774 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:48,774 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,774 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fdc0>
2023-10-30 12:29:48,775 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:48,775 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,775 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e650>
2023-10-30 12:29:48,775 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,775 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:48,775 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,776 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:48,776 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,776 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,777 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:48,777 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,783 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cca0>
2023-10-30 12:29:48,783 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:48,783 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,789 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c9d0>
2023-10-30 12:29:48,789 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:48,789 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,795 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c5e0>
2023-10-30 12:29:48,795 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:48,795 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:48,801 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e080>
2023-10-30 12:29:48,802 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:48,803 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:48,803 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,804 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:48,804 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:48,804 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,804 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:48,804 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,805 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dd80>
2023-10-30 12:29:48,805 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:48,805 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,805 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cc70>
2023-10-30 12:29:48,805 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:48,805 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,806 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d510>
2023-10-30 12:29:48,806 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:48,806 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:48,806 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c340>
2023-10-30 12:29:48,806 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,806 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:48,807 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,811 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:48,811 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-30 12:29:48,811 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:48,811 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:48,812 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs_k: {}
2023-10-30 12:29:48,812 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572da50>
2023-10-30 12:29:48,812 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:48,812 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs_k: {}
2023-10-30 12:29:48,812 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c0a0>
2023-10-30 12:29:48,813 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:48,813 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs_k: {}
2023-10-30 12:29:48,813 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fcd0>
2023-10-30 12:29:48,813 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:48,813 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs_k: {}
2023-10-30 12:29:48,814 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c8b0>
2023-10-30 12:29:48,814 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:48,814 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:48,814 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,819 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:48,822 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,822 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,822 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:48,822 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,834 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, (<mixtensor.MixTensor object at 0x7f0f84d4ddb0>, <mixtensor.MixTensor object at 0x7f0f84d4f1c0>))
2023-10-30 12:29:48,834 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:48,834 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,836 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efb0>, (<mixtensor.MixTensor object at 0x7f0f84d4db70>, <mixtensor.MixTensor object at 0x7f0f84d4da80>))
2023-10-30 12:29:48,836 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:48,836 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,838 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c670>, (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, <mixtensor.MixTensor object at 0x7f0f84d4e890>))
2023-10-30 12:29:48,838 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:48,838 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,839 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df60>, (<mixtensor.MixTensor object at 0x7f0f84d4f520>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:48,840 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,840 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:48,840 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,845 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:48,848 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,848 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,849 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:48,849 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,853 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d420>, (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, <mixtensor.MixTensor object at 0x7f0f84d4f460>))
2023-10-30 12:29:48,853 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:48,853 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,855 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce20>, (<mixtensor.MixTensor object at 0x7f0f84d4e200>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:48,855 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:48,856 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,857 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f670>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4f580>))
2023-10-30 12:29:48,857 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:48,857 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,859 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ca00>, (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, <mixtensor.MixTensor object at 0x7f0f84d4fee0>))
2023-10-30 12:29:48,859 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,860 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:48,860 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,864 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:48,868 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,868 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,868 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:48,868 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,871 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d330>, (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, <mixtensor.MixTensor object at 0x7f0f84d4f460>))
2023-10-30 12:29:48,871 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:48,871 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,873 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ea40>, (<mixtensor.MixTensor object at 0x7f0f84d4c2b0>, <mixtensor.MixTensor object at 0x7f0f84d4f280>))
2023-10-30 12:29:48,873 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:48,873 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,874 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4eaa0>))
2023-10-30 12:29:48,874 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:48,875 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,876 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d390>, (<mixtensor.MixTensor object at 0x7f0f84d4d420>, <mixtensor.MixTensor object at 0x7f0f84d4ffd0>))
2023-10-30 12:29:48,876 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,876 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:48,877 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,881 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:48,884 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,884 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,885 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:48,885 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,887 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f460>, <mixtensor.MixTensor object at 0x7f0f84d4c5e0>))
2023-10-30 12:29:48,888 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:48,888 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,889 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, <mixtensor.MixTensor object at 0x7f0f84d4f640>))
2023-10-30 12:29:48,890 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:48,890 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,891 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d44b20>, <mixtensor.MixTensor object at 0x7f0f84d44670>))
2023-10-30 12:29:48,892 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:48,892 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,893 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efb0>, (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, <mixtensor.MixTensor object at 0x7f0f84d4e080>))
2023-10-30 12:29:48,893 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,893 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:48,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,898 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:48,901 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,901 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,902 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:48,902 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,905 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f460>, (<mixtensor.MixTensor object at 0x7f0f84d4c5e0>, <mixtensor.MixTensor object at 0x7f0f84d441f0>))
2023-10-30 12:29:48,905 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:48,905 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,906 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443d0>, (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:48,906 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:48,907 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,908 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d454e0>, (<mixtensor.MixTensor object at 0x7f0f84d44e80>, <mixtensor.MixTensor object at 0x7f0f84d44af0>))
2023-10-30 12:29:48,908 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:48,908 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,910 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d540>, (<mixtensor.MixTensor object at 0x7f0f84d441f0>, <mixtensor.MixTensor object at 0x7f0f84d44460>))
2023-10-30 12:29:48,910 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,910 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:48,910 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,915 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:48,918 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,918 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,919 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:48,919 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,922 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d90>, (<mixtensor.MixTensor object at 0x7f0f84d443a0>, <mixtensor.MixTensor object at 0x7f0f84d44280>))
2023-10-30 12:29:48,922 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:48,922 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,936 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45390>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44dc0>))
2023-10-30 12:29:48,936 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:48,936 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,939 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d448e0>, (<mixtensor.MixTensor object at 0x7f0f84d44f40>, <mixtensor.MixTensor object at 0x7f0f84d45060>))
2023-10-30 12:29:48,939 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:48,940 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,942 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d44280>, <mixtensor.MixTensor object at 0x7f0f84d44df0>))
2023-10-30 12:29:48,943 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,944 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:48,944 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,951 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:48,956 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,957 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,957 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:48,958 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,961 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443d0>, (<mixtensor.MixTensor object at 0x7f0f84d454e0>, <mixtensor.MixTensor object at 0x7f0f84d450f0>))
2023-10-30 12:29:48,961 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:48,962 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,964 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44400>, (<mixtensor.MixTensor object at 0x7f0f84d44130>, <mixtensor.MixTensor object at 0x7f0f84d44b50>))
2023-10-30 12:29:48,964 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:48,965 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,966 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44910>, (<mixtensor.MixTensor object at 0x7f0f84d44730>, <mixtensor.MixTensor object at 0x7f0f84d44be0>))
2023-10-30 12:29:48,966 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:48,966 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,968 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d450f0>, (<mixtensor.MixTensor object at 0x7f0f84d44340>, <mixtensor.MixTensor object at 0x7f0f84d44e50>))
2023-10-30 12:29:48,968 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:48,968 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:48,968 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:48,973 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:48,976 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:48,976 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,976 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:48,977 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,981 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d443d0>, <mixtensor.MixTensor object at 0x7f0f84d451b0>))
2023-10-30 12:29:48,981 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:48,982 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,992 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44550>, (<mixtensor.MixTensor object at 0x7f0f84d441c0>, <mixtensor.MixTensor object at 0x7f0f84d44070>))
2023-10-30 12:29:48,992 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:48,992 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,997 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44970>, (<mixtensor.MixTensor object at 0x7f0f84d44430>, <mixtensor.MixTensor object at 0x7f0f84d45300>))
2023-10-30 12:29:48,997 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:48,997 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:48,999 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45390>, (<mixtensor.MixTensor object at 0x7f0f84d448e0>, <mixtensor.MixTensor object at 0x7f0f84d44310>))
2023-10-30 12:29:48,999 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:49,000 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:49,000 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:49,005 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:49,008 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,008 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,009 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:49,009 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,012 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d443d0>, <mixtensor.MixTensor object at 0x7f0f84d44610>))
2023-10-30 12:29:49,012 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:49,012 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,014 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d44d00>, <mixtensor.MixTensor object at 0x7f0f84d4f880>))
2023-10-30 12:29:49,015 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:49,015 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,016 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, (<mixtensor.MixTensor object at 0x7f0f84d4ca00>, <mixtensor.MixTensor object at 0x7f0f84d4df60>))
2023-10-30 12:29:49,017 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:49,017 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,019 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cf10>, (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, <mixtensor.MixTensor object at 0x7f0f8572d330>))
2023-10-30 12:29:49,019 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:49,019 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:49,020 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:49,024 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:49,028 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,028 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,028 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:49,028 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,031 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d510>, (<mixtensor.MixTensor object at 0x7f0f8572c1f0>, <mixtensor.MixTensor object at 0x7f0f8572cee0>))
2023-10-30 12:29:49,031 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:49,032 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,034 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ccd0>, (<mixtensor.MixTensor object at 0x7f0f8572c4f0>, <mixtensor.MixTensor object at 0x7f0f8572cfd0>))
2023-10-30 12:29:49,034 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:49,034 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,036 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d090>, (<mixtensor.MixTensor object at 0x7f0f8572c670>, <mixtensor.MixTensor object at 0x7f0f8572cd90>))
2023-10-30 12:29:49,036 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:49,036 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,038 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d660>, (<mixtensor.MixTensor object at 0x7f0f8572c820>, <mixtensor.MixTensor object at 0x7f0f8572c430>))
2023-10-30 12:29:49,038 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:49,039 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:49,039 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:49,044 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:49,047 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,047 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,047 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:49,048 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,050 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba60>, (<mixtensor.MixTensor object at 0x7f0f8565b100>, <mixtensor.MixTensor object at 0x7f0f8565bac0>))
2023-10-30 12:29:49,050 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:49,051 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,052 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45270>, (<mixtensor.MixTensor object at 0x7f0f84d44400>, <mixtensor.MixTensor object at 0x7f0f84d44ca0>))
2023-10-30 12:29:49,052 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:49,052 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,054 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44550>, (<mixtensor.MixTensor object at 0x7f0f84d44970>, <mixtensor.MixTensor object at 0x7f0f84d45390>))
2023-10-30 12:29:49,054 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:49,054 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,055 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cee0>, (<mixtensor.MixTensor object at 0x7f0f8572c160>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:49,056 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:49,056 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:49,056 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:49,057 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:49,060 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,061 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,061 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:49,061 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,064 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba60>, (<mixtensor.MixTensor object at 0x7f0f8565bac0>, <mixtensor.MixTensor object at 0x7f0f84d443a0>))
2023-10-30 12:29:49,064 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:49,064 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,066 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44610>, (<mixtensor.MixTensor object at 0x7f0f84d454e0>, <mixtensor.MixTensor object at 0x7f0f84d44250>))
2023-10-30 12:29:49,066 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:49,066 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,067 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44f10>, (<mixtensor.MixTensor object at 0x7f0f84d449d0>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:49,068 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:49,068 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,069 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d510>, (<mixtensor.MixTensor object at 0x7f0f84d443a0>, <mixtensor.MixTensor object at 0x7f0f84d443d0>))
2023-10-30 12:29:49,069 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 12:29:49,070 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:49,070 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:49,073 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:49,074 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,074 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,075 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:49,075 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,075 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cee0>
2023-10-30 12:29:49,075 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:49,075 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,075 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c1c0>
2023-10-30 12:29:49,076 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:49,076 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,076 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d45270>
2023-10-30 12:29:49,076 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:49,076 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,076 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44d90>
2023-10-30 12:29:49,077 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,077 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:49,077 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:49,077 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:49,078 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,078 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,078 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:49,078 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,088 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ccd0>
2023-10-30 12:29:49,088 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:49,089 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,094 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c8b0>
2023-10-30 12:29:49,094 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:49,095 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,100 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565b790>
2023-10-30 12:29:49,101 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:49,101 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,106 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565ba60>
2023-10-30 12:29:49,107 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:49,108 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:49,109 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:49,109 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:49,110 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:49,110 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,110 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:49,110 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,111 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f400>
2023-10-30 12:29:49,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:49,111 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,111 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e8c0>
2023-10-30 12:29:49,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:49,111 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,112 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f430>
2023-10-30 12:29:49,112 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:49,112 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,112 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e1d0>
2023-10-30 12:29:49,112 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,113 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:49,113 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:49,117 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:49,118 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-30 12:29:49,118 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,118 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:49,118 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs_k: {}
2023-10-30 12:29:49,119 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c370>
2023-10-30 12:29:49,119 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:49,119 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs_k: {}
2023-10-30 12:29:49,119 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fc10>
2023-10-30 12:29:49,119 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:49,119 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs_k: {}
2023-10-30 12:29:49,120 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e3b0>
2023-10-30 12:29:49,120 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:49,120 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs_k: {}
2023-10-30 12:29:49,120 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f700>
2023-10-30 12:29:49,120 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,121 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:49,121 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:49,125 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:49,130 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,130 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,130 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:49,130 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,134 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f7f0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f8572d750>))
2023-10-30 12:29:49,134 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:49,134 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,136 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d900>, (<mixtensor.MixTensor object at 0x7f0f8572d810>, <mixtensor.MixTensor object at 0x7f0f8572cd30>))
2023-10-30 12:29:49,136 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:49,137 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,140 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d600>, (<mixtensor.MixTensor object at 0x7f0f8572d5a0>, <mixtensor.MixTensor object at 0x7f0f8572c5b0>))
2023-10-30 12:29:49,140 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:49,140 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,142 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d750>, (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, <mixtensor.MixTensor object at 0x7f0f8572d9f0>))
2023-10-30 12:29:49,142 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,142 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:49,143 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:49,148 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:49,151 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,151 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,152 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:49,152 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,154 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c880>, (<mixtensor.MixTensor object at 0x7f0f8572c580>, <mixtensor.MixTensor object at 0x7f0f8572d2a0>))
2023-10-30 12:29:49,154 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:49,155 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,156 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c610>, (<mixtensor.MixTensor object at 0x7f0f8572d630>, <mixtensor.MixTensor object at 0x7f0f8572fd30>))
2023-10-30 12:29:49,156 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:49,156 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,158 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf70>, (<mixtensor.MixTensor object at 0x7f0f8572c5e0>, <mixtensor.MixTensor object at 0x7f0f8572c070>))
2023-10-30 12:29:49,158 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:49,158 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,160 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dae0>, (<mixtensor.MixTensor object at 0x7f0f8572db40>, <mixtensor.MixTensor object at 0x7f0f8572dcc0>))
2023-10-30 12:29:49,160 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,160 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:49,161 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:49,165 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:49,169 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,169 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,169 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:49,169 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,173 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c760>, (<mixtensor.MixTensor object at 0x7f0f8572c580>, <mixtensor.MixTensor object at 0x7f0f8572d2a0>))
2023-10-30 12:29:49,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:49,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d4b0>, (<mixtensor.MixTensor object at 0x7f0f8572d090>, <mixtensor.MixTensor object at 0x7f0f8572fcd0>))
2023-10-30 12:29:49,175 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:49,175 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,177 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c250>, (<mixtensor.MixTensor object at 0x7f0f8572d8d0>, <mixtensor.MixTensor object at 0x7f0f8572c940>))
2023-10-30 12:29:49,177 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:49,177 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,179 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd20>, (<mixtensor.MixTensor object at 0x7f0f8572c880>, <mixtensor.MixTensor object at 0x7f0f8572d840>))
2023-10-30 12:29:49,179 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,179 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:49,179 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:49,184 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:49,187 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,187 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,188 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:49,188 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,190 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c580>, (<mixtensor.MixTensor object at 0x7f0f8572d2a0>, <mixtensor.MixTensor object at 0x7f0f8572d7b0>))
2023-10-30 12:29:49,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:49,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,192 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cb50>, (<mixtensor.MixTensor object at 0x7f0f8572fc70>, <mixtensor.MixTensor object at 0x7f0f8572c970>))
2023-10-30 12:29:49,192 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:49,193 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,194 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cb20>, (<mixtensor.MixTensor object at 0x7f0f8572d5d0>, <mixtensor.MixTensor object at 0x7f0f8572d390>))
2023-10-30 12:29:49,194 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:49,194 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,196 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d690>, (<mixtensor.MixTensor object at 0x7f0f8572d900>, <mixtensor.MixTensor object at 0x7f0f8572d540>))
2023-10-30 12:29:49,196 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,196 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:49,196 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:49,201 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:49,204 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,205 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,205 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:49,205 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,208 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d2a0>, (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, <mixtensor.MixTensor object at 0x7f0f8572c0a0>))
2023-10-30 12:29:49,208 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:49,208 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,210 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd80>, (<mixtensor.MixTensor object at 0x7f0f8572d660>, <mixtensor.MixTensor object at 0x7f0f8565b1f0>))
2023-10-30 12:29:49,210 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:49,210 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,211 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b070>, (<mixtensor.MixTensor object at 0x7f0f8565ba30>, <mixtensor.MixTensor object at 0x7f0f85659c00>))
2023-10-30 12:29:49,212 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:49,212 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,213 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d570>, (<mixtensor.MixTensor object at 0x7f0f8572dae0>, <mixtensor.MixTensor object at 0x7f0f8572c8b0>))
2023-10-30 12:29:49,213 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,213 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:49,214 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:49,220 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:49,224 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,224 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,225 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:49,225 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, (<mixtensor.MixTensor object at 0x7f0f8572c0a0>, <mixtensor.MixTensor object at 0x7f0f8565b9a0>))
2023-10-30 12:29:49,229 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:49,229 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,231 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b820>, (<mixtensor.MixTensor object at 0x7f0f8565ad70>, <mixtensor.MixTensor object at 0x7f0f8565ada0>))
2023-10-30 12:29:49,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:49,232 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,234 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b0d0>, (<mixtensor.MixTensor object at 0x7f0f85658c40>, <mixtensor.MixTensor object at 0x7f0f8565b790>))
2023-10-30 12:29:49,234 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:49,234 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,236 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d2d0>, (<mixtensor.MixTensor object at 0x7f0f8565b9a0>, <mixtensor.MixTensor object at 0x7f0f8565b940>))
2023-10-30 12:29:49,237 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,237 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:49,237 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:49,242 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:49,246 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,246 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,246 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:49,246 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,249 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565aef0>, (<mixtensor.MixTensor object at 0x7f0f8565b070>, <mixtensor.MixTensor object at 0x7f0f856581f0>))
2023-10-30 12:29:49,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:49,250 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,251 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b160>, (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, <mixtensor.MixTensor object at 0x7f0f84d44520>))
2023-10-30 12:29:49,251 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:49,251 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,253 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45330>, (<mixtensor.MixTensor object at 0x7f0f84d44e20>, <mixtensor.MixTensor object at 0x7f0f84d45510>))
2023-10-30 12:29:49,253 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:49,253 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,255 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c0a0>, (<mixtensor.MixTensor object at 0x7f0f8572d750>, <mixtensor.MixTensor object at 0x7f0f8572d2a0>))
2023-10-30 12:29:49,255 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,255 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:49,255 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:49,260 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:49,263 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,263 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,264 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:49,264 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,266 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565aef0>, (<mixtensor.MixTensor object at 0x7f0f856581f0>, <mixtensor.MixTensor object at 0x7f0f84d44c70>))
2023-10-30 12:29:49,266 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:49,267 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,268 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44940>, (<mixtensor.MixTensor object at 0x7f0f84d451e0>, <mixtensor.MixTensor object at 0x7f0f84d45540>))
2023-10-30 12:29:49,268 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:49,269 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,270 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447c0>, (<mixtensor.MixTensor object at 0x7f0f84d445b0>, <mixtensor.MixTensor object at 0x7f0f84d44790>))
2023-10-30 12:29:49,270 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:49,270 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,272 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d570>, (<mixtensor.MixTensor object at 0x7f0f84d44c70>, <mixtensor.MixTensor object at 0x7f0f84d445e0>))
2023-10-30 12:29:49,272 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,272 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:49,272 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:49,277 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:49,280 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,280 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,280 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:49,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,283 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b80>, (<mixtensor.MixTensor object at 0x7f0f84d45330>, <mixtensor.MixTensor object at 0x7f0f84d44580>))
2023-10-30 12:29:49,283 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:49,284 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,286 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44cd0>, (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, <mixtensor.MixTensor object at 0x7f0f8565bac0>))
2023-10-30 12:29:49,286 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:49,286 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,287 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, (<mixtensor.MixTensor object at 0x7f0f8572d240>, <mixtensor.MixTensor object at 0x7f0f8572da50>))
2023-10-30 12:29:49,288 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:49,288 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,289 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c700>, (<mixtensor.MixTensor object at 0x7f0f84d4e530>, <mixtensor.MixTensor object at 0x7f0f84d4f5e0>))
2023-10-30 12:29:49,289 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,290 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:49,290 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:49,294 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:49,297 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,297 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,298 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:49,298 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,300 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f820>, <mixtensor.MixTensor object at 0x7f0f84d4fc40>))
2023-10-30 12:29:49,300 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:49,301 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,302 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c850>, (<mixtensor.MixTensor object at 0x7f0f84d4feb0>, <mixtensor.MixTensor object at 0x7f0f84d4cf40>))
2023-10-30 12:29:49,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:49,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,304 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe20>, (<mixtensor.MixTensor object at 0x7f0f84d4faf0>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:49,304 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:49,304 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,306 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fa00>, (<mixtensor.MixTensor object at 0x7f0f84d4f040>, <mixtensor.MixTensor object at 0x7f0f84d4e170>))
2023-10-30 12:29:49,306 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,306 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:49,306 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:49,311 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:49,314 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,314 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,315 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:49,315 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,317 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc40>, (<mixtensor.MixTensor object at 0x7f0f84d4d030>, <mixtensor.MixTensor object at 0x7f0f84d4cb50>))
2023-10-30 12:29:49,318 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:49,318 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,319 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d240>, (<mixtensor.MixTensor object at 0x7f0f84d4de10>, <mixtensor.MixTensor object at 0x7f0f84d4c5b0>))
2023-10-30 12:29:49,320 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:49,320 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,321 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce20>, (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, <mixtensor.MixTensor object at 0x7f0f84d4ddb0>))
2023-10-30 12:29:49,321 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:49,321 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,323 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f820>, <mixtensor.MixTensor object at 0x7f0f84d4e0b0>))
2023-10-30 12:29:49,323 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,323 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:49,324 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:49,324 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:49,328 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,328 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,328 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:49,328 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,335 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d030>, (<mixtensor.MixTensor object at 0x7f0f84d4cb50>, <mixtensor.MixTensor object at 0x7f0f84d44cd0>))
2023-10-30 12:29:49,335 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:49,336 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,337 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44940>, (<mixtensor.MixTensor object at 0x7f0f84d447c0>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:49,337 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:49,338 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,339 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45030>, (<mixtensor.MixTensor object at 0x7f0f84d45270>, <mixtensor.MixTensor object at 0x7f0f84d44d90>))
2023-10-30 12:29:49,339 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:49,339 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,341 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, (<mixtensor.MixTensor object at 0x7f0f84d44cd0>, <mixtensor.MixTensor object at 0x7f0f84d453c0>))
2023-10-30 12:29:49,341 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 12:29:49,341 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:49,342 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:49,345 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:49,345 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,346 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,346 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:49,346 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,346 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d3c0>
2023-10-30 12:29:49,346 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:49,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fa00>
2023-10-30 12:29:49,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:49,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d030>
2023-10-30 12:29:49,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:49,348 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,348 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f790>
2023-10-30 12:29:49,348 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,348 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:49,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:49,349 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:49,349 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,349 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,350 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:49,350 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,356 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e3b0>
2023-10-30 12:29:49,356 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:49,356 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,362 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c9d0>
2023-10-30 12:29:49,362 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:49,362 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,368 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e8c0>
2023-10-30 12:29:49,368 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:49,368 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:49,374 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f430>
2023-10-30 12:29:49,374 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:49,376 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:49,376 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:49,376 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:49,377 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:49,377 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,377 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:49,377 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,378 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cd90>
2023-10-30 12:29:49,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:49,378 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,378 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fe80>
2023-10-30 12:29:49,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:49,378 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,379 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d450>
2023-10-30 12:29:49,379 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:49,379 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:49,379 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c370>
2023-10-30 12:29:49,379 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,380 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:49,380 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:49,384 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:49,385 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-30 12:29:49,385 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:49,385 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:49,385 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs_k: {}
2023-10-30 12:29:49,385 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c4f0>
2023-10-30 12:29:49,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:49,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs_k: {}
2023-10-30 12:29:49,386 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f160>
2023-10-30 12:29:49,386 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:49,386 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs_k: {}
2023-10-30 12:29:49,387 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c700>
2023-10-30 12:29:49,387 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:49,387 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs_k: {}
2023-10-30 12:29:49,387 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dcf0>
2023-10-30 12:29:49,388 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:49,388 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:49,388 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:49,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:49,395 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,395 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,396 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:49,396 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,398 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, <mixtensor.MixTensor object at 0x7f0f84d4e200>))
2023-10-30 12:29:49,399 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:49,399 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,400 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e650>, (<mixtensor.MixTensor object at 0x7f0f84d4dae0>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:49,400 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:49,401 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,402 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f580>, (<mixtensor.MixTensor object at 0x7f0f84d4fee0>, <mixtensor.MixTensor object at 0x7f0f84d4f490>))
2023-10-30 12:29:49,402 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:49,402 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,404 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f400>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4efe0>))
2023-10-30 12:29:49,404 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:49,404 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:49,404 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:49,409 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:49,412 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,413 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,413 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:49,413 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,416 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, <mixtensor.MixTensor object at 0x7f0f84d4ecb0>))
2023-10-30 12:29:49,416 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:49,416 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,924 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ca00>, (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, <mixtensor.MixTensor object at 0x7f0f84d4f880>))
2023-10-30 12:29:49,925 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:49,925 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,928 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df60>, (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, <mixtensor.MixTensor object at 0x7f0f84d4d3c0>))
2023-10-30 12:29:49,929 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:49,929 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,931 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c370>, (<mixtensor.MixTensor object at 0x7f0f84d4f640>, <mixtensor.MixTensor object at 0x7f0f84d4e080>))
2023-10-30 12:29:49,931 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:49,932 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:49,932 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:49,937 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:49,940 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,940 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,941 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:49,941 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,943 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d420>, (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, <mixtensor.MixTensor object at 0x7f0f84d4ecb0>))
2023-10-30 12:29:49,943 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:49,944 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,945 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d030>, (<mixtensor.MixTensor object at 0x7f0f84d4f790>, <mixtensor.MixTensor object at 0x7f0f84d4ff10>))
2023-10-30 12:29:49,945 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:49,946 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,947 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4fac0>))
2023-10-30 12:29:49,947 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:49,947 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,949 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, <mixtensor.MixTensor object at 0x7f0f84d4fa00>))
2023-10-30 12:29:49,949 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:49,949 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:49,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:49,954 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:49,957 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,957 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,958 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:49,958 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,960 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, <mixtensor.MixTensor object at 0x7f0f84d4c730>))
2023-10-30 12:29:49,961 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:49,961 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,962 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d930>, (<mixtensor.MixTensor object at 0x7f0f84d4c2e0>, <mixtensor.MixTensor object at 0x7f0f84d4d240>))
2023-10-30 12:29:49,962 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:49,963 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,964 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce20>, (<mixtensor.MixTensor object at 0x7f0f84d4fc40>, <mixtensor.MixTensor object at 0x7f0f84d448b0>))
2023-10-30 12:29:49,964 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:49,964 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,966 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, (<mixtensor.MixTensor object at 0x7f0f84d4f580>, <mixtensor.MixTensor object at 0x7f0f84d4e3b0>))
2023-10-30 12:29:49,966 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:49,966 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:49,967 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:49,971 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:49,974 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,974 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,975 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:49,975 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,977 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ecb0>, (<mixtensor.MixTensor object at 0x7f0f84d4c730>, <mixtensor.MixTensor object at 0x7f0f84d44b20>))
2023-10-30 12:29:49,977 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:49,978 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,979 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44c10>, (<mixtensor.MixTensor object at 0x7f0f84d44670>, <mixtensor.MixTensor object at 0x7f0f84d441f0>))
2023-10-30 12:29:49,979 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:49,980 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,981 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45120>, (<mixtensor.MixTensor object at 0x7f0f84d454b0>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:49,981 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:49,981 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,983 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d44b20>, <mixtensor.MixTensor object at 0x7f0f84d44220>))
2023-10-30 12:29:49,983 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:49,983 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:49,983 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:49,988 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:49,991 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:49,991 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:49,991 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,994 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, (<mixtensor.MixTensor object at 0x7f0f84d44850>, <mixtensor.MixTensor object at 0x7f0f84d44460>))
2023-10-30 12:29:49,994 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:49,994 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,996 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45090>, (<mixtensor.MixTensor object at 0x7f0f84d444c0>, <mixtensor.MixTensor object at 0x7f0f84d44dc0>))
2023-10-30 12:29:49,996 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:49,996 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,998 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45060>, (<mixtensor.MixTensor object at 0x7f0f84d44df0>, <mixtensor.MixTensor object at 0x7f0f84d44490>))
2023-10-30 12:29:49,998 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:49,998 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:49,999 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44850>, (<mixtensor.MixTensor object at 0x7f0f84d44460>, <mixtensor.MixTensor object at 0x7f0f84d44fa0>))
2023-10-30 12:29:50,000 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,000 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:50,000 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:50,005 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:50,008 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,008 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,008 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:50,009 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,011 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44c10>, (<mixtensor.MixTensor object at 0x7f0f84d45120>, <mixtensor.MixTensor object at 0x7f0f84d44130>))
2023-10-30 12:29:50,011 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:50,012 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,013 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44340>, (<mixtensor.MixTensor object at 0x7f0f84d44a90>, <mixtensor.MixTensor object at 0x7f0f84d452d0>))
2023-10-30 12:29:50,013 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:50,014 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,015 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45240>, (<mixtensor.MixTensor object at 0x7f0f84d441c0>, <mixtensor.MixTensor object at 0x7f0f84d44430>))
2023-10-30 12:29:50,015 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:50,015 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,017 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44130>, (<mixtensor.MixTensor object at 0x7f0f84d44af0>, <mixtensor.MixTensor object at 0x7f0f84d44730>))
2023-10-30 12:29:50,017 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,017 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:50,017 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:50,022 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:50,025 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,026 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,026 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:50,026 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,029 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44850>, (<mixtensor.MixTensor object at 0x7f0f84d44c10>, <mixtensor.MixTensor object at 0x7f0f84d44760>))
2023-10-30 12:29:50,029 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:50,029 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,031 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44070>, (<mixtensor.MixTensor object at 0x7f0f84d45300>, <mixtensor.MixTensor object at 0x7f0f84d44910>))
2023-10-30 12:29:50,031 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:50,031 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,033 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44400>, (<mixtensor.MixTensor object at 0x7f0f84d44970>, <mixtensor.MixTensor object at 0x7f0f84d44640>))
2023-10-30 12:29:50,033 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:50,033 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,034 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45090>, (<mixtensor.MixTensor object at 0x7f0f84d45060>, <mixtensor.MixTensor object at 0x7f0f84d440d0>))
2023-10-30 12:29:50,035 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,035 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:50,035 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:50,040 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:50,043 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,044 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,044 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:50,044 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,047 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44850>, (<mixtensor.MixTensor object at 0x7f0f84d44c10>, <mixtensor.MixTensor object at 0x7f0f84d44ca0>))
2023-10-30 12:29:50,047 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:50,047 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,049 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d449a0>, <mixtensor.MixTensor object at 0x7f0f84d4e8c0>))
2023-10-30 12:29:50,049 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:50,050 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,055 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df60>, (<mixtensor.MixTensor object at 0x7f0f84d4d2a0>, <mixtensor.MixTensor object at 0x7f0f84d4db70>))
2023-10-30 12:29:50,055 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:50,055 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,056 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c2b0>, (<mixtensor.MixTensor object at 0x7f0f84d4ec50>, <mixtensor.MixTensor object at 0x7f0f84d4da80>))
2023-10-30 12:29:50,056 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,057 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:50,057 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:50,062 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:50,065 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,066 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,066 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:50,066 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,069 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572faf0>, (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, <mixtensor.MixTensor object at 0x7f0f8572c370>))
2023-10-30 12:29:50,069 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:50,069 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,071 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d030>, (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, <mixtensor.MixTensor object at 0x7f0f8572c340>))
2023-10-30 12:29:50,071 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:50,071 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,072 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b070>, (<mixtensor.MixTensor object at 0x7f0f856581f0>, <mixtensor.MixTensor object at 0x7f0f8565aef0>))
2023-10-30 12:29:50,072 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:50,073 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,074 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c700>, (<mixtensor.MixTensor object at 0x7f0f8572cfd0>, <mixtensor.MixTensor object at 0x7f0f8572d570>))
2023-10-30 12:29:50,074 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,074 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:50,075 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:50,079 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:50,083 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,083 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,083 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:50,083 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,087 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b2e0>, (<mixtensor.MixTensor object at 0x7f0f85658190>, <mixtensor.MixTensor object at 0x7f0f8565ba60>))
2023-10-30 12:29:50,087 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:50,087 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,089 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d448e0>, (<mixtensor.MixTensor object at 0x7f0f84d44340>, <mixtensor.MixTensor object at 0x7f0f84d44760>))
2023-10-30 12:29:50,089 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:50,089 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,090 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44fd0>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d44400>))
2023-10-30 12:29:50,091 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:50,091 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,092 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, (<mixtensor.MixTensor object at 0x7f0f8572f7c0>, <mixtensor.MixTensor object at 0x7f0f84d452a0>))
2023-10-30 12:29:50,092 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,092 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:50,093 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:50,094 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:50,097 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,097 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,097 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:50,098 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,100 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd80>, (<mixtensor.MixTensor object at 0x7f0f8572d870>, <mixtensor.MixTensor object at 0x7f0f84d44850>))
2023-10-30 12:29:50,100 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:50,101 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,102 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ca0>, (<mixtensor.MixTensor object at 0x7f0f84d45120>, <mixtensor.MixTensor object at 0x7f0f84d446a0>))
2023-10-30 12:29:50,102 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:50,103 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,104 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44f70>, (<mixtensor.MixTensor object at 0x7f0f84d44250>, <mixtensor.MixTensor object at 0x7f0f84d44d30>))
2023-10-30 12:29:50,104 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:50,104 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,106 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d4b0>, (<mixtensor.MixTensor object at 0x7f0f84d44850>, <mixtensor.MixTensor object at 0x7f0f84d44c10>))
2023-10-30 12:29:50,106 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 12:29:50,106 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:50,106 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:50,110 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:50,111 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,111 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:50,111 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,111 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c700>
2023-10-30 12:29:50,111 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:50,112 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,112 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c790>
2023-10-30 12:29:50,112 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:50,112 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,112 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d448e0>
2023-10-30 12:29:50,113 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:50,113 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,113 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d45090>
2023-10-30 12:29:50,113 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,113 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:50,114 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:50,114 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:50,114 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,115 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,115 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:50,115 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,124 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d210>
2023-10-30 12:29:50,124 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:50,124 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,131 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c700>
2023-10-30 12:29:50,131 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:50,131 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,137 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f070>
2023-10-30 12:29:50,138 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:50,138 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,144 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f160>
2023-10-30 12:29:50,145 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:50,146 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:50,147 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:50,147 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:50,148 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:50,148 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,148 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:50,148 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,149 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d630>
2023-10-30 12:29:50,149 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:50,149 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,149 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cb50>
2023-10-30 12:29:50,149 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:50,149 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,150 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fd30>
2023-10-30 12:29:50,150 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:50,150 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,150 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c070>
2023-10-30 12:29:50,150 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,151 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:50,151 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:50,155 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:50,156 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-30 12:29:50,156 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,156 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:50,156 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs_k: {}
2023-10-30 12:29:50,157 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d9f0>
2023-10-30 12:29:50,157 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:50,157 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs_k: {}
2023-10-30 12:29:50,157 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d840>
2023-10-30 12:29:50,157 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:50,157 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs_k: {}
2023-10-30 12:29:50,158 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ccd0>
2023-10-30 12:29:50,158 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:50,158 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs_k: {}
2023-10-30 12:29:50,158 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cf10>
2023-10-30 12:29:50,159 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,159 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:50,159 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:50,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:50,167 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,167 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,167 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:50,168 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,171 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c9a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, <mixtensor.MixTensor object at 0x7f0f84d4d5d0>))
2023-10-30 12:29:50,171 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:50,171 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,173 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e380>, (<mixtensor.MixTensor object at 0x7f0f84d4dd20>, <mixtensor.MixTensor object at 0x7f0f84d4ce80>))
2023-10-30 12:29:50,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:50,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cdf0>, (<mixtensor.MixTensor object at 0x7f0f84d4de40>, <mixtensor.MixTensor object at 0x7f0f84d4fdf0>))
2023-10-30 12:29:50,175 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:50,175 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,176 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d5d0>, (<mixtensor.MixTensor object at 0x7f0f84d4e5f0>, <mixtensor.MixTensor object at 0x7f0f84d4e740>))
2023-10-30 12:29:50,177 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,177 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:50,177 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:50,181 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:50,185 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,185 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,185 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:50,186 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,189 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dfc0>, (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, <mixtensor.MixTensor object at 0x7f0f84d4ff70>))
2023-10-30 12:29:50,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:50,190 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,191 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f340>, (<mixtensor.MixTensor object at 0x7f0f84d4eb00>, <mixtensor.MixTensor object at 0x7f0f84d4e680>))
2023-10-30 12:29:50,192 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:50,192 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,193 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe80>, (<mixtensor.MixTensor object at 0x7f0f84d4dff0>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:50,193 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:50,193 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,195 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d8d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d4c0d0>))
2023-10-30 12:29:50,195 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,195 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:50,196 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:50,200 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:50,203 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,203 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,204 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:50,204 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,207 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f760>, (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, <mixtensor.MixTensor object at 0x7f0f84d4ff70>))
2023-10-30 12:29:50,207 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:50,207 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,209 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e530>, (<mixtensor.MixTensor object at 0x7f0f84d4d810>, <mixtensor.MixTensor object at 0x7f0f84d4d870>))
2023-10-30 12:29:50,209 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:50,209 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,211 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4faf0>, (<mixtensor.MixTensor object at 0x7f0f84d4f040>, <mixtensor.MixTensor object at 0x7f0f84d4ead0>))
2023-10-30 12:29:50,211 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:50,211 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,212 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f520>, (<mixtensor.MixTensor object at 0x7f0f84d4dfc0>, <mixtensor.MixTensor object at 0x7f0f84d4d7b0>))
2023-10-30 12:29:50,213 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,213 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:50,213 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:50,218 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:50,221 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,221 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,222 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:50,222 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,224 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, <mixtensor.MixTensor object at 0x7f0f84d4cf40>))
2023-10-30 12:29:50,225 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:50,225 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,226 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e170>, (<mixtensor.MixTensor object at 0x7f0f84d4de10>, <mixtensor.MixTensor object at 0x7f0f84d4e0b0>))
2023-10-30 12:29:50,226 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:50,227 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c6d0>, (<mixtensor.MixTensor object at 0x7f0f84d4c700>, <mixtensor.MixTensor object at 0x7f0f84d4f070>))
2023-10-30 12:29:50,228 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:50,228 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,230 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d060>, (<mixtensor.MixTensor object at 0x7f0f84d4e380>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:50,230 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,230 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:50,231 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:50,235 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:50,238 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,239 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,239 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:50,239 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,242 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, (<mixtensor.MixTensor object at 0x7f0f84d4cf40>, <mixtensor.MixTensor object at 0x7f0f84d4d3f0>))
2023-10-30 12:29:50,242 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:50,242 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,244 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f970>, (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, <mixtensor.MixTensor object at 0x7f0f8565ba30>))
2023-10-30 12:29:50,244 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:50,244 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,245 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, (<mixtensor.MixTensor object at 0x7f0f8565b130>, <mixtensor.MixTensor object at 0x7f0f8565b1f0>))
2023-10-30 12:29:50,245 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:50,246 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,247 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f340>, (<mixtensor.MixTensor object at 0x7f0f84d4d540>, <mixtensor.MixTensor object at 0x7f0f84d4df60>))
2023-10-30 12:29:50,247 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,247 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:50,248 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:50,252 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:50,255 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,255 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,256 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:50,256 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,259 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cf40>, (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, <mixtensor.MixTensor object at 0x7f0f8565b6d0>))
2023-10-30 12:29:50,259 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:50,259 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,261 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ad70>, (<mixtensor.MixTensor object at 0x7f0f85658c40>, <mixtensor.MixTensor object at 0x7f0f8565b940>))
2023-10-30 12:29:50,261 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:50,261 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,262 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, (<mixtensor.MixTensor object at 0x7f0f8565bac0>, <mixtensor.MixTensor object at 0x7f0f8565ba90>))
2023-10-30 12:29:50,262 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:50,263 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,264 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f550>, (<mixtensor.MixTensor object at 0x7f0f8565b6d0>, <mixtensor.MixTensor object at 0x7f0f8565b220>))
2023-10-30 12:29:50,265 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,265 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:50,265 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:50,270 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:50,273 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,273 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,273 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:50,274 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,276 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85659c00>, (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, <mixtensor.MixTensor object at 0x7f0f8565b7c0>))
2023-10-30 12:29:50,276 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:50,277 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,278 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658190>, (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, <mixtensor.MixTensor object at 0x7f0f84d444f0>))
2023-10-30 12:29:50,278 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:50,279 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,280 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44520>, (<mixtensor.MixTensor object at 0x7f0f84d45510>, <mixtensor.MixTensor object at 0x7f0f84d44370>))
2023-10-30 12:29:50,280 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:50,280 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,282 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, <mixtensor.MixTensor object at 0x7f0f8565b310>))
2023-10-30 12:29:50,282 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,282 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:50,283 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:50,287 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:50,291 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,291 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,291 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:50,291 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,294 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d0c0>, (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, <mixtensor.MixTensor object at 0x7f0f84d451e0>))
2023-10-30 12:29:50,294 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:50,294 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,296 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44c70>, (<mixtensor.MixTensor object at 0x7f0f84d442e0>, <mixtensor.MixTensor object at 0x7f0f84d44eb0>))
2023-10-30 12:29:50,296 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:50,296 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,298 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b80>, (<mixtensor.MixTensor object at 0x7f0f84d45330>, <mixtensor.MixTensor object at 0x7f0f84d44580>))
2023-10-30 12:29:50,298 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:50,298 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,299 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3f0>, (<mixtensor.MixTensor object at 0x7f0f84d451e0>, <mixtensor.MixTensor object at 0x7f0f84d445b0>))
2023-10-30 12:29:50,299 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,300 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:50,300 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:50,304 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:50,308 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,308 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,308 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:50,308 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,311 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45150>, (<mixtensor.MixTensor object at 0x7f0f84d44520>, <mixtensor.MixTensor object at 0x7f0f84d44ee0>))
2023-10-30 12:29:50,311 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:50,311 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,313 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447c0>, (<mixtensor.MixTensor object at 0x7f0f84d45270>, <mixtensor.MixTensor object at 0x7f0f8565b3a0>))
2023-10-30 12:29:50,313 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:50,313 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,315 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c610>, (<mixtensor.MixTensor object at 0x7f0f8572c820>, <mixtensor.MixTensor object at 0x7f0f8572c430>))
2023-10-30 12:29:50,315 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:50,315 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,317 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c9a0>, (<mixtensor.MixTensor object at 0x7f0f8572c1f0>, <mixtensor.MixTensor object at 0x7f0f8572fe80>))
2023-10-30 12:29:50,317 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,317 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:50,318 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:50,322 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:50,325 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,326 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,326 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:50,326 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,329 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e170>, (<mixtensor.MixTensor object at 0x7f0f84d4d900>, <mixtensor.MixTensor object at 0x7f0f84d4e7a0>))
2023-10-30 12:29:50,329 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:50,329 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,331 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, (<mixtensor.MixTensor object at 0x7f0f84d4f610>, <mixtensor.MixTensor object at 0x7f0f84d4cf40>))
2023-10-30 12:29:50,331 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:50,331 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,332 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f3d0>, (<mixtensor.MixTensor object at 0x7f0f84d4f310>, <mixtensor.MixTensor object at 0x7f0f84d4cdf0>))
2023-10-30 12:29:50,333 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:50,333 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,334 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fa60>, (<mixtensor.MixTensor object at 0x7f0f84d4f340>, <mixtensor.MixTensor object at 0x7f0f84d4ffa0>))
2023-10-30 12:29:50,334 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,335 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:50,335 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:50,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:50,343 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,343 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,916 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:50,916 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,919 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, (<mixtensor.MixTensor object at 0x7f0f84d4d5d0>, <mixtensor.MixTensor object at 0x7f0f84d4c6d0>))
2023-10-30 12:29:50,919 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:50,919 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,921 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f520>, (<mixtensor.MixTensor object at 0x7f0f84d4c850>, <mixtensor.MixTensor object at 0x7f0f84d4d8a0>))
2023-10-30 12:29:50,921 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:50,921 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,923 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c310>, (<mixtensor.MixTensor object at 0x7f0f84d4f130>, <mixtensor.MixTensor object at 0x7f0f84d4ee30>))
2023-10-30 12:29:50,923 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:50,924 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,926 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e170>, (<mixtensor.MixTensor object at 0x7f0f84d4d900>, <mixtensor.MixTensor object at 0x7f0f84d4f2b0>))
2023-10-30 12:29:50,926 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,926 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:50,927 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:50,928 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:50,932 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,932 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,932 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:50,932 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,935 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d5d0>, (<mixtensor.MixTensor object at 0x7f0f84d4c6d0>, <mixtensor.MixTensor object at 0x7f0f84d447c0>))
2023-10-30 12:29:50,935 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:50,935 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,937 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44c70>, (<mixtensor.MixTensor object at 0x7f0f84d44b80>, <mixtensor.MixTensor object at 0x7f0f84d44d90>))
2023-10-30 12:29:50,937 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:50,937 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,939 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d453c0>, (<mixtensor.MixTensor object at 0x7f0f84d448e0>, <mixtensor.MixTensor object at 0x7f0f84d45090>))
2023-10-30 12:29:50,939 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:50,939 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:50,941 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d0c0>, (<mixtensor.MixTensor object at 0x7f0f84d447c0>, <mixtensor.MixTensor object at 0x7f0f84d44160>))
2023-10-30 12:29:50,941 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 12:29:50,941 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:50,941 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:50,945 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:50,946 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,946 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,946 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:50,946 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,947 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e1d0>
2023-10-30 12:29:50,947 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:50,947 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,947 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f3d0>
2023-10-30 12:29:50,947 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:50,947 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,948 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d5d0>
2023-10-30 12:29:50,948 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:50,948 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,948 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fa30>
2023-10-30 12:29:50,948 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,949 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:50,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:50,950 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:50,950 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:50,951 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,951 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:50,951 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,958 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fd30>
2023-10-30 12:29:50,959 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:50,959 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,964 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f550>
2023-10-30 12:29:50,965 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:50,965 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,971 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d0c0>
2023-10-30 12:29:50,971 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:50,971 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:50,977 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e170>
2023-10-30 12:29:50,978 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:50,980 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:50,980 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:50,981 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:50,981 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:50,981 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,981 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:50,982 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,982 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c6a0>
2023-10-30 12:29:50,982 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:50,982 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,982 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c760>
2023-10-30 12:29:50,983 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:50,983 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,983 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d870>
2023-10-30 12:29:50,983 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:50,983 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:50,983 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cb20>
2023-10-30 12:29:50,984 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,984 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:50,984 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:50,989 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:50,989 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-30 12:29:50,989 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:50,990 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:50,990 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs_k: {}
2023-10-30 12:29:50,990 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c520>
2023-10-30 12:29:50,990 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:50,991 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs_k: {}
2023-10-30 12:29:50,991 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dab0>
2023-10-30 12:29:50,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:50,991 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs_k: {}
2023-10-30 12:29:50,991 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d9f0>
2023-10-30 12:29:50,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:50,992 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs_k: {}
2023-10-30 12:29:50,992 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cb50>
2023-10-30 12:29:50,992 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:50,993 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:50,993 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:50,997 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,000 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,000 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,001 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:51,001 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,004 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fee0>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4eaa0>))
2023-10-30 12:29:51,004 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:51,004 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,006 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f490>, (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, <mixtensor.MixTensor object at 0x7f0f84d4d300>))
2023-10-30 12:29:51,006 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:51,006 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,007 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb80>, (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, <mixtensor.MixTensor object at 0x7f0f84d4e0e0>))
2023-10-30 12:29:51,008 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:51,008 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,009 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f280>, (<mixtensor.MixTensor object at 0x7f0f84d4ffd0>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:51,009 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,010 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:51,010 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,015 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:51,018 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,018 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,019 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:51,019 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,021 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d3c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4fbb0>))
2023-10-30 12:29:51,022 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:51,022 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,023 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, <mixtensor.MixTensor object at 0x7f0f84d4fa00>))
2023-10-30 12:29:51,023 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:51,024 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,025 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c2e0>, (<mixtensor.MixTensor object at 0x7f0f84d4fc40>, <mixtensor.MixTensor object at 0x7f0f84d4f580>))
2023-10-30 12:29:51,025 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:51,025 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,027 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e590>, (<mixtensor.MixTensor object at 0x7f0f84d4c640>, <mixtensor.MixTensor object at 0x7f0f84d4e410>))
2023-10-30 12:29:51,027 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,027 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:51,028 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,032 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,036 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,036 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,036 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:51,037 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,039 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f880>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4fbb0>))
2023-10-30 12:29:51,039 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:51,040 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,041 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d2a0>, (<mixtensor.MixTensor object at 0x7f0f84d4ec50>, <mixtensor.MixTensor object at 0x7f0f84d4e8c0>))
2023-10-30 12:29:51,041 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:51,042 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,044 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f3d0>, (<mixtensor.MixTensor object at 0x7f0f84d4d5d0>, <mixtensor.MixTensor object at 0x7f0f84d4fa30>))
2023-10-30 12:29:51,044 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:51,044 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,046 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, (<mixtensor.MixTensor object at 0x7f0f84d4d3c0>, <mixtensor.MixTensor object at 0x7f0f84d4e3b0>))
2023-10-30 12:29:51,046 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,047 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:51,047 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,052 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,055 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,056 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,056 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:51,056 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,059 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f430>, (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, <mixtensor.MixTensor object at 0x7f0f84d4cd90>))
2023-10-30 12:29:51,059 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:51,059 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,061 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe20>, (<mixtensor.MixTensor object at 0x7f0f84d4f550>, <mixtensor.MixTensor object at 0x7f0f84d4fd30>))
2023-10-30 12:29:51,061 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:51,061 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,063 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ceb0>, (<mixtensor.MixTensor object at 0x7f0f84d4f520>, <mixtensor.MixTensor object at 0x7f0f84d4c310>))
2023-10-30 12:29:51,063 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:51,063 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,065 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4eaa0>, <mixtensor.MixTensor object at 0x7f0f84d4fa60>))
2023-10-30 12:29:51,065 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,065 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:51,066 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,070 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,074 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,074 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,074 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:51,075 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,077 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, <mixtensor.MixTensor object at 0x7f0f84d44700>))
2023-10-30 12:29:51,077 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:51,078 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,079 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, (<mixtensor.MixTensor object at 0x7f0f84d44e80>, <mixtensor.MixTensor object at 0x7f0f84d454b0>))
2023-10-30 12:29:51,079 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:51,079 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,081 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44b20>, (<mixtensor.MixTensor object at 0x7f0f84d440a0>, <mixtensor.MixTensor object at 0x7f0f84d45360>))
2023-10-30 12:29:51,081 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:51,081 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,082 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d240>, (<mixtensor.MixTensor object at 0x7f0f84d44700>, <mixtensor.MixTensor object at 0x7f0f84d44820>))
2023-10-30 12:29:51,083 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,083 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:51,083 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,088 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,091 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,091 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,091 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:51,091 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,094 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd90>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:51,095 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:51,095 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,097 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d446d0>, (<mixtensor.MixTensor object at 0x7f0f84d44b50>, <mixtensor.MixTensor object at 0x7f0f84d44e50>))
2023-10-30 12:29:51,097 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:51,097 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,098 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44dc0>, (<mixtensor.MixTensor object at 0x7f0f84d44490>, <mixtensor.MixTensor object at 0x7f0f84d44fa0>))
2023-10-30 12:29:51,098 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:51,099 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,100 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, (<mixtensor.MixTensor object at 0x7f0f84d44040>, <mixtensor.MixTensor object at 0x7f0f84d44220>))
2023-10-30 12:29:51,100 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,100 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:51,101 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,105 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,109 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,109 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,109 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:51,109 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,112 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, (<mixtensor.MixTensor object at 0x7f0f84d44b20>, <mixtensor.MixTensor object at 0x7f0f84d44d00>))
2023-10-30 12:29:51,112 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:51,112 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,114 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d441c0>, (<mixtensor.MixTensor object at 0x7f0f84d44af0>, <mixtensor.MixTensor object at 0x7f0f84d44730>))
2023-10-30 12:29:51,114 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:51,114 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,116 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44880>, (<mixtensor.MixTensor object at 0x7f0f84d454e0>, <mixtensor.MixTensor object at 0x7f0f84d45300>))
2023-10-30 12:29:51,116 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:51,116 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,117 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44d00>, (<mixtensor.MixTensor object at 0x7f0f84d441f0>, <mixtensor.MixTensor object at 0x7f0f84d44a90>))
2023-10-30 12:29:51,117 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,118 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:51,118 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,123 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,126 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,126 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,127 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:51,127 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,129 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44dc0>, (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, <mixtensor.MixTensor object at 0x7f0f84d45060>))
2023-10-30 12:29:51,129 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:51,130 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,131 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d44910>, <mixtensor.MixTensor object at 0x7f0f84d45240>))
2023-10-30 12:29:51,131 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:51,131 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,133 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44340>, (<mixtensor.MixTensor object at 0x7f0f84d44070>, <mixtensor.MixTensor object at 0x7f0f84d45210>))
2023-10-30 12:29:51,133 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:51,133 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,135 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44310>, (<mixtensor.MixTensor object at 0x7f0f84d446d0>, <mixtensor.MixTensor object at 0x7f0f84d449d0>))
2023-10-30 12:29:51,135 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,135 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:51,135 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,140 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,143 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,143 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,144 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:51,144 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,146 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44dc0>, (<mixtensor.MixTensor object at 0x7f0f84d44ac0>, <mixtensor.MixTensor object at 0x7f0f84d44760>))
2023-10-30 12:29:51,147 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:51,147 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,149 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d44f10>, <mixtensor.MixTensor object at 0x7f0f8572cb80>))
2023-10-30 12:29:51,149 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:51,149 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,151 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c340>, (<mixtensor.MixTensor object at 0x7f0f8572d840>, <mixtensor.MixTensor object at 0x7f0f8572d870>))
2023-10-30 12:29:51,151 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:51,151 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,152 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c730>, (<mixtensor.MixTensor object at 0x7f0f8572d810>, <mixtensor.MixTensor object at 0x7f0f8572dd20>))
2023-10-30 12:29:51,153 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,153 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:51,153 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,158 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,161 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,161 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,162 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:51,162 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,165 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, <mixtensor.MixTensor object at 0x7f0f84d4ec80>))
2023-10-30 12:29:51,165 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:51,165 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,167 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f430>, (<mixtensor.MixTensor object at 0x7f0f84d4fd90>, <mixtensor.MixTensor object at 0x7f0f84d4ff40>))
2023-10-30 12:29:51,167 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:51,167 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,169 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ec20>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4f880>))
2023-10-30 12:29:51,169 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:51,169 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,170 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4e140>, <mixtensor.MixTensor object at 0x7f0f84d4f280>))
2023-10-30 12:29:51,171 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,171 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:51,171 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,176 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,180 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,180 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,180 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:51,180 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,183 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, <mixtensor.MixTensor object at 0x7f0f8565b070>))
2023-10-30 12:29:51,183 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:51,183 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,185 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b0d0>, (<mixtensor.MixTensor object at 0x7f0f85658190>, <mixtensor.MixTensor object at 0x7f0f8565aef0>))
2023-10-30 12:29:51,185 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:51,185 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,187 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f856581f0>, (<mixtensor.MixTensor object at 0x7f0f8565bc40>, <mixtensor.MixTensor object at 0x7f0f8565ad70>))
2023-10-30 12:29:51,187 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:51,187 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,188 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b9d0>, (<mixtensor.MixTensor object at 0x7f0f8565b070>, <mixtensor.MixTensor object at 0x7f0f8565ba60>))
2023-10-30 12:29:51,189 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,189 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:51,189 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,190 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,193 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,194 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,194 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:51,194 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,197 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d960>, (<mixtensor.MixTensor object at 0x7f0f84d4f3d0>, <mixtensor.MixTensor object at 0x7f0f8565ae60>))
2023-10-30 12:29:51,197 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:51,197 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,199 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bb80>, (<mixtensor.MixTensor object at 0x7f0f8565b640>, <mixtensor.MixTensor object at 0x7f0f84d45390>))
2023-10-30 12:29:51,199 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:51,199 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,205 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d443a0>, (<mixtensor.MixTensor object at 0x7f0f84d44340>, <mixtensor.MixTensor object at 0x7f0f84d44310>))
2023-10-30 12:29:51,205 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:51,205 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,207 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4ff10>))
2023-10-30 12:29:51,207 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 12:29:51,207 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:51,208 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,212 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,212 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,212 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,213 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:51,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,213 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c970>
2023-10-30 12:29:51,213 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:51,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,214 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ff70>
2023-10-30 12:29:51,214 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:51,214 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,214 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d3f0>
2023-10-30 12:29:51,214 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:51,215 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,215 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c970>
2023-10-30 12:29:51,215 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,215 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:51,215 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,216 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,216 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,216 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,217 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:51,217 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,226 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cb50>
2023-10-30 12:29:51,226 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:51,226 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,232 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d9f0>
2023-10-30 12:29:51,232 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:51,232 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,238 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cee0>
2023-10-30 12:29:51,238 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:51,239 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,244 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c370>
2023-10-30 12:29:51,245 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:51,246 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:51,246 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,247 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,247 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:51,248 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,248 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:51,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,248 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cee0>
2023-10-30 12:29:51,248 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:51,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,249 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dab0>
2023-10-30 12:29:51,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:51,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,249 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cf10>
2023-10-30 12:29:51,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:51,250 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,250 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cb50>
2023-10-30 12:29:51,250 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,250 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:51,250 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,255 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,255 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-30 12:29:51,256 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,256 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:51,256 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs_k: {}
2023-10-30 12:29:51,256 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cfa0>
2023-10-30 12:29:51,256 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:51,257 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs_k: {}
2023-10-30 12:29:51,257 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c430>
2023-10-30 12:29:51,257 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:51,257 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs_k: {}
2023-10-30 12:29:51,257 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c370>
2023-10-30 12:29:51,258 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:51,258 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs_k: {}
2023-10-30 12:29:51,258 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c820>
2023-10-30 12:29:51,258 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,259 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:51,259 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:51,264 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,267 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,267 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,268 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:51,268 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,270 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658c40>, (<mixtensor.MixTensor object at 0x7f0f8565bac0>, <mixtensor.MixTensor object at 0x7f0f8565b6d0>))
2023-10-30 12:29:51,270 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:51,271 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,272 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b220>, (<mixtensor.MixTensor object at 0x7f0f8565b7c0>, <mixtensor.MixTensor object at 0x7f0f8565b310>))
2023-10-30 12:29:51,272 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:51,273 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,274 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, (<mixtensor.MixTensor object at 0x7f0f8565ae60>, <mixtensor.MixTensor object at 0x7f0f856581f0>))
2023-10-30 12:29:51,274 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:51,274 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,276 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85658d00>, (<mixtensor.MixTensor object at 0x7f0f8565b940>, <mixtensor.MixTensor object at 0x7f0f8565ba90>))
2023-10-30 12:29:51,276 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,276 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:51,276 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,282 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:51,285 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,286 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,286 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:51,286 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,289 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e650>, (<mixtensor.MixTensor object at 0x7f0f84d4e5f0>, <mixtensor.MixTensor object at 0x7f0f84d4e5c0>))
2023-10-30 12:29:51,290 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:51,290 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,291 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdf0>, (<mixtensor.MixTensor object at 0x7f0f84d4e740>, <mixtensor.MixTensor object at 0x7f0f84d4f5e0>))
2023-10-30 12:29:51,292 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:51,292 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,294 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c1f0>, (<mixtensor.MixTensor object at 0x7f0f84d4eb00>, <mixtensor.MixTensor object at 0x7f0f84d4dff0>))
2023-10-30 12:29:51,294 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:51,294 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,296 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eda0>, (<mixtensor.MixTensor object at 0x7f0f84d4f850>, <mixtensor.MixTensor object at 0x7f0f84d4ce80>))
2023-10-30 12:29:51,296 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,296 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:51,296 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,301 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,305 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,305 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,305 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:51,306 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,308 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df90>, (<mixtensor.MixTensor object at 0x7f0f84d4e650>, <mixtensor.MixTensor object at 0x7f0f84d4e5f0>))
2023-10-30 12:29:51,308 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:51,309 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,310 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4feb0>, (<mixtensor.MixTensor object at 0x7f0f84d4e680>, <mixtensor.MixTensor object at 0x7f0f84d4fc10>))
2023-10-30 12:29:51,310 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:51,311 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,312 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f040>, (<mixtensor.MixTensor object at 0x7f0f84d4dfc0>, <mixtensor.MixTensor object at 0x7f0f84d4c5b0>))
2023-10-30 12:29:51,312 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:51,312 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,314 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, (<mixtensor.MixTensor object at 0x7f0f84d4fa90>, <mixtensor.MixTensor object at 0x7f0f84d4d090>))
2023-10-30 12:29:51,314 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,314 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:51,314 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,319 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,322 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,322 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,323 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:51,323 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,325 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e650>, (<mixtensor.MixTensor object at 0x7f0f84d4e5f0>, <mixtensor.MixTensor object at 0x7f0f84d4d870>))
2023-10-30 12:29:51,326 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:51,326 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,327 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d7b0>, (<mixtensor.MixTensor object at 0x7f0f84d4de10>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:51,328 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:51,328 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,329 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dc90>, (<mixtensor.MixTensor object at 0x7f0f84d4d540>, <mixtensor.MixTensor object at 0x7f0f84d4df60>))
2023-10-30 12:29:51,329 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:51,329 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,331 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4eef0>, <mixtensor.MixTensor object at 0x7f0f84d4ead0>))
2023-10-30 12:29:51,331 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,331 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:51,332 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,336 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,339 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,339 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,340 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:51,340 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,342 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e5f0>, (<mixtensor.MixTensor object at 0x7f0f84d4d870>, <mixtensor.MixTensor object at 0x7f0f84d4d8d0>))
2023-10-30 12:29:51,343 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:51,343 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,345 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f310>, (<mixtensor.MixTensor object at 0x7f0f84d4f340>, <mixtensor.MixTensor object at 0x7f0f84d4ffa0>))
2023-10-30 12:29:51,345 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:51,345 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,346 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c850>, (<mixtensor.MixTensor object at 0x7f0f84d4f130>, <mixtensor.MixTensor object at 0x7f0f84d4d900>))
2023-10-30 12:29:51,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:51,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,348 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdf0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d4f610>))
2023-10-30 12:29:51,348 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,348 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:51,349 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,353 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,356 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,357 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,357 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:51,357 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,360 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d870>, (<mixtensor.MixTensor object at 0x7f0f84d4d8d0>, <mixtensor.MixTensor object at 0x7f0f84d4ee30>))
2023-10-30 12:29:51,360 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:51,360 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,365 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f640>, (<mixtensor.MixTensor object at 0x7f0f84d4ff70>, <mixtensor.MixTensor object at 0x7f0f84d4f3d0>))
2023-10-30 12:29:51,365 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:51,365 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,367 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f6a0>, <mixtensor.MixTensor object at 0x7f0f84d4fbb0>))
2023-10-30 12:29:51,367 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:51,367 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,368 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd60>, (<mixtensor.MixTensor object at 0x7f0f84d4ee90>, <mixtensor.MixTensor object at 0x7f0f84d4f2b0>))
2023-10-30 12:29:51,369 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,369 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:51,369 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,374 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,377 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,377 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:51,378 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,381 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d8d0>, (<mixtensor.MixTensor object at 0x7f0f84d4ee30>, <mixtensor.MixTensor object at 0x7f0f84d4ceb0>))
2023-10-30 12:29:51,381 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:51,382 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,383 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d960>, (<mixtensor.MixTensor object at 0x7f0f84d44a00>, <mixtensor.MixTensor object at 0x7f0f84d45540>))
2023-10-30 12:29:51,383 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:51,384 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,385 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, (<mixtensor.MixTensor object at 0x7f0f84d45510>, <mixtensor.MixTensor object at 0x7f0f84d44790>))
2023-10-30 12:29:51,385 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:51,385 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,387 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4e770>, <mixtensor.MixTensor object at 0x7f0f84d4ec20>))
2023-10-30 12:29:51,387 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,387 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:51,387 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,395 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,395 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,396 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:51,396 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,398 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ee30>, (<mixtensor.MixTensor object at 0x7f0f84d4ceb0>, <mixtensor.MixTensor object at 0x7f0f84d444f0>))
2023-10-30 12:29:51,399 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:51,399 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,400 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44cd0>, (<mixtensor.MixTensor object at 0x7f0f84d44a60>, <mixtensor.MixTensor object at 0x7f0f84d44100>))
2023-10-30 12:29:51,400 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:51,401 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,402 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44eb0>, (<mixtensor.MixTensor object at 0x7f0f84d44580>, <mixtensor.MixTensor object at 0x7f0f84d445b0>))
2023-10-30 12:29:51,402 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:51,402 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,404 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f370>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f84d44370>))
2023-10-30 12:29:51,404 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,404 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:51,405 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,409 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,413 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,413 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,413 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:51,413 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,416 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445e0>, (<mixtensor.MixTensor object at 0x7f0f84d44bb0>, <mixtensor.MixTensor object at 0x7f0f84d45150>))
2023-10-30 12:29:51,416 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:51,416 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,578 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44ee0>, (<mixtensor.MixTensor object at 0x7f0f84d44d60>, <mixtensor.MixTensor object at 0x7f0f8588f520>))
2023-10-30 12:29:51,578 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:51,578 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,592 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588ee60>, (<mixtensor.MixTensor object at 0x7f0f8588eec0>, <mixtensor.MixTensor object at 0x7f0f8588edd0>))
2023-10-30 12:29:51,592 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:51,592 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,594 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588ec20>, (<mixtensor.MixTensor object at 0x7f0f8588ebf0>, <mixtensor.MixTensor object at 0x7f0f8588ebc0>))
2023-10-30 12:29:51,594 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,594 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:51,594 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,599 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,602 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,602 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,603 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:51,603 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,605 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588e200>, (<mixtensor.MixTensor object at 0x7f0f8588e1d0>, <mixtensor.MixTensor object at 0x7f0f8588e1a0>))
2023-10-30 12:29:51,606 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:51,606 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,607 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588e0e0>, (<mixtensor.MixTensor object at 0x7f0f8588e0b0>, <mixtensor.MixTensor object at 0x7f0f8588e080>))
2023-10-30 12:29:51,607 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:51,608 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,609 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588dfc0>, (<mixtensor.MixTensor object at 0x7f0f8588df90>, <mixtensor.MixTensor object at 0x7f0f8588df60>))
2023-10-30 12:29:51,609 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:51,609 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,611 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588dff0>, (<mixtensor.MixTensor object at 0x7f0f8588df30>, <mixtensor.MixTensor object at 0x7f0f8588df00>))
2023-10-30 12:29:51,611 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,611 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:51,612 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,616 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,619 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,620 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,620 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:51,620 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,623 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588d9c0>, (<mixtensor.MixTensor object at 0x7f0f8588d990>, <mixtensor.MixTensor object at 0x7f0f8588d900>))
2023-10-30 12:29:51,623 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:51,623 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,625 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588d870>, (<mixtensor.MixTensor object at 0x7f0f8588d7e0>, <mixtensor.MixTensor object at 0x7f0f8588d690>))
2023-10-30 12:29:51,625 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:51,625 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,627 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588c760>, (<mixtensor.MixTensor object at 0x7f0f8584fbe0>, <mixtensor.MixTensor object at 0x7f0f859b28c0>))
2023-10-30 12:29:51,628 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:51,628 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,631 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588cfd0>, (<mixtensor.MixTensor object at 0x7f0f859b2920>, <mixtensor.MixTensor object at 0x7f0f859b2980>))
2023-10-30 12:29:51,631 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,631 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:51,632 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,633 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,637 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,637 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,637 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:51,638 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,640 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f87434d60>, (<mixtensor.MixTensor object at 0x7f0f87434d00>, <mixtensor.MixTensor object at 0x7f0f87434ca0>))
2023-10-30 12:29:51,640 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:51,640 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,642 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f87434bb0>, (<mixtensor.MixTensor object at 0x7f0f87434c10>, <mixtensor.MixTensor object at 0x7f0f87434be0>))
2023-10-30 12:29:51,642 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:51,642 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,644 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f87434ac0>, (<mixtensor.MixTensor object at 0x7f0f87434af0>, <mixtensor.MixTensor object at 0x7f0f87434a90>))
2023-10-30 12:29:51,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:51,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,645 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f87434b80>, (<mixtensor.MixTensor object at 0x7f0f87434a30>, <mixtensor.MixTensor object at 0x7f0f87434a00>))
2023-10-30 12:29:51,646 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 12:29:51,646 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:51,646 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,650 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,651 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,651 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,651 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:51,651 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,651 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f87434d30>
2023-10-30 12:29:51,651 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:51,652 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,652 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283ac50>
2023-10-30 12:29:51,652 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:51,652 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,652 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283abc0>
2023-10-30 12:29:51,652 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:51,652 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,653 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283ab60>
2023-10-30 12:29:51,653 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,653 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:51,653 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,654 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,654 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,654 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,654 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:51,654 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,661 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a9e0>
2023-10-30 12:29:51,661 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:51,661 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,667 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a860>
2023-10-30 12:29:51,667 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:51,667 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,673 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a7a0>
2023-10-30 12:29:51,673 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:51,673 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,679 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a740>
2023-10-30 12:29:51,680 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:51,681 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:51,682 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,682 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,683 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:51,683 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,683 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:51,683 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,684 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283aa10>
2023-10-30 12:29:51,684 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:51,684 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,684 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a5f0>
2023-10-30 12:29:51,684 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:51,684 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,684 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a530>
2023-10-30 12:29:51,685 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:51,685 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,685 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a4d0>
2023-10-30 12:29:51,685 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,685 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:51,686 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,690 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,690 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-30 12:29:51,690 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,691 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:51,691 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs_k: {}
2023-10-30 12:29:51,691 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a860>
2023-10-30 12:29:51,691 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:51,691 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs_k: {}
2023-10-30 12:29:51,692 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe2eff7f0>
2023-10-30 12:29:51,692 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:51,692 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs_k: {}
2023-10-30 12:29:51,692 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe2eff6a0>
2023-10-30 12:29:51,692 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:51,692 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs_k: {}
2023-10-30 12:29:51,693 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283abc0>
2023-10-30 12:29:51,693 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,693 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:51,693 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:51,698 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,701 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,701 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,701 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:51,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,704 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e200>, (<mixtensor.MixTensor object at 0x7f0f8576e380>, <mixtensor.MixTensor object at 0x7f0f8576e1d0>))
2023-10-30 12:29:51,704 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:51,705 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,706 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e3b0>, (<mixtensor.MixTensor object at 0x7f0f8576e350>, <mixtensor.MixTensor object at 0x7f0f8576e2f0>))
2023-10-30 12:29:51,707 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:51,707 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,708 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e3e0>, (<mixtensor.MixTensor object at 0x7f0f8576e530>, <mixtensor.MixTensor object at 0x7f0f8576e5c0>))
2023-10-30 12:29:51,708 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:51,708 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,710 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e410>, (<mixtensor.MixTensor object at 0x7f0f8576e5f0>, <mixtensor.MixTensor object at 0x7f0f8576e500>))
2023-10-30 12:29:51,710 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,710 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:51,711 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,715 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:51,718 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,718 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,719 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:51,719 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,721 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576efb0>, (<mixtensor.MixTensor object at 0x7f0f8576f130>, <mixtensor.MixTensor object at 0x7f0f8576ef80>))
2023-10-30 12:29:51,722 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:51,722 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,724 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576f160>, (<mixtensor.MixTensor object at 0x7f0f8576f100>, <mixtensor.MixTensor object at 0x7f0f8576f0a0>))
2023-10-30 12:29:51,724 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:51,724 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,725 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576f190>, (<mixtensor.MixTensor object at 0x7f0f8576c0d0>, <mixtensor.MixTensor object at 0x7f0f8576f2e0>))
2023-10-30 12:29:51,726 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:51,726 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,727 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c670>, (<mixtensor.MixTensor object at 0x7f0f8576f310>, <mixtensor.MixTensor object at 0x7f0f8576cca0>))
2023-10-30 12:29:51,727 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,728 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:51,728 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,732 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:51,736 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,736 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,737 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:51,737 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,740 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c040>, (<mixtensor.MixTensor object at 0x7f0f8576d810>, <mixtensor.MixTensor object at 0x7f0f8576d840>))
2023-10-30 12:29:51,741 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:51,741 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,744 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576d600>, (<mixtensor.MixTensor object at 0x7f0f8576d7b0>, <mixtensor.MixTensor object at 0x7f0f8576d660>))
2023-10-30 12:29:51,744 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:51,744 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,746 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576d630>, (<mixtensor.MixTensor object at 0x7f0f8576d690>, <mixtensor.MixTensor object at 0x7f0f8576d540>))
2023-10-30 12:29:51,746 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:51,746 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,748 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576d5d0>, (<mixtensor.MixTensor object at 0x7f0f8576d570>, <mixtensor.MixTensor object at 0x7f0f8576d3f0>))
2023-10-30 12:29:51,749 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,749 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:51,750 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,754 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:51,758 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,758 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,758 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:51,759 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,763 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ceb0>, (<mixtensor.MixTensor object at 0x7f0f8576cee0>, <mixtensor.MixTensor object at 0x7f0f8576cbe0>))
2023-10-30 12:29:51,763 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:51,763 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,765 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ce20>, (<mixtensor.MixTensor object at 0x7f0f8576cbb0>, <mixtensor.MixTensor object at 0x7f0f8576cc10>))
2023-10-30 12:29:51,765 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:51,765 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,767 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576cb50>, (<mixtensor.MixTensor object at 0x7f0f8576cb80>, <mixtensor.MixTensor object at 0x7f0f8576ca30>))
2023-10-30 12:29:51,767 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:51,767 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,768 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c9a0>, (<mixtensor.MixTensor object at 0x7f0f8576ca60>, <mixtensor.MixTensor object at 0x7f0f8576c940>))
2023-10-30 12:29:51,768 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,769 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:51,769 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,773 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:51,776 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,777 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,777 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:51,777 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,780 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c310>, (<mixtensor.MixTensor object at 0x7f0f8576c400>, <mixtensor.MixTensor object at 0x7f0f8576c3a0>))
2023-10-30 12:29:51,780 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:51,780 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,782 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c520>, (<mixtensor.MixTensor object at 0x7f0f8576d120>, <mixtensor.MixTensor object at 0x7f0f8576f4f0>))
2023-10-30 12:29:51,782 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:51,783 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,784 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c2e0>, (<mixtensor.MixTensor object at 0x7f0f8576c370>, <mixtensor.MixTensor object at 0x7f0f8576c1f0>))
2023-10-30 12:29:51,784 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:51,784 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,786 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c280>, (<mixtensor.MixTensor object at 0x7f0f8576c0a0>, <mixtensor.MixTensor object at 0x7f0f8576c220>))
2023-10-30 12:29:51,786 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,786 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:51,786 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,791 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:51,794 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,794 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,795 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:51,795 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,798 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576fbe0>, (<mixtensor.MixTensor object at 0x7f0f8576fd60>, <mixtensor.MixTensor object at 0x7f0f8576fbb0>))
2023-10-30 12:29:51,798 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:51,798 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,800 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576fd90>, (<mixtensor.MixTensor object at 0x7f0f8576fd30>, <mixtensor.MixTensor object at 0x7f0f8576fcd0>))
2023-10-30 12:29:51,800 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:51,800 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,801 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576fdc0>, (<mixtensor.MixTensor object at 0x7f0f8576ff10>, <mixtensor.MixTensor object at 0x7f0f8576fe50>))
2023-10-30 12:29:51,802 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:51,802 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,803 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ff70>, (<mixtensor.MixTensor object at 0x7f0f8576feb0>, <mixtensor.MixTensor object at 0x7f0f8576ffa0>))
2023-10-30 12:29:51,804 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,804 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:51,804 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,809 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:51,812 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,812 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,812 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:51,813 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,815 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576dbd0>, (<mixtensor.MixTensor object at 0x7f0f8576dcc0>, <mixtensor.MixTensor object at 0x7f0f8576e290>))
2023-10-30 12:29:51,815 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:51,815 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,817 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e9e0>, (<mixtensor.MixTensor object at 0x7f0f8576e710>, <mixtensor.MixTensor object at 0x7f0f8576e920>))
2023-10-30 12:29:51,817 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:51,817 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,819 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576df90>, (<mixtensor.MixTensor object at 0x7f0f8576e4d0>, <mixtensor.MixTensor object at 0x7f0f8576f610>))
2023-10-30 12:29:51,819 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:51,819 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,820 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e590>, (<mixtensor.MixTensor object at 0x7f0f8576cd60>, <mixtensor.MixTensor object at 0x7f0f8576ca00>))
2023-10-30 12:29:51,820 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,821 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:51,821 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,825 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:51,828 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,829 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,829 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:51,829 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,832 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c0d0>, (<mixtensor.MixTensor object at 0x7f0f8579c040>, <mixtensor.MixTensor object at 0x7f0f8579c160>))
2023-10-30 12:29:51,832 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:51,832 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,834 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c1f0>, (<mixtensor.MixTensor object at 0x7f0f8579c190>, <mixtensor.MixTensor object at 0x7f0f8579c130>))
2023-10-30 12:29:51,834 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:51,834 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,835 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c220>, (<mixtensor.MixTensor object at 0x7f0f8579c370>, <mixtensor.MixTensor object at 0x7f0f8579c400>))
2023-10-30 12:29:51,835 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:51,836 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,837 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c250>, (<mixtensor.MixTensor object at 0x7f0f8579c430>, <mixtensor.MixTensor object at 0x7f0f8579c340>))
2023-10-30 12:29:51,837 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,837 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:51,838 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,842 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:51,845 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,846 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,846 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:51,846 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,849 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c5b0>, (<mixtensor.MixTensor object at 0x7f0f8579c700>, <mixtensor.MixTensor object at 0x7f0f8579c880>))
2023-10-30 12:29:51,849 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:51,849 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,851 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c7c0>, (<mixtensor.MixTensor object at 0x7f0f8579c8e0>, <mixtensor.MixTensor object at 0x7f0f8579c8b0>))
2023-10-30 12:29:51,851 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:51,851 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,853 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579caf0>, (<mixtensor.MixTensor object at 0x7f0f8579c940>, <mixtensor.MixTensor object at 0x7f0f8579ca90>))
2023-10-30 12:29:51,853 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:51,853 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,855 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c970>, (<mixtensor.MixTensor object at 0x7f0f8579c9d0>, <mixtensor.MixTensor object at 0x7f0f8579ca30>))
2023-10-30 12:29:51,856 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,856 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:51,857 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,862 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:51,867 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,867 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,867 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:51,868 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,870 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579cd30>, (<mixtensor.MixTensor object at 0x7f0f8579ccd0>, <mixtensor.MixTensor object at 0x7f0f8579cc70>))
2023-10-30 12:29:51,870 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:51,870 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,872 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579cd60>, (<mixtensor.MixTensor object at 0x7f0f8579ceb0>, <mixtensor.MixTensor object at 0x7f0f8579cdf0>))
2023-10-30 12:29:51,872 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:51,872 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,873 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ce80>, (<mixtensor.MixTensor object at 0x7f0f8579cfd0>, <mixtensor.MixTensor object at 0x7f0f8579d0f0>))
2023-10-30 12:29:51,874 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:51,874 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,875 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579cee0>, (<mixtensor.MixTensor object at 0x7f0f8579cfa0>, <mixtensor.MixTensor object at 0x7f0f8579cf70>))
2023-10-30 12:29:51,875 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,876 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:51,876 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,880 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:51,883 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,883 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,884 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:51,884 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,887 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d270>, (<mixtensor.MixTensor object at 0x7f0f8579d2d0>, <mixtensor.MixTensor object at 0x7f0f8579d3c0>))
2023-10-30 12:29:51,888 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:51,888 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,889 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d5d0>, (<mixtensor.MixTensor object at 0x7f0f8579d420>, <mixtensor.MixTensor object at 0x7f0f8579d3f0>))
2023-10-30 12:29:51,889 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:51,890 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,891 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d600>, (<mixtensor.MixTensor object at 0x7f0f8579d660>, <mixtensor.MixTensor object at 0x7f0f8576df90>))
2023-10-30 12:29:51,891 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:51,891 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,893 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d3c0>, (<mixtensor.MixTensor object at 0x7f0f8579d450>, <mixtensor.MixTensor object at 0x7f0f8579cd30>))
2023-10-30 12:29:51,893 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,893 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:51,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:51,898 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,898 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,898 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:51,898 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,901 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d870>, (<mixtensor.MixTensor object at 0x7f0f8579d810>, <mixtensor.MixTensor object at 0x7f0f8579d7b0>))
2023-10-30 12:29:51,901 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:51,901 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,903 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d8a0>, (<mixtensor.MixTensor object at 0x7f0f8579d9f0>, <mixtensor.MixTensor object at 0x7f0f8579d930>))
2023-10-30 12:29:51,903 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:51,903 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,904 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d9c0>, (<mixtensor.MixTensor object at 0x7f0f8579db10>, <mixtensor.MixTensor object at 0x7f0f8579dae0>))
2023-10-30 12:29:51,904 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:51,904 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:51,906 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579da20>, (<mixtensor.MixTensor object at 0x7f0f8579dc60>, <mixtensor.MixTensor object at 0x7f0f8579dab0>))
2023-10-30 12:29:51,906 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 12:29:51,906 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:51,907 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,910 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:51,911 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,911 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,911 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:51,911 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,911 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579d7b0>
2023-10-30 12:29:51,911 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:51,912 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,912 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579dea0>
2023-10-30 12:29:51,912 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:51,912 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,912 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579df90>
2023-10-30 12:29:51,912 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:51,913 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,913 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579e0e0>
2023-10-30 12:29:51,913 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,913 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:51,913 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,914 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:51,914 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:51,914 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,914 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:51,915 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,921 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe283a530>
2023-10-30 12:29:51,921 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:51,921 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,927 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579d3c0>
2023-10-30 12:29:51,927 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:51,927 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,933 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579e020>
2023-10-30 12:29:51,933 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:51,933 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:51,939 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579e0b0>
2023-10-30 12:29:51,940 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:51,941 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:51,941 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,942 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:51,942 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:51,942 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,943 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:51,943 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,943 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588edd0>
2023-10-30 12:29:51,943 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:51,943 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,943 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588dd20>
2023-10-30 12:29:51,943 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:51,944 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,944 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588d7e0>
2023-10-30 12:29:51,944 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:51,944 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:51,944 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588dc00>
2023-10-30 12:29:51,945 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:51,945 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:51,945 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:51,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:51,950 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-30 12:29:51,950 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:51,950 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:51,950 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs_k: {}
2023-10-30 12:29:51,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588ea10>
2023-10-30 12:29:51,951 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:51,951 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs_k: {}
2023-10-30 12:29:51,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f87434e20>
2023-10-30 12:29:51,951 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:51,952 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs_k: {}
2023-10-30 12:29:52,082 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f87434e50>
2023-10-30 12:29:52,083 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:52,083 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs_k: {}
2023-10-30 12:29:52,083 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe2eff8b0>
2023-10-30 12:29:52,083 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,084 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:52,084 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,089 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,092 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,092 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,093 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:52,093 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,096 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f87434ac0>, (<mixtensor.MixTensor object at 0x7f0f8579d2d0>, <mixtensor.MixTensor object at 0x7f0f8579d7b0>))
2023-10-30 12:29:52,097 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:52,097 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,099 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d3c0>, (<mixtensor.MixTensor object at 0x7f0f8579e020>, <mixtensor.MixTensor object at 0x7f0f8579e0b0>))
2023-10-30 12:29:52,099 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:52,099 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,101 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e320>, (<mixtensor.MixTensor object at 0x7f0f8579e170>, <mixtensor.MixTensor object at 0x7f0f8579e140>))
2023-10-30 12:29:52,101 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:52,101 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,103 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e1a0>, (<mixtensor.MixTensor object at 0x7f0f8579e290>, <mixtensor.MixTensor object at 0x7f0f8579e260>))
2023-10-30 12:29:52,103 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,103 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:52,104 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,108 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,112 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,112 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,112 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:52,112 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,116 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e7a0>, (<mixtensor.MixTensor object at 0x7f0f8579e5f0>, <mixtensor.MixTensor object at 0x7f0f8579e740>))
2023-10-30 12:29:52,116 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:52,116 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,118 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e770>, (<mixtensor.MixTensor object at 0x7f0f8579e710>, <mixtensor.MixTensor object at 0x7f0f8579e860>))
2023-10-30 12:29:52,118 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:52,118 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,120 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e950>, (<mixtensor.MixTensor object at 0x7f0f8579e890>, <mixtensor.MixTensor object at 0x7f0f8579e8f0>))
2023-10-30 12:29:52,120 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:52,120 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,121 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e800>, (<mixtensor.MixTensor object at 0x7f0f8579e9e0>, <mixtensor.MixTensor object at 0x7f0f8579e980>))
2023-10-30 12:29:52,122 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,122 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:52,122 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,127 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,130 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,130 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,131 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:52,131 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,134 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ece0>, (<mixtensor.MixTensor object at 0x7f0f8579ecb0>, <mixtensor.MixTensor object at 0x7f0f8579ee30>))
2023-10-30 12:29:52,134 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:52,134 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,136 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ed70>, (<mixtensor.MixTensor object at 0x7f0f8579ee60>, <mixtensor.MixTensor object at 0x7f0f8579ee00>))
2023-10-30 12:29:52,136 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:52,136 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,137 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579eec0>, (<mixtensor.MixTensor object at 0x7f0f8579ee90>, <mixtensor.MixTensor object at 0x7f0f8579efe0>))
2023-10-30 12:29:52,137 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:52,138 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,139 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f070>, (<mixtensor.MixTensor object at 0x7f0f8579efb0>, <mixtensor.MixTensor object at 0x7f0f8579f0d0>))
2023-10-30 12:29:52,139 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,139 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:52,140 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,144 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,147 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,148 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,148 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:52,148 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,151 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f2b0>, (<mixtensor.MixTensor object at 0x7f0f8579f250>, <mixtensor.MixTensor object at 0x7f0f8579f3a0>))
2023-10-30 12:29:52,151 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:52,152 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,153 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f490>, (<mixtensor.MixTensor object at 0x7f0f8579f3d0>, <mixtensor.MixTensor object at 0x7f0f8579f430>))
2023-10-30 12:29:52,153 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:52,154 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,155 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f5b0>, (<mixtensor.MixTensor object at 0x7f0f8579f580>, <mixtensor.MixTensor object at 0x7f0f8579f700>))
2023-10-30 12:29:52,155 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:52,155 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,157 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f460>, (<mixtensor.MixTensor object at 0x7f0f8579f550>, <mixtensor.MixTensor object at 0x7f0f8579f6a0>))
2023-10-30 12:29:52,157 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,157 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:52,158 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,162 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,166 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,166 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,166 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:52,166 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,169 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f940>, (<mixtensor.MixTensor object at 0x7f0f8579f970>, <mixtensor.MixTensor object at 0x7f0f8579f880>))
2023-10-30 12:29:52,169 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:52,169 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,171 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fb80>, (<mixtensor.MixTensor object at 0x7f0f8579f9d0>, <mixtensor.MixTensor object at 0x7f0f8579fb20>))
2023-10-30 12:29:52,171 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:52,171 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,173 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8588eb00>, (<mixtensor.MixTensor object at 0x7f0f8579f2b0>, <mixtensor.MixTensor object at 0x7f0f8579fa00>))
2023-10-30 12:29:52,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:52,174 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f880>, (<mixtensor.MixTensor object at 0x7f0f8579fa60>, <mixtensor.MixTensor object at 0x7f0f8579fac0>))
2023-10-30 12:29:52,175 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,175 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:52,176 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,183 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,183 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,184 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:52,184 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,187 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fdf0>, (<mixtensor.MixTensor object at 0x7f0f8579fe50>, <mixtensor.MixTensor object at 0x7f0f8579fee0>))
2023-10-30 12:29:52,187 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:52,187 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,189 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ffd0>, (<mixtensor.MixTensor object at 0x7f0f8579ffa0>, <mixtensor.MixTensor object at 0x7f0f8579ff10>))
2023-10-30 12:29:52,189 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:52,189 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,190 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d480>, (<mixtensor.MixTensor object at 0x7f0f8579dd50>, <mixtensor.MixTensor object at 0x7f0f8579e1d0>))
2023-10-30 12:29:52,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:52,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,192 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d000>, (<mixtensor.MixTensor object at 0x7f0f8579d5a0>, <mixtensor.MixTensor object at 0x7f0f8579d4b0>))
2023-10-30 12:29:52,192 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,192 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:52,193 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,198 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,202 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,202 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,202 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:52,203 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,206 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d750>, (<mixtensor.MixTensor object at 0x7f0f8579ca00>, <mixtensor.MixTensor object at 0x7f0f8579c2e0>))
2023-10-30 12:29:52,206 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:52,206 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,207 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579dd80>, (<mixtensor.MixTensor object at 0x7f0f8579f1f0>, <mixtensor.MixTensor object at 0x7f0f8579fca0>))
2023-10-30 12:29:52,208 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:52,208 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,209 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fd90>, (<mixtensor.MixTensor object at 0x7f0f8579ead0>, <mixtensor.MixTensor object at 0x7f0f8579e230>))
2023-10-30 12:29:52,209 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:52,209 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,211 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f910>, (<mixtensor.MixTensor object at 0x7f0f8579ef80>, <mixtensor.MixTensor object at 0x7f0f8579ed40>))
2023-10-30 12:29:52,211 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,211 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:52,212 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,216 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,219 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,220 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,220 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:52,220 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,224 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f7f0>, (<mixtensor.MixTensor object at 0x7f0f8579dff0>, <mixtensor.MixTensor object at 0x7f0f8579de70>))
2023-10-30 12:29:52,224 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:52,224 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,226 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ddb0>, (<mixtensor.MixTensor object at 0x7f0f8579d4e0>, <mixtensor.MixTensor object at 0x7f0f8579d570>))
2023-10-30 12:29:52,226 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:52,226 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f610>, (<mixtensor.MixTensor object at 0x7f0f8579cc10>, <mixtensor.MixTensor object at 0x7f0f857d0040>))
2023-10-30 12:29:52,228 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:52,228 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,229 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ff40>, (<mixtensor.MixTensor object at 0x7f0f857d0070>, <mixtensor.MixTensor object at 0x7f0f857d00a0>))
2023-10-30 12:29:52,229 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,230 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:52,230 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,235 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,238 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,238 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,238 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:52,238 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,241 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0340>, (<mixtensor.MixTensor object at 0x7f0f857d0310>, <mixtensor.MixTensor object at 0x7f0f857d0490>))
2023-10-30 12:29:52,241 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:52,241 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,243 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d03d0>, (<mixtensor.MixTensor object at 0x7f0f857d04c0>, <mixtensor.MixTensor object at 0x7f0f857d0460>))
2023-10-30 12:29:52,243 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:52,243 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,245 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0520>, (<mixtensor.MixTensor object at 0x7f0f857d04f0>, <mixtensor.MixTensor object at 0x7f0f857d0640>))
2023-10-30 12:29:52,245 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:52,245 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,246 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0670>, (<mixtensor.MixTensor object at 0x7f0f857d06d0>, <mixtensor.MixTensor object at 0x7f0f857d0700>))
2023-10-30 12:29:52,247 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,247 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:52,247 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,251 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,255 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,255 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,255 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:52,255 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,259 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d08e0>, (<mixtensor.MixTensor object at 0x7f0f857d0880>, <mixtensor.MixTensor object at 0x7f0f857d09d0>))
2023-10-30 12:29:52,259 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:52,259 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,261 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0ac0>, (<mixtensor.MixTensor object at 0x7f0f857d0a90>, <mixtensor.MixTensor object at 0x7f0f857d0bb0>))
2023-10-30 12:29:52,261 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:52,261 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,262 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0c40>, (<mixtensor.MixTensor object at 0x7f0f857d0dc0>, <mixtensor.MixTensor object at 0x7f0f857d0c10>))
2023-10-30 12:29:52,263 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:52,263 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,265 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0c70>, (<mixtensor.MixTensor object at 0x7f0f857d0d60>, <mixtensor.MixTensor object at 0x7f0f857d0ca0>))
2023-10-30 12:29:52,265 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,265 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:52,265 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,270 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,273 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,273 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,273 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:52,274 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,276 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0f10>, (<mixtensor.MixTensor object at 0x7f0f857d1000>, <mixtensor.MixTensor object at 0x7f0f857d0fa0>))
2023-10-30 12:29:52,277 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:52,277 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,278 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d11e0>, (<mixtensor.MixTensor object at 0x7f0f857d1030>, <mixtensor.MixTensor object at 0x7f0f857d1180>))
2023-10-30 12:29:52,279 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:52,279 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,280 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d11b0>, (<mixtensor.MixTensor object at 0x7f0f857d1150>, <mixtensor.MixTensor object at 0x7f0f857d12a0>))
2023-10-30 12:29:52,280 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:52,280 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,282 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1210>, (<mixtensor.MixTensor object at 0x7f0f857d13c0>, <mixtensor.MixTensor object at 0x7f0f857d1270>))
2023-10-30 12:29:52,282 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,282 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:52,283 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,283 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,287 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,287 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,287 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:52,288 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,290 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576f190>, (<mixtensor.MixTensor object at 0x7f0f857d0c40>, <mixtensor.MixTensor object at 0x7f0f857d1000>))
2023-10-30 12:29:52,290 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:52,291 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,292 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d14b0>, (<mixtensor.MixTensor object at 0x7f0f857d1600>, <mixtensor.MixTensor object at 0x7f0f857d1540>))
2023-10-30 12:29:52,292 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:52,293 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,295 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d15d0>, (<mixtensor.MixTensor object at 0x7f0f857d1720>, <mixtensor.MixTensor object at 0x7f0f857d18a0>))
2023-10-30 12:29:52,295 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:52,295 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,296 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1630>, (<mixtensor.MixTensor object at 0x7f0f857d16f0>, <mixtensor.MixTensor object at 0x7f0f857d16c0>))
2023-10-30 12:29:52,297 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 12:29:52,297 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:52,297 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,301 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,301 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,301 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:52,302 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,302 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1000>
2023-10-30 12:29:52,302 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:52,302 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,303 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1a50>
2023-10-30 12:29:52,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:52,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,303 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1bd0>
2023-10-30 12:29:52,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:52,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,304 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1d20>
2023-10-30 12:29:52,304 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,304 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:52,304 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,305 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,305 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,305 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,305 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:52,305 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,312 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579db70>
2023-10-30 12:29:52,312 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:52,312 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,318 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d11b0>
2023-10-30 12:29:52,318 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:52,318 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,324 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1c60>
2023-10-30 12:29:52,324 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:52,325 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,330 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1cf0>
2023-10-30 12:29:52,331 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:52,333 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:52,333 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,333 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,334 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:52,334 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,334 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:52,334 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,335 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576dc60>
2023-10-30 12:29:52,335 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:52,335 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,335 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576cca0>
2023-10-30 12:29:52,335 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:52,335 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,336 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576d690>
2023-10-30 12:29:52,336 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:52,336 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,336 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576d330>
2023-10-30 12:29:52,336 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,337 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:52,337 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,341 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,341 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-30 12:29:52,341 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,342 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:52,342 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs_k: {}
2023-10-30 12:29:52,342 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576e5c0>
2023-10-30 12:29:52,342 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:52,342 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs_k: {}
2023-10-30 12:29:52,343 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576cbb0>
2023-10-30 12:29:52,343 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:52,343 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs_k: {}
2023-10-30 12:29:52,343 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576c790>
2023-10-30 12:29:52,343 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:52,343 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs_k: {}
2023-10-30 12:29:52,344 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576e740>
2023-10-30 12:29:52,344 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,344 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:52,345 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,352 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,352 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,352 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:52,352 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,355 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c1f0>, (<mixtensor.MixTensor object at 0x7f0f8576c220>, <mixtensor.MixTensor object at 0x7f0f8576dba0>))
2023-10-30 12:29:52,355 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:52,356 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,357 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576feb0>, (<mixtensor.MixTensor object at 0x7f0f8576d0f0>, <mixtensor.MixTensor object at 0x7f0f8576caf0>))
2023-10-30 12:29:52,357 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:52,358 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,359 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576fe80>, (<mixtensor.MixTensor object at 0x7f0f8576fac0>, <mixtensor.MixTensor object at 0x7f0f8576e710>))
2023-10-30 12:29:52,359 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:52,359 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,361 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ffa0>, (<mixtensor.MixTensor object at 0x7f0f8576e4d0>, <mixtensor.MixTensor object at 0x7f0f8576cd60>))
2023-10-30 12:29:52,361 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,362 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:52,362 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,366 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,369 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,370 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,370 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:52,370 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,372 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c190>, (<mixtensor.MixTensor object at 0x7f0f8579c370>, <mixtensor.MixTensor object at 0x7f0f8579c430>))
2023-10-30 12:29:52,373 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:52,373 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,374 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c400>, (<mixtensor.MixTensor object at 0x7f0f8579c340>, <mixtensor.MixTensor object at 0x7f0f8579cd00>))
2023-10-30 12:29:52,375 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:52,375 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,376 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c9d0>, (<mixtensor.MixTensor object at 0x7f0f8579cca0>, <mixtensor.MixTensor object at 0x7f0f8579cbe0>))
2023-10-30 12:29:52,376 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:52,377 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,378 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c940>, (<mixtensor.MixTensor object at 0x7f0f8579c8b0>, <mixtensor.MixTensor object at 0x7f0f8579ca90>))
2023-10-30 12:29:52,378 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,378 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:52,379 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,383 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,386 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,387 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,387 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:52,387 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,390 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579cf70>, (<mixtensor.MixTensor object at 0x7f0f8579d840>, <mixtensor.MixTensor object at 0x7f0f8579d690>))
2023-10-30 12:29:52,390 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:52,390 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,392 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d7e0>, (<mixtensor.MixTensor object at 0x7f0f8579d720>, <mixtensor.MixTensor object at 0x7f0f8579d3f0>))
2023-10-30 12:29:52,392 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:52,392 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,394 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d9f0>, (<mixtensor.MixTensor object at 0x7f0f8579db10>, <mixtensor.MixTensor object at 0x7f0f8579dc60>))
2023-10-30 12:29:52,394 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:52,394 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,395 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579dd20>, (<mixtensor.MixTensor object at 0x7f0f8579de40>, <mixtensor.MixTensor object at 0x7f0f8579dcf0>))
2023-10-30 12:29:52,396 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,396 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:52,396 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,400 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,404 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,404 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,404 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:52,404 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,407 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1d20>, (<mixtensor.MixTensor object at 0x7f0f857d11b0>, <mixtensor.MixTensor object at 0x7f0f857d1c60>))
2023-10-30 12:29:52,407 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:52,408 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,409 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1de0>, (<mixtensor.MixTensor object at 0x7f0f857d1db0>, <mixtensor.MixTensor object at 0x7f0f857d1f30>))
2023-10-30 12:29:52,409 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:52,409 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,411 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1e70>, (<mixtensor.MixTensor object at 0x7f0f857d1f60>, <mixtensor.MixTensor object at 0x7f0f857d1f00>))
2023-10-30 12:29:52,411 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:52,411 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,413 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1e10>, (<mixtensor.MixTensor object at 0x7f0f857d1ea0>, <mixtensor.MixTensor object at 0x7f0f857d1ff0>))
2023-10-30 12:29:52,413 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,413 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:52,413 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,418 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,421 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,421 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,421 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:52,421 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,424 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d23b0>, (<mixtensor.MixTensor object at 0x7f0f857d2200>, <mixtensor.MixTensor object at 0x7f0f857d2350>))
2023-10-30 12:29:52,424 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:52,424 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,426 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2380>, (<mixtensor.MixTensor object at 0x7f0f857d2320>, <mixtensor.MixTensor object at 0x7f0f857d2470>))
2023-10-30 12:29:52,426 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:52,426 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,429 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1d20>, (<mixtensor.MixTensor object at 0x7f0f857d23e0>, <mixtensor.MixTensor object at 0x7f0f857d2440>))
2023-10-30 12:29:52,429 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:52,429 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,431 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2350>, (<mixtensor.MixTensor object at 0x7f0f857d2410>, <mixtensor.MixTensor object at 0x7f0f857d2560>))
2023-10-30 12:29:52,431 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,431 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:52,432 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,436 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,439 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,440 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,440 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:52,440 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,448 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d10f0>, (<mixtensor.MixTensor object at 0x7f0f857d1870>, <mixtensor.MixTensor object at 0x7f0f857d1840>))
2023-10-30 12:29:52,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:52,449 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,450 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0a60>, (<mixtensor.MixTensor object at 0x7f0f857d03a0>, <mixtensor.MixTensor object at 0x7f0f857d0a30>))
2023-10-30 12:29:52,450 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:52,451 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,452 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d05e0>, (<mixtensor.MixTensor object at 0x7f0f857d0cd0>, <mixtensor.MixTensor object at 0x7f0f857d06a0>))
2023-10-30 12:29:52,452 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:52,452 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,454 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0820>, (<mixtensor.MixTensor object at 0x7f0f857d1e40>, <mixtensor.MixTensor object at 0x7f0f857d1c30>))
2023-10-30 12:29:52,454 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,454 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:52,455 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,459 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,462 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,462 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,463 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:52,463 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,466 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2710>, (<mixtensor.MixTensor object at 0x7f0f857d26b0>, <mixtensor.MixTensor object at 0x7f0f857d2830>))
2023-10-30 12:29:52,466 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:52,466 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,468 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2950>, (<mixtensor.MixTensor object at 0x7f0f857d29e0>, <mixtensor.MixTensor object at 0x7f0f857d29b0>))
2023-10-30 12:29:52,469 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:52,469 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,470 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2b00>, (<mixtensor.MixTensor object at 0x7f0f857d2a10>, <mixtensor.MixTensor object at 0x7f0f857d2b90>))
2023-10-30 12:29:52,471 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:52,471 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,472 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2ad0>, (<mixtensor.MixTensor object at 0x7f0f857d2aa0>, <mixtensor.MixTensor object at 0x7f0f857d2b60>))
2023-10-30 12:29:52,472 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,473 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:52,473 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,477 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,480 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,481 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,481 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:52,481 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,484 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2e30>, (<mixtensor.MixTensor object at 0x7f0f857d2e00>, <mixtensor.MixTensor object at 0x7f0f857d2f80>))
2023-10-30 12:29:52,484 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:52,484 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,486 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2ec0>, (<mixtensor.MixTensor object at 0x7f0f857d2fb0>, <mixtensor.MixTensor object at 0x7f0f857d2f50>))
2023-10-30 12:29:52,486 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:52,486 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,487 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3010>, (<mixtensor.MixTensor object at 0x7f0f857d2fe0>, <mixtensor.MixTensor object at 0x7f0f857d3130>))
2023-10-30 12:29:52,488 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:52,488 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,490 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3160>, (<mixtensor.MixTensor object at 0x7f0f857d31c0>, <mixtensor.MixTensor object at 0x7f0f857d31f0>))
2023-10-30 12:29:52,490 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,490 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:52,491 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,498 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,498 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,499 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:52,499 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,502 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d33d0>, (<mixtensor.MixTensor object at 0x7f0f857d3370>, <mixtensor.MixTensor object at 0x7f0f857d34c0>))
2023-10-30 12:29:52,502 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:52,502 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,503 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d35b0>, (<mixtensor.MixTensor object at 0x7f0f857d3580>, <mixtensor.MixTensor object at 0x7f0f857d36a0>))
2023-10-30 12:29:52,504 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:52,504 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,596 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3730>, (<mixtensor.MixTensor object at 0x7f0f857d38b0>, <mixtensor.MixTensor object at 0x7f0f857d3700>))
2023-10-30 12:29:52,596 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:52,596 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,598 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3760>, (<mixtensor.MixTensor object at 0x7f0f857d3850>, <mixtensor.MixTensor object at 0x7f0f857d3790>))
2023-10-30 12:29:52,598 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,598 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:52,599 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,603 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,607 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,607 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,607 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:52,607 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,610 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3a00>, (<mixtensor.MixTensor object at 0x7f0f857d3af0>, <mixtensor.MixTensor object at 0x7f0f857d3a90>))
2023-10-30 12:29:52,610 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:52,610 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,612 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3cd0>, (<mixtensor.MixTensor object at 0x7f0f857d3b20>, <mixtensor.MixTensor object at 0x7f0f857d3c70>))
2023-10-30 12:29:52,612 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:52,612 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,614 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3ca0>, (<mixtensor.MixTensor object at 0x7f0f857d3c40>, <mixtensor.MixTensor object at 0x7f0f857d3d90>))
2023-10-30 12:29:52,614 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:52,614 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,617 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3d00>, (<mixtensor.MixTensor object at 0x7f0f857d3eb0>, <mixtensor.MixTensor object at 0x7f0f857d3d60>))
2023-10-30 12:29:52,618 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,618 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:52,618 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,623 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,626 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,627 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,627 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:52,627 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,630 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d24a0>, (<mixtensor.MixTensor object at 0x7f0f857d1750>, <mixtensor.MixTensor object at 0x7f0f857d2020>))
2023-10-30 12:29:52,630 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:52,630 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,632 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3070>, (<mixtensor.MixTensor object at 0x7f0f857d34f0>, <mixtensor.MixTensor object at 0x7f0f857d3dc0>))
2023-10-30 12:29:52,632 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:52,632 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,633 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3fd0>, (<mixtensor.MixTensor object at 0x7f0f857d3e20>, <mixtensor.MixTensor object at 0x7f0f857d3be0>))
2023-10-30 12:29:52,634 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:52,634 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,635 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d39d0>, (<mixtensor.MixTensor object at 0x7f0f857d80a0>, <mixtensor.MixTensor object at 0x7f0f857d8070>))
2023-10-30 12:29:52,635 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,636 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:52,636 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,637 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,640 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,640 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,641 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:52,641 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,648 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579c1f0>, (<mixtensor.MixTensor object at 0x7f0f8579dd80>, <mixtensor.MixTensor object at 0x7f0f8579fbb0>))
2023-10-30 12:29:52,648 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:52,648 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,650 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8220>, (<mixtensor.MixTensor object at 0x7f0f857d81f0>, <mixtensor.MixTensor object at 0x7f0f857d8340>))
2023-10-30 12:29:52,650 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:52,650 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,652 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8460>, (<mixtensor.MixTensor object at 0x7f0f857d84f0>, <mixtensor.MixTensor object at 0x7f0f857d84c0>))
2023-10-30 12:29:52,652 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:52,652 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,654 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8400>, (<mixtensor.MixTensor object at 0x7f0f857d8640>, <mixtensor.MixTensor object at 0x7f0f857d8490>))
2023-10-30 12:29:52,654 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 12:29:52,654 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:52,655 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,659 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,660 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,660 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,660 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:52,660 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,661 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d3c10>
2023-10-30 12:29:52,661 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:52,661 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,661 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8730>
2023-10-30 12:29:52,661 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:52,661 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,662 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d87c0>
2023-10-30 12:29:52,662 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:52,662 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,662 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d88e0>
2023-10-30 12:29:52,662 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,663 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:52,663 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,663 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,664 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,664 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,664 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:52,664 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,670 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576d690>
2023-10-30 12:29:52,670 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:52,670 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,676 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8400>
2023-10-30 12:29:52,677 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:52,677 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,683 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8940>
2023-10-30 12:29:52,683 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:52,683 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,689 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8a90>
2023-10-30 12:29:52,691 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:52,692 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:52,692 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,693 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,693 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:52,693 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,694 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:52,694 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,694 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d0850>
2023-10-30 12:29:52,694 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:52,694 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,694 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d0d60>
2023-10-30 12:29:52,695 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:52,695 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,695 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d0c10>
2023-10-30 12:29:52,695 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:52,695 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,695 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1480>
2023-10-30 12:29:52,696 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,696 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:52,696 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,700 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,701 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-30 12:29:52,701 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,701 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:52,701 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs_k: {}
2023-10-30 12:29:52,701 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d04c0>
2023-10-30 12:29:52,702 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:52,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs_k: {}
2023-10-30 12:29:52,702 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1270>
2023-10-30 12:29:52,702 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:52,702 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs_k: {}
2023-10-30 12:29:52,703 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d19c0>
2023-10-30 12:29:52,703 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:52,703 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs_k: {}
2023-10-30 12:29:52,703 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d08b0>
2023-10-30 12:29:52,703 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,704 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:52,704 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,708 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,711 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,711 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,712 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:52,712 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,715 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e8f0>, (<mixtensor.MixTensor object at 0x7f0f8579e980>, <mixtensor.MixTensor object at 0x7f0f8579f130>))
2023-10-30 12:29:52,715 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:52,715 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,717 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579efb0>, (<mixtensor.MixTensor object at 0x7f0f8579f1c0>, <mixtensor.MixTensor object at 0x7f0f8579f220>))
2023-10-30 12:29:52,717 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:52,717 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,718 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f7c0>, (<mixtensor.MixTensor object at 0x7f0f8579f8e0>, <mixtensor.MixTensor object at 0x7f0f8579f3d0>))
2023-10-30 12:29:52,718 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:52,719 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,720 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f0d0>, (<mixtensor.MixTensor object at 0x7f0f8579f580>, <mixtensor.MixTensor object at 0x7f0f8579f550>))
2023-10-30 12:29:52,720 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,720 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:52,721 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,725 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,728 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,729 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,729 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:52,729 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,732 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579ef20>, (<mixtensor.MixTensor object at 0x7f0f8579fca0>, <mixtensor.MixTensor object at 0x7f0f8579e230>))
2023-10-30 12:29:52,732 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:52,732 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,734 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d570>, (<mixtensor.MixTensor object at 0x7f0f8579c1f0>, <mixtensor.MixTensor object at 0x7f0f8584fbe0>))
2023-10-30 12:29:52,734 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:52,734 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,736 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e770>, (<mixtensor.MixTensor object at 0x7f0f8576e7d0>, <mixtensor.MixTensor object at 0x7f0f8576e650>))
2023-10-30 12:29:52,736 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:52,736 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,737 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e8c0>, (<mixtensor.MixTensor object at 0x7f0f8576e800>, <mixtensor.MixTensor object at 0x7f0f8576e680>))
2023-10-30 12:29:52,738 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,738 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:52,738 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,743 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,746 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,746 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,746 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:52,747 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,749 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576d960>, (<mixtensor.MixTensor object at 0x7f0f8576eb30>, <mixtensor.MixTensor object at 0x7f0f8576ed40>))
2023-10-30 12:29:52,749 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:52,750 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,751 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ecb0>, (<mixtensor.MixTensor object at 0x7f0f8576ec80>, <mixtensor.MixTensor object at 0x7f0f8576edd0>))
2023-10-30 12:29:52,751 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:52,752 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,753 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576e9b0>, (<mixtensor.MixTensor object at 0x7f0f8576c8e0>, <mixtensor.MixTensor object at 0x7f0f8576e020>))
2023-10-30 12:29:52,753 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:52,753 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,755 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c850>, (<mixtensor.MixTensor object at 0x7f0f8576cd00>, <mixtensor.MixTensor object at 0x7f0f8576d3f0>))
2023-10-30 12:29:52,755 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,755 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:52,756 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,760 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,763 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,763 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,764 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:52,764 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,766 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d88e0>, (<mixtensor.MixTensor object at 0x7f0f857d8400>, <mixtensor.MixTensor object at 0x7f0f857d8940>))
2023-10-30 12:29:52,767 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:52,767 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,768 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d89d0>, (<mixtensor.MixTensor object at 0x7f0f857d8b20>, <mixtensor.MixTensor object at 0x7f0f857d8c40>))
2023-10-30 12:29:52,769 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:52,769 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,770 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8ca0>, (<mixtensor.MixTensor object at 0x7f0f857d8cd0>, <mixtensor.MixTensor object at 0x7f0f857d8be0>))
2023-10-30 12:29:52,770 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:52,771 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,772 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8c10>, (<mixtensor.MixTensor object at 0x7f0f857d8d00>, <mixtensor.MixTensor object at 0x7f0f857d8d90>))
2023-10-30 12:29:52,772 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,772 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:52,773 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,777 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:52,780 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,780 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,781 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:52,781 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,783 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9120>, (<mixtensor.MixTensor object at 0x7f0f857d8f70>, <mixtensor.MixTensor object at 0x7f0f857d8f40>))
2023-10-30 12:29:52,784 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:52,784 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,785 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9150>, (<mixtensor.MixTensor object at 0x7f0f857d91b0>, <mixtensor.MixTensor object at 0x7f0f857d9240>))
2023-10-30 12:29:52,786 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:52,786 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,787 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d88e0>, (<mixtensor.MixTensor object at 0x7f0f857d9180>, <mixtensor.MixTensor object at 0x7f0f857d9210>))
2023-10-30 12:29:52,788 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:52,788 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,789 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8f40>, (<mixtensor.MixTensor object at 0x7f0f857d9390>, <mixtensor.MixTensor object at 0x7f0f857d91e0>))
2023-10-30 12:29:52,789 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,790 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:52,790 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,794 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:52,798 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,798 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,798 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:52,798 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,801 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9540>, (<mixtensor.MixTensor object at 0x7f0f857d9480>, <mixtensor.MixTensor object at 0x7f0f857d94e0>))
2023-10-30 12:29:52,801 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:52,802 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,803 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9660>, (<mixtensor.MixTensor object at 0x7f0f857d9630>, <mixtensor.MixTensor object at 0x7f0f857d97b0>))
2023-10-30 12:29:52,803 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:52,804 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,805 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d96f0>, (<mixtensor.MixTensor object at 0x7f0f857d97e0>, <mixtensor.MixTensor object at 0x7f0f857d9780>))
2023-10-30 12:29:52,805 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:52,805 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,807 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9690>, (<mixtensor.MixTensor object at 0x7f0f857d9720>, <mixtensor.MixTensor object at 0x7f0f857d9870>))
2023-10-30 12:29:52,807 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,807 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:52,807 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,812 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:52,815 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,815 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,816 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:52,816 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,818 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c30>, (<mixtensor.MixTensor object at 0x7f0f857d9a80>, <mixtensor.MixTensor object at 0x7f0f857d9bd0>))
2023-10-30 12:29:52,818 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:52,819 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,820 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c00>, (<mixtensor.MixTensor object at 0x7f0f857d9ba0>, <mixtensor.MixTensor object at 0x7f0f857d9cf0>))
2023-10-30 12:29:52,820 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:52,820 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,822 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9de0>, (<mixtensor.MixTensor object at 0x7f0f857d9db0>, <mixtensor.MixTensor object at 0x7f0f857d9ed0>))
2023-10-30 12:29:52,822 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:52,822 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,824 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c90>, (<mixtensor.MixTensor object at 0x7f0f857d9ea0>, <mixtensor.MixTensor object at 0x7f0f857d9f00>))
2023-10-30 12:29:52,824 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,824 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:52,824 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,829 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:52,832 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,832 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,832 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:52,833 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,836 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da1a0>, (<mixtensor.MixTensor object at 0x7f0f857da170>, <mixtensor.MixTensor object at 0x7f0f857da2f0>))
2023-10-30 12:29:52,836 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:52,836 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,838 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da230>, (<mixtensor.MixTensor object at 0x7f0f857da320>, <mixtensor.MixTensor object at 0x7f0f857da2c0>))
2023-10-30 12:29:52,838 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:52,838 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,839 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da500>, (<mixtensor.MixTensor object at 0x7f0f857da350>, <mixtensor.MixTensor object at 0x7f0f857da4a0>))
2023-10-30 12:29:52,839 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:52,840 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,841 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da380>, (<mixtensor.MixTensor object at 0x7f0f857da3e0>, <mixtensor.MixTensor object at 0x7f0f857da440>))
2023-10-30 12:29:52,841 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,841 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:52,842 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,846 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:52,849 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,850 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,850 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:52,850 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,853 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da680>, (<mixtensor.MixTensor object at 0x7f0f857da7a0>, <mixtensor.MixTensor object at 0x7f0f857da830>))
2023-10-30 12:29:52,853 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:52,853 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,855 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da920>, (<mixtensor.MixTensor object at 0x7f0f857da860>, <mixtensor.MixTensor object at 0x7f0f857da8c0>))
2023-10-30 12:29:52,855 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:52,855 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,857 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857daa40>, (<mixtensor.MixTensor object at 0x7f0f857dabc0>, <mixtensor.MixTensor object at 0x7f0f857daa10>))
2023-10-30 12:29:52,857 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:52,857 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,858 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da8f0>, (<mixtensor.MixTensor object at 0x7f0f857da9e0>, <mixtensor.MixTensor object at 0x7f0f857dab30>))
2023-10-30 12:29:52,858 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,859 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:52,859 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,863 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:52,867 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,867 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,867 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:52,867 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,870 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dad70>, (<mixtensor.MixTensor object at 0x7f0f857dae60>, <mixtensor.MixTensor object at 0x7f0f857dae00>))
2023-10-30 12:29:52,870 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:52,870 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,872 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db040>, (<mixtensor.MixTensor object at 0x7f0f857dae90>, <mixtensor.MixTensor object at 0x7f0f857dafe0>))
2023-10-30 12:29:52,872 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:52,872 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,874 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db010>, (<mixtensor.MixTensor object at 0x7f0f857dafb0>, <mixtensor.MixTensor object at 0x7f0f857db100>))
2023-10-30 12:29:52,874 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:52,874 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,875 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db070>, (<mixtensor.MixTensor object at 0x7f0f857db0d0>, <mixtensor.MixTensor object at 0x7f0f857db250>))
2023-10-30 12:29:52,875 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,876 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:52,876 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,881 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:52,884 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,884 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,884 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:52,885 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,887 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db2b0>, (<mixtensor.MixTensor object at 0x7f0f857db400>, <mixtensor.MixTensor object at 0x7f0f857db490>))
2023-10-30 12:29:52,887 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:52,888 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,889 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db580>, (<mixtensor.MixTensor object at 0x7f0f857db550>, <mixtensor.MixTensor object at 0x7f0f857db6d0>))
2023-10-30 12:29:52,889 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:52,889 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,891 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db610>, (<mixtensor.MixTensor object at 0x7f0f857db700>, <mixtensor.MixTensor object at 0x7f0f857db6a0>))
2023-10-30 12:29:52,891 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:52,891 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,893 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db5b0>, (<mixtensor.MixTensor object at 0x7f0f857db640>, <mixtensor.MixTensor object at 0x7f0f857db790>))
2023-10-30 12:29:52,893 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,893 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:52,893 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:52,897 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,898 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,898 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:52,898 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,901 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576c850>, (<mixtensor.MixTensor object at 0x7f0f857db010>, <mixtensor.MixTensor object at 0x7f0f857db400>))
2023-10-30 12:29:52,901 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:52,901 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,903 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dba00>, (<mixtensor.MixTensor object at 0x7f0f857dbb80>, <mixtensor.MixTensor object at 0x7f0f857db9d0>))
2023-10-30 12:29:52,903 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:52,903 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,905 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbbb0>, (<mixtensor.MixTensor object at 0x7f0f857dbb50>, <mixtensor.MixTensor object at 0x7f0f857dbaf0>))
2023-10-30 12:29:52,905 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:52,905 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,906 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbac0>, (<mixtensor.MixTensor object at 0x7f0f857dbc40>, <mixtensor.MixTensor object at 0x7f0f857dbc10>))
2023-10-30 12:29:52,907 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 12:29:52,907 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:52,907 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,911 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:52,911 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,911 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,912 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:52,912 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,912 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857db400>
2023-10-30 12:29:52,912 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:52,912 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,913 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857dbdf0>
2023-10-30 12:29:52,913 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:52,913 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,913 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857dbfd0>
2023-10-30 12:29:52,913 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:52,913 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,914 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8b50>
2023-10-30 12:29:52,914 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,914 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:52,914 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,915 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:52,915 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,915 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,915 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:52,915 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,921 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588de70>
2023-10-30 12:29:52,922 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:52,922 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,927 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857db610>
2023-10-30 12:29:52,928 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:52,928 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,933 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857da5f0>
2023-10-30 12:29:52,934 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:52,934 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:52,941 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d90c0>
2023-10-30 12:29:52,942 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:52,943 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:52,943 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,944 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:52,944 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:52,944 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,945 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:52,945 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,945 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579d720>
2023-10-30 12:29:52,945 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:52,945 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,946 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576d0f0>
2023-10-30 12:29:52,946 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:52,946 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,946 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576e710>
2023-10-30 12:29:52,946 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:52,946 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:52,947 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576c850>
2023-10-30 12:29:52,947 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,947 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:52,947 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,952 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:52,952 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-30 12:29:52,952 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:52,952 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:52,953 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs_k: {}
2023-10-30 12:29:52,953 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579cdf0>
2023-10-30 12:29:52,953 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:52,953 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs_k: {}
2023-10-30 12:29:52,954 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d0c70>
2023-10-30 12:29:52,954 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:52,954 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs_k: {}
2023-10-30 12:29:52,954 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1990>
2023-10-30 12:29:52,954 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:52,954 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs_k: {}
2023-10-30 12:29:52,955 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f87434df0>
2023-10-30 12:29:52,955 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:52,955 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:52,955 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,960 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:52,963 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,963 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,963 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:52,963 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,966 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d17b0>, (<mixtensor.MixTensor object at 0x7f0f857d2320>, <mixtensor.MixTensor object at 0x7f0f857d23e0>))
2023-10-30 12:29:52,966 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:52,966 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,968 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2470>, (<mixtensor.MixTensor object at 0x7f0f857d2440>, <mixtensor.MixTensor object at 0x7f0f857d2560>))
2023-10-30 12:29:52,968 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:52,968 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,970 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0cd0>, (<mixtensor.MixTensor object at 0x7f0f857d1e40>, <mixtensor.MixTensor object at 0x7f0f857d22c0>))
2023-10-30 12:29:52,970 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:52,970 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,971 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2740>, (<mixtensor.MixTensor object at 0x7f0f857d0a30>, <mixtensor.MixTensor object at 0x7f0f857d06a0>))
2023-10-30 12:29:52,972 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:52,972 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:52,972 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,977 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:52,980 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,980 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,981 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:52,981 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,984 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d31c0>, (<mixtensor.MixTensor object at 0x7f0f857d32e0>, <mixtensor.MixTensor object at 0x7f0f857d3340>))
2023-10-30 12:29:52,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:52,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,986 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3940>, (<mixtensor.MixTensor object at 0x7f0f857d3ac0>, <mixtensor.MixTensor object at 0x7f0f857d3580>))
2023-10-30 12:29:52,986 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:52,986 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,987 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3a60>, (<mixtensor.MixTensor object at 0x7f0f857d36a0>, <mixtensor.MixTensor object at 0x7f0f857d3700>))
2023-10-30 12:29:52,987 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:52,988 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,989 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3910>, (<mixtensor.MixTensor object at 0x7f0f857d3790>, <mixtensor.MixTensor object at 0x7f0f857d3f70>))
2023-10-30 12:29:52,989 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:52,989 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:52,990 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:52,994 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:52,997 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:52,998 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:52,998 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:52,998 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,001 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3e20>, (<mixtensor.MixTensor object at 0x7f0f857d3dc0>, <mixtensor.MixTensor object at 0x7f0f857d3be0>))
2023-10-30 12:29:53,001 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:53,001 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,003 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8160>, (<mixtensor.MixTensor object at 0x7f0f857d8100>, <mixtensor.MixTensor object at 0x7f0f857d80a0>))
2023-10-30 12:29:53,003 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:53,003 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,004 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d85b0>, (<mixtensor.MixTensor object at 0x7f0f857d8700>, <mixtensor.MixTensor object at 0x7f0f857d81f0>))
2023-10-30 12:29:53,004 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:53,005 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,006 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8070>, (<mixtensor.MixTensor object at 0x7f0f857d84f0>, <mixtensor.MixTensor object at 0x7f0f857d8640>))
2023-10-30 12:29:53,006 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,006 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:53,007 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,011 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,015 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,015 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,015 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:53,015 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,018 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db610>, (<mixtensor.MixTensor object at 0x7f0f857da5f0>, <mixtensor.MixTensor object at 0x7f0f857d90c0>))
2023-10-30 12:29:53,018 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:53,018 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,152 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d90f0>, (<mixtensor.MixTensor object at 0x7f0f857d92a0>, <mixtensor.MixTensor object at 0x7f0f857d8df0>))
2023-10-30 12:29:53,152 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:53,152 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,154 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d83a0>, (<mixtensor.MixTensor object at 0x7f0f857d8370>, <mixtensor.MixTensor object at 0x7f0f857d8970>))
2023-10-30 12:29:53,154 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:53,154 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,158 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9000>, (<mixtensor.MixTensor object at 0x7f0f857d8b80>, <mixtensor.MixTensor object at 0x7f0f857db8b0>))
2023-10-30 12:29:53,158 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,159 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:53,159 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,167 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,168 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,168 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:53,168 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,171 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db160>, (<mixtensor.MixTensor object at 0x7f0f857d9d50>, <mixtensor.MixTensor object at 0x7f0f857da710>))
2023-10-30 12:29:53,171 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:53,171 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,173 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da200>, (<mixtensor.MixTensor object at 0x7f0f857d9ff0>, <mixtensor.MixTensor object at 0x7f0f857da620>))
2023-10-30 12:29:53,173 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:53,173 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c100>, (<mixtensor.MixTensor object at 0x7f0f8578c1c0>, <mixtensor.MixTensor object at 0x7f0f8578c070>))
2023-10-30 12:29:53,175 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:53,175 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,177 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c040>, (<mixtensor.MixTensor object at 0x7f0f8578c190>, <mixtensor.MixTensor object at 0x7f0f8578c1f0>))
2023-10-30 12:29:53,177 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,177 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:53,178 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,182 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,186 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,186 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,187 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:53,187 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,189 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c3a0>, (<mixtensor.MixTensor object at 0x7f0f8578c490>, <mixtensor.MixTensor object at 0x7f0f8578c430>))
2023-10-30 12:29:53,190 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:53,190 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,192 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c4f0>, (<mixtensor.MixTensor object at 0x7f0f8578c4c0>, <mixtensor.MixTensor object at 0x7f0f8578c610>))
2023-10-30 12:29:53,192 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:53,192 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,193 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c730>, (<mixtensor.MixTensor object at 0x7f0f8578c7c0>, <mixtensor.MixTensor object at 0x7f0f8578c790>))
2023-10-30 12:29:53,194 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:53,194 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,195 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578c6d0>, (<mixtensor.MixTensor object at 0x7f0f8578c910>, <mixtensor.MixTensor object at 0x7f0f8578c760>))
2023-10-30 12:29:53,195 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,196 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:53,196 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,201 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,204 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,204 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,205 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:53,205 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,207 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578cb20>, (<mixtensor.MixTensor object at 0x7f0f8578ca90>, <mixtensor.MixTensor object at 0x7f0f8578cac0>))
2023-10-30 12:29:53,208 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:53,208 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,209 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578cc40>, (<mixtensor.MixTensor object at 0x7f0f8578cc10>, <mixtensor.MixTensor object at 0x7f0f8578cd90>))
2023-10-30 12:29:53,210 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:53,210 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,212 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578ccd0>, (<mixtensor.MixTensor object at 0x7f0f8578cdc0>, <mixtensor.MixTensor object at 0x7f0f8576dab0>))
2023-10-30 12:29:53,212 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:53,212 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,213 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578ca90>, (<mixtensor.MixTensor object at 0x7f0f8578cac0>, <mixtensor.MixTensor object at 0x7f0f8578caf0>))
2023-10-30 12:29:53,214 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,214 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:53,214 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,219 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,222 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,222 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,222 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:53,223 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,226 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578cf40>, (<mixtensor.MixTensor object at 0x7f0f8578cfd0>, <mixtensor.MixTensor object at 0x7f0f8578d000>))
2023-10-30 12:29:53,226 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:53,226 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d090>, (<mixtensor.MixTensor object at 0x7f0f8578d210>, <mixtensor.MixTensor object at 0x7f0f8578d060>))
2023-10-30 12:29:53,228 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:53,228 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,229 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d240>, (<mixtensor.MixTensor object at 0x7f0f8578d1e0>, <mixtensor.MixTensor object at 0x7f0f8578d180>))
2023-10-30 12:29:53,229 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:53,230 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,231 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d150>, (<mixtensor.MixTensor object at 0x7f0f8578d2d0>, <mixtensor.MixTensor object at 0x7f0f8578d450>))
2023-10-30 12:29:53,231 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,231 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:53,232 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,236 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,240 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,240 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,240 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:53,240 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,244 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d510>, (<mixtensor.MixTensor object at 0x7f0f8578d660>, <mixtensor.MixTensor object at 0x7f0f8578d5a0>))
2023-10-30 12:29:53,244 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:53,244 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,246 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d630>, (<mixtensor.MixTensor object at 0x7f0f8578d780>, <mixtensor.MixTensor object at 0x7f0f8578d750>))
2023-10-30 12:29:53,246 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:53,246 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,247 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d7b0>, (<mixtensor.MixTensor object at 0x7f0f8578d810>, <mixtensor.MixTensor object at 0x7f0f8578d900>))
2023-10-30 12:29:53,247 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:53,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,249 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d870>, (<mixtensor.MixTensor object at 0x7f0f8578d8a0>, <mixtensor.MixTensor object at 0x7f0f8578d840>))
2023-10-30 12:29:53,249 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,249 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:53,250 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,254 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,258 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,258 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,258 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:53,259 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,261 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578dba0>, (<mixtensor.MixTensor object at 0x7f0f8578dcc0>, <mixtensor.MixTensor object at 0x7f0f8578db70>))
2023-10-30 12:29:53,261 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:53,262 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,263 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578dd50>, (<mixtensor.MixTensor object at 0x7f0f8578dc60>, <mixtensor.MixTensor object at 0x7f0f8578dd80>))
2023-10-30 12:29:53,263 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:53,263 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,265 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578ddb0>, (<mixtensor.MixTensor object at 0x7f0f8578df00>, <mixtensor.MixTensor object at 0x7f0f8578de40>))
2023-10-30 12:29:53,266 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:53,266 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,267 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578df60>, (<mixtensor.MixTensor object at 0x7f0f8578dea0>, <mixtensor.MixTensor object at 0x7f0f8578df90>))
2023-10-30 12:29:53,267 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,268 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:53,268 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,272 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,276 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,276 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,277 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:53,277 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,280 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e230>, (<mixtensor.MixTensor object at 0x7f0f8578e2c0>, <mixtensor.MixTensor object at 0x7f0f8578e290>))
2023-10-30 12:29:53,280 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:53,280 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,281 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e2f0>, (<mixtensor.MixTensor object at 0x7f0f8578e350>, <mixtensor.MixTensor object at 0x7f0f8578e440>))
2023-10-30 12:29:53,282 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:53,282 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,283 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e4a0>, (<mixtensor.MixTensor object at 0x7f0f8578e620>, <mixtensor.MixTensor object at 0x7f0f8578e470>))
2023-10-30 12:29:53,283 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:53,284 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,285 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e4d0>, (<mixtensor.MixTensor object at 0x7f0f8578e5c0>, <mixtensor.MixTensor object at 0x7f0f8578e500>))
2023-10-30 12:29:53,285 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,285 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:53,286 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,287 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,290 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,290 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,291 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:53,291 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,293 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e770>, (<mixtensor.MixTensor object at 0x7f0f8578e860>, <mixtensor.MixTensor object at 0x7f0f8578e800>))
2023-10-30 12:29:53,294 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:53,294 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,295 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e8c0>, (<mixtensor.MixTensor object at 0x7f0f8578e890>, <mixtensor.MixTensor object at 0x7f0f8578e9e0>))
2023-10-30 12:29:53,295 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:53,296 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,297 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578ead0>, (<mixtensor.MixTensor object at 0x7f0f8578eb60>, <mixtensor.MixTensor object at 0x7f0f8578eb30>))
2023-10-30 12:29:53,297 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:53,297 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,299 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e9b0>, (<mixtensor.MixTensor object at 0x7f0f8578ecb0>, <mixtensor.MixTensor object at 0x7f0f8578eb00>))
2023-10-30 12:29:53,299 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 12:29:53,299 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:53,300 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,303 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,304 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,304 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,304 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:53,304 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,305 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e800>
2023-10-30 12:29:53,305 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:53,305 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,305 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578eef0>
2023-10-30 12:29:53,305 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:53,305 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,306 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e950>
2023-10-30 12:29:53,306 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:53,306 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,306 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e170>
2023-10-30 12:29:53,306 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,307 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:53,307 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,307 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,308 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,308 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,308 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:53,308 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,314 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857da200>
2023-10-30 12:29:53,314 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:53,315 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,320 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e4d0>
2023-10-30 12:29:53,320 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:53,321 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,327 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e320>
2023-10-30 12:29:53,327 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:53,327 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,333 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578de70>
2023-10-30 12:29:53,334 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:53,336 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:53,336 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,337 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,337 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:53,337 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,337 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:53,337 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,338 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576c8e0>
2023-10-30 12:29:53,338 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:53,338 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,338 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe2eff880>
2023-10-30 12:29:53,338 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:53,339 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,339 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8f10>
2023-10-30 12:29:53,339 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:53,339 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,339 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8cd0>
2023-10-30 12:29:53,340 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,340 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:53,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,345 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-30 12:29:53,345 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,345 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:53,345 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs_k: {}
2023-10-30 12:29:53,346 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576d690>
2023-10-30 12:29:53,346 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:53,346 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs_k: {}
2023-10-30 12:29:53,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8e50>
2023-10-30 12:29:53,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:53,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs_k: {}
2023-10-30 12:29:53,347 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8d00>
2023-10-30 12:29:53,347 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:53,347 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs_k: {}
2023-10-30 12:29:53,348 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1000>
2023-10-30 12:29:53,348 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,348 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:53,349 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,353 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,356 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,356 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,356 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:53,356 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,359 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d97b0>, (<mixtensor.MixTensor object at 0x7f0f857d9780>, <mixtensor.MixTensor object at 0x7f0f857d9870>))
2023-10-30 12:29:53,359 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:53,360 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,361 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9db0>, (<mixtensor.MixTensor object at 0x7f0f857d9ea0>, <mixtensor.MixTensor object at 0x7f0f857da110>))
2023-10-30 12:29:53,362 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:53,362 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,363 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9f00>, (<mixtensor.MixTensor object at 0x7f0f857da590>, <mixtensor.MixTensor object at 0x7f0f857da560>))
2023-10-30 12:29:53,363 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:53,364 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,365 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9ed0>, (<mixtensor.MixTensor object at 0x7f0f857da320>, <mixtensor.MixTensor object at 0x7f0f857da350>))
2023-10-30 12:29:53,365 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,365 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:53,366 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,371 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,374 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,374 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,375 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:53,375 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,377 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db940>, (<mixtensor.MixTensor object at 0x7f0f857db550>, <mixtensor.MixTensor object at 0x7f0f857db700>))
2023-10-30 12:29:53,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:53,378 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,379 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db6d0>, (<mixtensor.MixTensor object at 0x7f0f857db6a0>, <mixtensor.MixTensor object at 0x7f0f857db790>))
2023-10-30 12:29:53,379 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:53,380 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,381 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbb50>, (<mixtensor.MixTensor object at 0x7f0f857dbc40>, <mixtensor.MixTensor object at 0x7f0f857dbd00>))
2023-10-30 12:29:53,381 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:53,381 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,383 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbb80>, (<mixtensor.MixTensor object at 0x7f0f857dbe50>, <mixtensor.MixTensor object at 0x7f0f857db9d0>))
2023-10-30 12:29:53,383 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,383 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:53,384 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,388 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,392 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,392 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,393 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:53,393 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,395 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f6a0>, (<mixtensor.MixTensor object at 0x7f0f8579f700>, <mixtensor.MixTensor object at 0x7f0f8579f430>))
2023-10-30 12:29:53,396 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:53,396 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,398 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579e020>, (<mixtensor.MixTensor object at 0x7f0f8579e3e0>, <mixtensor.MixTensor object at 0x7f0f8579e590>))
2023-10-30 12:29:53,399 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:53,399 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,431 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fbe0>, (<mixtensor.MixTensor object at 0x7f0f8579f1c0>, <mixtensor.MixTensor object at 0x7f0f8579f8e0>))
2023-10-30 12:29:53,432 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:53,432 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,434 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fc10>, (<mixtensor.MixTensor object at 0x7f0f8579f580>, <mixtensor.MixTensor object at 0x7f0f8579f9d0>))
2023-10-30 12:29:53,434 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,435 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:53,435 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,440 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,443 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,444 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,444 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:53,444 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,447 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e170>, (<mixtensor.MixTensor object at 0x7f0f8578e4d0>, <mixtensor.MixTensor object at 0x7f0f8578e320>))
2023-10-30 12:29:53,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:53,448 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,449 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d9f0>, (<mixtensor.MixTensor object at 0x7f0f8578d3f0>, <mixtensor.MixTensor object at 0x7f0f8578dcf0>))
2023-10-30 12:29:53,450 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:53,450 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,453 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d5d0>, (<mixtensor.MixTensor object at 0x7f0f8578cee0>, <mixtensor.MixTensor object at 0x7f0f8578c670>))
2023-10-30 12:29:53,453 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:53,453 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,455 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d360>, (<mixtensor.MixTensor object at 0x7f0f8578d120>, <mixtensor.MixTensor object at 0x7f0f8578cca0>))
2023-10-30 12:29:53,455 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,456 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:53,456 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,461 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,464 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,464 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,465 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:53,465 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,468 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578edd0>, (<mixtensor.MixTensor object at 0x7f0f8578ee00>, <mixtensor.MixTensor object at 0x7f0f8578f010>))
2023-10-30 12:29:53,468 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:53,468 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,470 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f220>, (<mixtensor.MixTensor object at 0x7f0f8578f070>, <mixtensor.MixTensor object at 0x7f0f8578f040>))
2023-10-30 12:29:53,470 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:53,470 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,472 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f250>, (<mixtensor.MixTensor object at 0x7f0f8578f2b0>, <mixtensor.MixTensor object at 0x7f0f8578f340>))
2023-10-30 12:29:53,472 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:53,472 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,474 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f280>, (<mixtensor.MixTensor object at 0x7f0f8578f310>, <mixtensor.MixTensor object at 0x7f0f8578f490>))
2023-10-30 12:29:53,474 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,474 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:53,475 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,479 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,482 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,482 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,483 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:53,483 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,486 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f4f0>, (<mixtensor.MixTensor object at 0x7f0f8578f640>, <mixtensor.MixTensor object at 0x7f0f8578f580>))
2023-10-30 12:29:53,486 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:53,486 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,489 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f610>, (<mixtensor.MixTensor object at 0x7f0f8578f760>, <mixtensor.MixTensor object at 0x7f0f8578f730>))
2023-10-30 12:29:53,489 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:53,489 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,491 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f790>, (<mixtensor.MixTensor object at 0x7f0f8578f7f0>, <mixtensor.MixTensor object at 0x7f0f8578f8e0>))
2023-10-30 12:29:53,491 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:53,491 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,492 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f850>, (<mixtensor.MixTensor object at 0x7f0f8578f820>, <mixtensor.MixTensor object at 0x7f0f8578f880>))
2023-10-30 12:29:53,493 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,493 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:53,493 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,498 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,501 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,501 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,501 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:53,502 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,505 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d810>, (<mixtensor.MixTensor object at 0x7f0f84d4f5e0>, <mixtensor.MixTensor object at 0x7f0f84d4dff0>))
2023-10-30 12:29:53,505 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:53,505 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,507 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e680>, (<mixtensor.MixTensor object at 0x7f0f84d4dfc0>, <mixtensor.MixTensor object at 0x7f0f84d4fa90>))
2023-10-30 12:29:53,507 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:53,507 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,509 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c5b0>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f857d9030>))
2023-10-30 12:29:53,509 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:53,510 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,511 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d01c0>, (<mixtensor.MixTensor object at 0x7f0f8576dea0>, <mixtensor.MixTensor object at 0x7f0f8576c880>))
2023-10-30 12:29:53,512 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,512 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:53,512 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,516 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,520 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,520 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,520 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:53,521 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,524 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c2b0>, (<mixtensor.MixTensor object at 0x7f0f84d4de10>, <mixtensor.MixTensor object at 0x7f0f84d4d540>))
2023-10-30 12:29:53,524 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:53,524 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,526 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f190>, (<mixtensor.MixTensor object at 0x7f0f84d4df60>, <mixtensor.MixTensor object at 0x7f0f84d4ead0>))
2023-10-30 12:29:53,526 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:53,526 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,528 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f130>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d4fb80>))
2023-10-30 12:29:53,528 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:53,528 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,530 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f340>, (<mixtensor.MixTensor object at 0x7f0f84d4ec80>, <mixtensor.MixTensor object at 0x7f0f84d4ffa0>))
2023-10-30 12:29:53,530 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,530 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:53,530 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,535 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,538 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,538 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,539 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:53,539 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,542 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9d0>, (<mixtensor.MixTensor object at 0x7f0f84d4c5e0>, <mixtensor.MixTensor object at 0x7f0f84d4d330>))
2023-10-30 12:29:53,542 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:53,542 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,544 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce20>, (<mixtensor.MixTensor object at 0x7f0f84d4d780>, <mixtensor.MixTensor object at 0x7f0f84d4f8b0>))
2023-10-30 12:29:53,544 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:53,544 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,546 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e7a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, <mixtensor.MixTensor object at 0x7f0f84d4d870>))
2023-10-30 12:29:53,546 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:53,546 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,548 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e200>, (<mixtensor.MixTensor object at 0x7f0f84d4f910>, <mixtensor.MixTensor object at 0x7f0f84d4c820>))
2023-10-30 12:29:53,548 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,548 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:53,548 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,553 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,556 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,557 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,557 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:53,557 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,560 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d570>, (<mixtensor.MixTensor object at 0x7f0f84d4fe50>, <mixtensor.MixTensor object at 0x7f0f84d4f310>))
2023-10-30 12:29:53,560 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:53,560 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,562 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cc40>, (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, <mixtensor.MixTensor object at 0x7f0f84d4efb0>))
2023-10-30 12:29:53,562 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:53,562 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,564 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d930>, (<mixtensor.MixTensor object at 0x7f0f84d4eb90>, <mixtensor.MixTensor object at 0x7f0f84d4f160>))
2023-10-30 12:29:53,564 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:53,564 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,566 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, (<mixtensor.MixTensor object at 0x7f0f84d4cd30>, <mixtensor.MixTensor object at 0x7f0f84d4fe20>))
2023-10-30 12:29:53,566 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,566 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:53,566 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,571 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,574 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,574 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,575 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:53,575 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,578 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fb50>, (<mixtensor.MixTensor object at 0x7f0f84d4d030>, <mixtensor.MixTensor object at 0x7f0f84d4d420>))
2023-10-30 12:29:53,578 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:53,578 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,580 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e050>, (<mixtensor.MixTensor object at 0x7f0f84d4feb0>, <mixtensor.MixTensor object at 0x7f0f84d4fb20>))
2023-10-30 12:29:53,580 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:53,580 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,581 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fcd0>, (<mixtensor.MixTensor object at 0x7f0f84d4faf0>, <mixtensor.MixTensor object at 0x7f0f84d4fe80>))
2023-10-30 12:29:53,582 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:53,582 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,583 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c6a0>, (<mixtensor.MixTensor object at 0x7f0f84d4dab0>, <mixtensor.MixTensor object at 0x7f0f84d4c370>))
2023-10-30 12:29:53,583 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,584 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:53,584 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,585 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,588 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,588 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,588 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:53,589 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,592 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c670>, (<mixtensor.MixTensor object at 0x7f0f84d4c1f0>, <mixtensor.MixTensor object at 0x7f0f84d4fbe0>))
2023-10-30 12:29:53,592 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:53,592 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,594 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4efe0>, (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, <mixtensor.MixTensor object at 0x7f0f84d4ffd0>))
2023-10-30 12:29:53,595 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:53,595 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,596 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e0e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, <mixtensor.MixTensor object at 0x7f0f84d4db70>))
2023-10-30 12:29:53,597 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:53,597 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,598 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d300>, (<mixtensor.MixTensor object at 0x7f0f84d4da80>, <mixtensor.MixTensor object at 0x7f0f84d4fac0>))
2023-10-30 12:29:53,598 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 12:29:53,599 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:53,599 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,602 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,603 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,603 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,604 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:53,604 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,604 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fbe0>
2023-10-30 12:29:53,604 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:53,604 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,604 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d3c0>
2023-10-30 12:29:53,605 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:53,605 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,605 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fa30>
2023-10-30 12:29:53,605 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:53,605 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,605 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f550>
2023-10-30 12:29:53,606 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,606 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:53,606 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,606 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,607 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,607 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,607 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:53,607 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,614 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576dba0>
2023-10-30 12:29:53,614 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:53,614 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,620 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c6a0>
2023-10-30 12:29:53,620 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:53,620 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,626 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c310>
2023-10-30 12:29:53,626 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:53,626 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,632 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4dae0>
2023-10-30 12:29:53,633 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:53,635 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:53,635 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,636 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,636 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:53,636 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,636 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:53,637 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,637 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578ca60>
2023-10-30 12:29:53,637 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:53,637 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,637 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578cdc0>
2023-10-30 12:29:53,637 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:53,638 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,638 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578cd90>
2023-10-30 12:29:53,638 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:53,638 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,638 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d480>
2023-10-30 12:29:53,639 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,639 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:53,639 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,643 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,644 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-30 12:29:53,644 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:53,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs_k: {}
2023-10-30 12:29:53,644 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578c9d0>
2023-10-30 12:29:53,645 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:53,645 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs_k: {}
2023-10-30 12:29:53,645 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d750>
2023-10-30 12:29:53,645 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:53,645 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs_k: {}
2023-10-30 12:29:53,646 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d4e0>
2023-10-30 12:29:53,646 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:53,646 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs_k: {}
2023-10-30 12:29:53,646 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578ca00>
2023-10-30 12:29:53,646 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,647 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:53,647 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,651 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,654 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,654 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,655 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:53,655 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,658 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578ed40>, (<mixtensor.MixTensor object at 0x7f0f8578ed10>, <mixtensor.MixTensor object at 0x7f0f8578e9e0>))
2023-10-30 12:29:53,658 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:53,658 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,659 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d16c0>, (<mixtensor.MixTensor object at 0x7f0f857d18a0>, <mixtensor.MixTensor object at 0x7f0f857d1540>))
2023-10-30 12:29:53,660 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:53,660 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,661 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d17b0>, (<mixtensor.MixTensor object at 0x7f0f857d20b0>, <mixtensor.MixTensor object at 0x7f0f857d12a0>))
2023-10-30 12:29:53,661 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:53,662 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,663 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1b10>, (<mixtensor.MixTensor object at 0x7f0f857d1180>, <mixtensor.MixTensor object at 0x7f0f857d1660>))
2023-10-30 12:29:53,664 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,664 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:53,664 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,668 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,672 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,672 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,672 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:53,672 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,675 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576dab0>, (<mixtensor.MixTensor object at 0x7f0f8576dba0>, <mixtensor.MixTensor object at 0x7f0f857daa10>))
2023-10-30 12:29:53,676 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:53,676 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,677 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da9e0>, (<mixtensor.MixTensor object at 0x7f0f857dabc0>, <mixtensor.MixTensor object at 0x7f0f857da4a0>))
2023-10-30 12:29:53,678 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:53,678 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,679 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da6b0>, (<mixtensor.MixTensor object at 0x7f0f857d9870>, <mixtensor.MixTensor object at 0x7f0f857d91b0>))
2023-10-30 12:29:53,679 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:53,680 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,681 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da740>, (<mixtensor.MixTensor object at 0x7f0f857d9390>, <mixtensor.MixTensor object at 0x7f0f857db220>))
2023-10-30 12:29:53,681 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,681 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:53,682 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,686 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,690 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,690 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,690 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:53,690 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,694 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db070>, (<mixtensor.MixTensor object at 0x7f0f857db400>, <mixtensor.MixTensor object at 0x7f0f857d8100>))
2023-10-30 12:29:53,694 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:53,694 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,696 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbfd0>, (<mixtensor.MixTensor object at 0x7f0f857d80a0>, <mixtensor.MixTensor object at 0x7f0f857d81f0>))
2023-10-30 12:29:53,696 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:53,696 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,698 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d92a0>, (<mixtensor.MixTensor object at 0x7f0f857d8370>, <mixtensor.MixTensor object at 0x7f0f857d8b80>))
2023-10-30 12:29:53,698 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:53,698 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,700 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db3a0>, (<mixtensor.MixTensor object at 0x7f0f857d9e10>, <mixtensor.MixTensor object at 0x7f0f857da890>))
2023-10-30 12:29:53,700 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,700 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:53,700 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,704 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,708 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,708 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,708 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:53,708 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,711 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f550>, (<mixtensor.MixTensor object at 0x7f0f84d4c6a0>, <mixtensor.MixTensor object at 0x7f0f84d4c310>))
2023-10-30 12:29:53,711 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:53,711 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,713 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd90>, (<mixtensor.MixTensor object at 0x7f0f84d4f010>, <mixtensor.MixTensor object at 0x7f0f84d4e140>))
2023-10-30 12:29:53,713 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:53,713 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,715 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f880>, (<mixtensor.MixTensor object at 0x7f0f84d4f280>, <mixtensor.MixTensor object at 0x7f0f84d4f430>))
2023-10-30 12:29:53,715 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:53,715 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,717 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, <mixtensor.MixTensor object at 0x7f0f84d4cee0>))
2023-10-30 12:29:53,717 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,717 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:53,717 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,722 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,725 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,725 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,726 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:53,726 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,729 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c430>, (<mixtensor.MixTensor object at 0x7f0f84d4c3d0>, <mixtensor.MixTensor object at 0x7f0f84d4c340>))
2023-10-30 12:29:53,729 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:53,729 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,731 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c490>, (<mixtensor.MixTensor object at 0x7f0f84d4c4c0>, <mixtensor.MixTensor object at 0x7f0f84d4c580>))
2023-10-30 12:29:53,731 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:53,731 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,733 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c250>, (<mixtensor.MixTensor object at 0x7f0f84d4c190>, <mixtensor.MixTensor object at 0x7f0f84d4c8b0>))
2023-10-30 12:29:53,733 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:53,733 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,734 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c3a0>, (<mixtensor.MixTensor object at 0x7f0f84d4ca90>, <mixtensor.MixTensor object at 0x7f0f84d4ca60>))
2023-10-30 12:29:53,735 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,735 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:53,735 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,739 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,743 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,743 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,743 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:53,743 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,746 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cfd0>, (<mixtensor.MixTensor object at 0x7f0f84d4ca30>, <mixtensor.MixTensor object at 0x7f0f84d4d0f0>))
2023-10-30 12:29:53,746 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:53,746 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,748 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cdc0>, (<mixtensor.MixTensor object at 0x7f0f84d4d1b0>, <mixtensor.MixTensor object at 0x7f0f84d4cc70>))
2023-10-30 12:29:53,748 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:53,748 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,750 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d4e0>, (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, <mixtensor.MixTensor object at 0x7f0f84d4d600>))
2023-10-30 12:29:53,750 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:53,750 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,752 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d480>, (<mixtensor.MixTensor object at 0x7f0f84d4d510>, <mixtensor.MixTensor object at 0x7f0f84d4cc10>))
2023-10-30 12:29:53,752 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,752 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:53,753 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,757 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,760 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,760 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,761 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:53,761 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,764 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4da20>, (<mixtensor.MixTensor object at 0x7f0f84d4d120>, <mixtensor.MixTensor object at 0x7f0f84d4dc30>))
2023-10-30 12:29:53,764 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:53,764 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,766 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d660>, (<mixtensor.MixTensor object at 0x7f0f84d4df00>, <mixtensor.MixTensor object at 0x7f0f84d4c160>))
2023-10-30 12:29:53,766 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:53,766 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,768 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d6f0>, (<mixtensor.MixTensor object at 0x7f0f84d4dc00>, <mixtensor.MixTensor object at 0x7f0fe297a020>))
2023-10-30 12:29:53,768 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:53,768 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,770 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f850>, (<mixtensor.MixTensor object at 0x7f0f8576c310>, <mixtensor.MixTensor object at 0x7f0f8576de40>))
2023-10-30 12:29:53,770 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,770 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:53,771 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,775 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:53,778 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,779 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,779 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:53,779 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,782 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df30>, (<mixtensor.MixTensor object at 0x7f0f84d4d840>, <mixtensor.MixTensor object at 0x7f0f84d4e020>))
2023-10-30 12:29:53,782 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:53,782 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,784 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4db40>, (<mixtensor.MixTensor object at 0x7f0f84d4e2f0>, <mixtensor.MixTensor object at 0x7f0f84d4e2c0>))
2023-10-30 12:29:53,784 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:53,784 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,786 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e350>, (<mixtensor.MixTensor object at 0x7f0f84d4dba0>, <mixtensor.MixTensor object at 0x7f0f84d4e560>))
2023-10-30 12:29:53,786 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:53,786 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,787 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e440>, (<mixtensor.MixTensor object at 0x7f0f84d4e110>, <mixtensor.MixTensor object at 0x7f0f84d4e500>))
2023-10-30 12:29:53,788 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,788 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:53,788 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,792 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:53,796 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,796 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,796 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:53,796 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,799 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e620>, (<mixtensor.MixTensor object at 0x7f0f84d4ea10>, <mixtensor.MixTensor object at 0x7f0f84d4e4d0>))
2023-10-30 12:29:53,799 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:53,799 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,801 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ed40>, (<mixtensor.MixTensor object at 0x7f0f84d4e6b0>, <mixtensor.MixTensor object at 0x7f0f84d4ee60>))
2023-10-30 12:29:53,801 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:53,801 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,803 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb30>, (<mixtensor.MixTensor object at 0x7f0f84d4ef20>, <mixtensor.MixTensor object at 0x7f0f84d4e9e0>))
2023-10-30 12:29:53,803 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:53,803 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,805 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ef80>, (<mixtensor.MixTensor object at 0x7f0f84d4e9b0>, <mixtensor.MixTensor object at 0x7f0f84d4f220>))
2023-10-30 12:29:53,805 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,805 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:53,805 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,810 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:53,813 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,814 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,814 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:53,814 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,817 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2e0>, (<mixtensor.MixTensor object at 0x7f0f84d4f040>, <mixtensor.MixTensor object at 0x7f0f84d4f7c0>))
2023-10-30 12:29:53,817 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:53,817 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,819 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f10ad78fd60>, (<mixtensor.MixTensor object at 0x7f0f8572d600>, <mixtensor.MixTensor object at 0x7f0f8572d630>))
2023-10-30 12:29:53,819 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:53,819 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,820 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f610>, (<mixtensor.MixTensor object at 0x7f0f8572c670>, <mixtensor.MixTensor object at 0x7f0f8572c940>))
2023-10-30 12:29:53,821 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:53,821 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,822 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fcd0>, (<mixtensor.MixTensor object at 0x7f0f8572dd80>, <mixtensor.MixTensor object at 0x7f0f8572d7b0>))
2023-10-30 12:29:53,822 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,823 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:53,823 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,827 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:53,831 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,831 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,831 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:53,831 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,834 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d390>, (<mixtensor.MixTensor object at 0x7f0f8572cc70>, <mixtensor.MixTensor object at 0x7f0f8572d5d0>))
2023-10-30 12:29:53,834 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:53,835 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,836 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca30>, (<mixtensor.MixTensor object at 0x7f0f8572cd30>, <mixtensor.MixTensor object at 0x7f0f8572c5b0>))
2023-10-30 12:29:53,836 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:53,837 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,838 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c2e0>, (<mixtensor.MixTensor object at 0x7f0f8572dc60>, <mixtensor.MixTensor object at 0x7f0f8572dcc0>))
2023-10-30 12:29:53,838 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:53,838 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,840 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d330>, (<mixtensor.MixTensor object at 0x7f0f8572dba0>, <mixtensor.MixTensor object at 0x7f0f8572c9a0>))
2023-10-30 12:29:53,840 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,840 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:53,841 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,841 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:53,845 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,845 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,845 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:53,845 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,848 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cb80>, (<mixtensor.MixTensor object at 0x7f0f8572d870>, <mixtensor.MixTensor object at 0x7f0f8572dd20>))
2023-10-30 12:29:53,848 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:53,849 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,850 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572caf0>, (<mixtensor.MixTensor object at 0x7f0f8572cfa0>, <mixtensor.MixTensor object at 0x7f0f8572c430>))
2023-10-30 12:29:53,850 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:53,850 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,852 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c6a0>, (<mixtensor.MixTensor object at 0x7f0f8572d570>, <mixtensor.MixTensor object at 0x7f0f8572dab0>))
2023-10-30 12:29:53,852 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:53,852 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,854 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cee0>, (<mixtensor.MixTensor object at 0x7f0f8572cf10>, <mixtensor.MixTensor object at 0x7f0f8572c1c0>))
2023-10-30 12:29:53,854 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 12:29:53,854 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:53,855 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,858 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:53,859 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,859 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,859 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:53,859 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,859 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572dd20>
2023-10-30 12:29:53,860 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:53,860 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,860 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c310>
2023-10-30 12:29:53,860 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:53,860 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,860 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fd60>
2023-10-30 12:29:53,861 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:53,861 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,861 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c250>
2023-10-30 12:29:53,861 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,861 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:53,862 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,862 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:53,862 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,863 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,863 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:53,863 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,872 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578cd90>
2023-10-30 12:29:53,872 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:53,872 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,878 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d330>
2023-10-30 12:29:53,878 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:53,878 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,884 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c6d0>
2023-10-30 12:29:53,884 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:53,885 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:53,891 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572faf0>
2023-10-30 12:29:53,892 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:53,894 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:53,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,895 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:53,896 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:53,896 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,897 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:53,897 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,898 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d9ea0>
2023-10-30 12:29:53,898 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:53,898 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,898 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857db6a0>
2023-10-30 12:29:53,899 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:53,899 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,899 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857dbd00>
2023-10-30 12:29:53,899 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:53,899 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:53,900 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d9030>
2023-10-30 12:29:53,900 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,900 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:53,900 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,905 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:53,905 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-30 12:29:53,905 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:53,905 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:53,906 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs_k: {}
2023-10-30 12:29:53,906 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576dea0>
2023-10-30 12:29:53,906 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:53,906 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs_k: {}
2023-10-30 12:29:53,907 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e770>
2023-10-30 12:29:53,907 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:53,907 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs_k: {}
2023-10-30 12:29:53,907 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f2b0>
2023-10-30 12:29:53,907 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:53,908 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs_k: {}
2023-10-30 12:29:53,908 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579e590>
2023-10-30 12:29:53,908 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:53,909 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:53,909 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,913 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:53,916 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,916 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,916 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:53,917 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,919 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f400>, (<mixtensor.MixTensor object at 0x7f0f84d4fb20>, <mixtensor.MixTensor object at 0x7f0f84d4fe80>))
2023-10-30 12:29:53,919 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:53,920 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,921 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cb80>, (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, <mixtensor.MixTensor object at 0x7f0f84d4da80>))
2023-10-30 12:29:53,921 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:53,922 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,923 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4db70>, (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, <mixtensor.MixTensor object at 0x7f0f8588ead0>))
2023-10-30 12:29:53,923 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:53,923 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,925 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ffd0>, (<mixtensor.MixTensor object at 0x7f0f857d3ac0>, <mixtensor.MixTensor object at 0x7f0f857d1000>))
2023-10-30 12:29:53,925 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:53,925 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:53,926 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,930 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:53,933 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,934 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,934 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:53,934 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,937 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e620>, (<mixtensor.MixTensor object at 0x7f0f8578db10>, <mixtensor.MixTensor object at 0x7f0f8578d8a0>))
2023-10-30 12:29:53,937 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:53,937 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,939 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d9c0>, (<mixtensor.MixTensor object at 0x7f0f8578d450>, <mixtensor.MixTensor object at 0x7f0f8578d180>))
2023-10-30 12:29:53,939 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:53,939 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,941 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d1e0>, (<mixtensor.MixTensor object at 0x7f0f8578d2d0>, <mixtensor.MixTensor object at 0x7f0f8578d210>))
2023-10-30 12:29:53,941 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:53,941 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,942 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d570>, (<mixtensor.MixTensor object at 0x7f0f8578d900>, <mixtensor.MixTensor object at 0x7f0f8578c070>))
2023-10-30 12:29:53,942 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:53,943 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:53,943 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,948 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:53,951 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,951 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,951 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:53,952 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,954 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e740>, (<mixtensor.MixTensor object at 0x7f0f8578dcf0>, <mixtensor.MixTensor object at 0x7f0f8578c670>))
2023-10-30 12:29:53,954 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:53,954 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,956 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f070>, (<mixtensor.MixTensor object at 0x7f0f8578f2b0>, <mixtensor.MixTensor object at 0x7f0f8578f310>))
2023-10-30 12:29:53,956 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:53,956 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,958 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f340>, (<mixtensor.MixTensor object at 0x7f0f8578f490>, <mixtensor.MixTensor object at 0x7f0f8578f760>))
2023-10-30 12:29:53,958 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:53,958 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,960 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f040>, (<mixtensor.MixTensor object at 0x7f0f8578f7f0>, <mixtensor.MixTensor object at 0x7f0f8578f820>))
2023-10-30 12:29:53,960 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:53,960 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:53,960 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,965 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:53,968 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,969 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,969 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:53,969 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,972 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c250>, (<mixtensor.MixTensor object at 0x7f0f8572d330>, <mixtensor.MixTensor object at 0x7f0f8572c6d0>))
2023-10-30 12:29:53,972 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:53,972 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,974 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fc40>, (<mixtensor.MixTensor object at 0x7f0f8572d210>, <mixtensor.MixTensor object at 0x7f0f8572f7c0>))
2023-10-30 12:29:53,974 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:53,974 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,975 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dbd0>, (<mixtensor.MixTensor object at 0x7f0f8572d690>, <mixtensor.MixTensor object at 0x7f0f8572ceb0>))
2023-10-30 12:29:53,976 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:53,976 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,977 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca00>, (<mixtensor.MixTensor object at 0x7f0f8572d060>, <mixtensor.MixTensor object at 0x7f0f8572ca60>))
2023-10-30 12:29:53,977 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:53,977 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:53,978 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:53,982 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:53,986 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:53,986 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,986 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:53,986 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,989 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c160>, (<mixtensor.MixTensor object at 0x7f0f8572cf40>, <mixtensor.MixTensor object at 0x7f0f8572cbb0>))
2023-10-30 12:29:53,989 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:53,989 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,991 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f760>, (<mixtensor.MixTensor object at 0x7f0f8572f790>, <mixtensor.MixTensor object at 0x7f0f8572c2b0>))
2023-10-30 12:29:53,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:53,991 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,993 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f520>, (<mixtensor.MixTensor object at 0x7f0f8572f940>, <mixtensor.MixTensor object at 0x7f0f8572fbb0>))
2023-10-30 12:29:53,993 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:53,993 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:53,994 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f640>, (<mixtensor.MixTensor object at 0x7f0f8572f580>, <mixtensor.MixTensor object at 0x7f0f8572fca0>))
2023-10-30 12:29:53,995 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:53,995 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:53,995 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:53,999 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:54,003 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,003 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,003 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:54,004 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,007 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fe50>, (<mixtensor.MixTensor object at 0x7f0f8572c550>, <mixtensor.MixTensor object at 0x7f0f8572c4c0>))
2023-10-30 12:29:54,007 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:54,007 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,009 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fdf0>, (<mixtensor.MixTensor object at 0x7f0f8572fb50>, <mixtensor.MixTensor object at 0x7f0f8572fa60>))
2023-10-30 12:29:54,009 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:54,009 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,011 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ce20>, (<mixtensor.MixTensor object at 0x7f0f8572cdc0>, <mixtensor.MixTensor object at 0x7f0f8572d120>))
2023-10-30 12:29:54,011 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:54,011 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,012 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f490>, (<mixtensor.MixTensor object at 0x7f0f8572d0c0>, <mixtensor.MixTensor object at 0x7f0f8572d420>))
2023-10-30 12:29:54,013 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,013 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:54,013 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,018 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:54,022 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,022 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,023 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:54,023 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,025 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fac0>, (<mixtensor.MixTensor object at 0x7f0f8572d150>, <mixtensor.MixTensor object at 0x7f0f8572d480>))
2023-10-30 12:29:54,025 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:54,026 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,027 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572da20>, (<mixtensor.MixTensor object at 0x7f0f8572cb50>, <mixtensor.MixTensor object at 0x7f0f8572cd60>))
2023-10-30 12:29:54,027 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:54,027 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,030 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d8d0>, (<mixtensor.MixTensor object at 0x7f0f8572cb20>, <mixtensor.MixTensor object at 0x7f0f8579fd00>))
2023-10-30 12:29:54,030 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:54,030 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,159 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d990>, (<mixtensor.MixTensor object at 0x7f0f8578cc40>, <mixtensor.MixTensor object at 0x7f0f8578d090>))
2023-10-30 12:29:54,160 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,160 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:54,160 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,165 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,168 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,168 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,168 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:54,169 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,171 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c220>, (<mixtensor.MixTensor object at 0x7f0f8572c880>, <mixtensor.MixTensor object at 0x7f0f8572d510>))
2023-10-30 12:29:54,172 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:54,172 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,174 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c1f0>, (<mixtensor.MixTensor object at 0x7f0f8572d8a0>, <mixtensor.MixTensor object at 0x7f0f8572c040>))
2023-10-30 12:29:54,174 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:54,174 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,175 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa30>, (<mixtensor.MixTensor object at 0x7f0f8572c340>, <mixtensor.MixTensor object at 0x7f0f8572d270>))
2023-10-30 12:29:54,176 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:54,176 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,177 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd00>, (<mixtensor.MixTensor object at 0x7f0f8572fee0>, <mixtensor.MixTensor object at 0x7f0f8572ff70>))
2023-10-30 12:29:54,177 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,178 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:54,178 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,182 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,186 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,186 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,186 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:54,186 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,189 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bb80>, (<mixtensor.MixTensor object at 0x7f0f8565b6d0>, <mixtensor.MixTensor object at 0x7f0f8565b100>))
2023-10-30 12:29:54,189 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:54,189 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,191 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ada0>, (<mixtensor.MixTensor object at 0x7f0f85658d00>, <mixtensor.MixTensor object at 0x7f0f8565b3a0>))
2023-10-30 12:29:54,191 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:54,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,193 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b9a0>, (<mixtensor.MixTensor object at 0x7f0f8565b610>, <mixtensor.MixTensor object at 0x7f0f8565b910>))
2023-10-30 12:29:54,193 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:54,193 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,195 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba00>, (<mixtensor.MixTensor object at 0x7f0f8565b820>, <mixtensor.MixTensor object at 0x7f0f8565af20>))
2023-10-30 12:29:54,195 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,195 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:54,195 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,200 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,203 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,203 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,203 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:54,204 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,206 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85659c00>, (<mixtensor.MixTensor object at 0x7f0f8565b4c0>, <mixtensor.MixTensor object at 0x7f0f8565b700>))
2023-10-30 12:29:54,206 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:54,206 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,208 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565bb20>, (<mixtensor.MixTensor object at 0x7f0f85659cf0>, <mixtensor.MixTensor object at 0x7f0f8565ba30>))
2023-10-30 12:29:54,208 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:54,209 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,210 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565baf0>, (<mixtensor.MixTensor object at 0x7f0f85659330>, <mixtensor.MixTensor object at 0x7f0f8565b7f0>))
2023-10-30 12:29:54,210 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:54,210 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,212 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b160>, (<mixtensor.MixTensor object at 0x7f0f857fb160>, <mixtensor.MixTensor object at 0x7f0f857fa3e0>))
2023-10-30 12:29:54,212 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,212 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:54,213 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,217 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,220 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,221 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,221 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:54,221 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,224 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f869ec640>, (<mixtensor.MixTensor object at 0x7f0f869ec5e0>, <mixtensor.MixTensor object at 0x7f0f869ec610>))
2023-10-30 12:29:54,224 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:54,224 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,226 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0fe2f58e50>, (<mixtensor.MixTensor object at 0x7f0fe2f58e80>, <mixtensor.MixTensor object at 0x7f0fe2f58ee0>))
2023-10-30 12:29:54,226 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:54,226 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,228 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d442b0>, (<mixtensor.MixTensor object at 0x7f0f84d442e0>, <mixtensor.MixTensor object at 0x7f0f84d45330>))
2023-10-30 12:29:54,228 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:54,228 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,229 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447f0>, (<mixtensor.MixTensor object at 0x7f0f84d44a00>, <mixtensor.MixTensor object at 0x7f0f84d45510>))
2023-10-30 12:29:54,230 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,230 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:54,230 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,231 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,234 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,234 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,235 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:54,235 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,237 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d448e0>, (<mixtensor.MixTensor object at 0x7f0f84d44100>, <mixtensor.MixTensor object at 0x7f0f84d445b0>))
2023-10-30 12:29:54,238 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:54,238 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,240 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44f40>, (<mixtensor.MixTensor object at 0x7f0f84d44280>, <mixtensor.MixTensor object at 0x7f0f84d44670>))
2023-10-30 12:29:54,240 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:54,240 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,241 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d440a0>, (<mixtensor.MixTensor object at 0x7f0f84d44700>, <mixtensor.MixTensor object at 0x7f0f84d44460>))
2023-10-30 12:29:54,242 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:54,242 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,243 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44e80>, (<mixtensor.MixTensor object at 0x7f0f84d44be0>, <mixtensor.MixTensor object at 0x7f0f84d454b0>))
2023-10-30 12:29:54,244 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 12:29:54,244 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:54,244 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,247 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,248 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,248 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,248 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:54,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,249 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d445b0>
2023-10-30 12:29:54,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:54,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,249 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44220>
2023-10-30 12:29:54,249 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:54,249 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,250 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d454e0>
2023-10-30 12:29:54,250 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:54,250 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,250 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d449a0>
2023-10-30 12:29:54,250 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,251 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:54,251 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,251 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,252 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,252 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,252 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:54,252 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,258 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe2f58e50>
2023-10-30 12:29:54,259 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:54,259 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,265 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44e80>
2023-10-30 12:29:54,265 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:54,265 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,271 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d44a90>
2023-10-30 12:29:54,271 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:54,271 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,277 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d45120>
2023-10-30 12:29:54,278 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:54,279 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:54,279 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,280 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,280 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:54,281 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,281 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:54,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,281 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d9870>
2023-10-30 12:29:54,281 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:54,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,282 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857dabc0>
2023-10-30 12:29:54,282 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:54,282 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,282 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d91b0>
2023-10-30 12:29:54,282 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:54,282 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,283 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857db220>
2023-10-30 12:29:54,283 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,283 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:54,283 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:54,287 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,288 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-30 12:29:54,288 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,288 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:54,288 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs_k: {}
2023-10-30 12:29:54,289 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1660>
2023-10-30 12:29:54,289 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:54,289 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs_k: {}
2023-10-30 12:29:54,289 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d900>
2023-10-30 12:29:54,290 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:54,290 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs_k: {}
2023-10-30 12:29:54,290 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d540>
2023-10-30 12:29:54,290 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:54,290 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs_k: {}
2023-10-30 12:29:54,291 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0fe297a020>
2023-10-30 12:29:54,291 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,291 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:54,292 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:54,296 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:54,299 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,299 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,299 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:54,299 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,302 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, (<mixtensor.MixTensor object at 0x7f0f84d4edd0>, <mixtensor.MixTensor object at 0x7f0f84d4c880>))
2023-10-30 12:29:54,303 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:54,303 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,305 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, (<mixtensor.MixTensor object at 0x7f0f84d4f970>, <mixtensor.MixTensor object at 0x7f0f84d4c730>))
2023-10-30 12:29:54,305 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:54,305 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,306 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f910>, (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, <mixtensor.MixTensor object at 0x7f0f84d4d780>))
2023-10-30 12:29:54,307 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:54,307 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,308 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c880>, (<mixtensor.MixTensor object at 0x7f0f84d4c820>, <mixtensor.MixTensor object at 0x7f0f84d4d870>))
2023-10-30 12:29:54,309 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,309 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:54,309 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:54,313 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:54,317 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,317 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,317 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:54,318 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,320 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f070>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4dfc0>))
2023-10-30 12:29:54,320 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:54,321 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,322 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f850>, (<mixtensor.MixTensor object at 0x7f0f84d4eb00>, <mixtensor.MixTensor object at 0x7f0f84d4e740>))
2023-10-30 12:29:54,322 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:54,323 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,324 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, (<mixtensor.MixTensor object at 0x7f0f84d4fb80>, <mixtensor.MixTensor object at 0x7f0f84d4ead0>))
2023-10-30 12:29:54,324 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:54,324 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,326 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, (<mixtensor.MixTensor object at 0x7f0f84d4e0b0>, <mixtensor.MixTensor object at 0x7f0f84d4f820>))
2023-10-30 12:29:54,326 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,326 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:54,327 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:54,331 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:54,334 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,335 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,335 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:54,335 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,338 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f070>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4fbe0>))
2023-10-30 12:29:54,338 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:54,338 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,340 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d960>, (<mixtensor.MixTensor object at 0x7f0f84d4e890>, <mixtensor.MixTensor object at 0x7f0f84d4c040>))
2023-10-30 12:29:54,340 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:54,340 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,341 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e140>, (<mixtensor.MixTensor object at 0x7f0f84d4f430>, <mixtensor.MixTensor object at 0x7f0f84d4cee0>))
2023-10-30 12:29:54,342 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:54,342 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,343 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb90>, (<mixtensor.MixTensor object at 0x7f0f84d4fc10>, <mixtensor.MixTensor object at 0x7f0f84d4d3c0>))
2023-10-30 12:29:54,343 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,344 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:54,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:54,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:54,351 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,352 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,352 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:54,352 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,355 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c4c0>, (<mixtensor.MixTensor object at 0x7f0f84d4c190>, <mixtensor.MixTensor object at 0x7f0f84d4ca90>))
2023-10-30 12:29:54,355 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:54,355 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,357 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d9c0>, (<mixtensor.MixTensor object at 0x7f0f84d4d990>, <mixtensor.MixTensor object at 0x7f0f84d4d1b0>))
2023-10-30 12:29:54,357 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:54,357 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,359 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, (<mixtensor.MixTensor object at 0x7f0f84d4d510>, <mixtensor.MixTensor object at 0x7f0f84d4d9f0>))
2023-10-30 12:29:54,359 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:54,359 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,360 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d090>, (<mixtensor.MixTensor object at 0x7f0f84d4cca0>, <mixtensor.MixTensor object at 0x7f0f84d4ca60>))
2023-10-30 12:29:54,361 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,361 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:54,361 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:54,366 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:54,369 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,369 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,370 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:54,370 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,373 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d600>, (<mixtensor.MixTensor object at 0x7f0f84d4cc10>, <mixtensor.MixTensor object at 0x7f0f84d4cfd0>))
2023-10-30 12:29:54,373 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:54,373 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,375 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4e4a0>, <mixtensor.MixTensor object at 0x7f0f84d4e950>))
2023-10-30 12:29:54,375 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:54,375 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,376 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e2f0>, (<mixtensor.MixTensor object at 0x7f0f84d4dba0>, <mixtensor.MixTensor object at 0x7f0f84d4e110>))
2023-10-30 12:29:54,377 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:54,377 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,378 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c190>, (<mixtensor.MixTensor object at 0x7f0f84d4cc70>, <mixtensor.MixTensor object at 0x7f0f84d4d1e0>))
2023-10-30 12:29:54,378 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,379 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:54,379 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:54,383 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:54,386 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,387 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,387 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:54,387 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,390 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cfd0>, (<mixtensor.MixTensor object at 0x7f0f84d4dd80>, <mixtensor.MixTensor object at 0x7f0f84d4e2c0>))
2023-10-30 12:29:54,390 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:54,390 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,392 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e500>, (<mixtensor.MixTensor object at 0x7f0f84d4e9b0>, <mixtensor.MixTensor object at 0x7f0f84d4c850>))
2023-10-30 12:29:54,392 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:54,392 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,394 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, (<mixtensor.MixTensor object at 0x7f0f84d4ee60>, <mixtensor.MixTensor object at 0x7f0f84d4e9e0>))
2023-10-30 12:29:54,394 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:54,394 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,396 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d600>, (<mixtensor.MixTensor object at 0x7f0f84d4cc10>, <mixtensor.MixTensor object at 0x7f0f84d4e560>))
2023-10-30 12:29:54,396 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,396 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:54,397 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,402 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:54,406 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,406 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,406 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:54,407 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,410 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, (<mixtensor.MixTensor object at 0x7f0fe2f58e50>, <mixtensor.MixTensor object at 0x7f0f8579d2a0>))
2023-10-30 12:29:54,410 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:54,410 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,412 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f4c0>, (<mixtensor.MixTensor object at 0x7f0f8579f070>, <mixtensor.MixTensor object at 0x7f0f8579e770>))
2023-10-30 12:29:54,412 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:54,412 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,413 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f8e0>, (<mixtensor.MixTensor object at 0x7f0f8579e230>, <mixtensor.MixTensor object at 0x7f0f8579f0d0>))
2023-10-30 12:29:54,414 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:54,414 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,415 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e2c0>, (<mixtensor.MixTensor object at 0x7f0f8579d2a0>, <mixtensor.MixTensor object at 0x7f0f8579f880>))
2023-10-30 12:29:54,416 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,416 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:54,416 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,421 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,424 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,425 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,425 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:54,425 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,440 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579d000>, (<mixtensor.MixTensor object at 0x7f0f8579f2b0>, <mixtensor.MixTensor object at 0x7f0f8579fb20>))
2023-10-30 12:29:54,440 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:54,441 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,448 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d448e0>, (<mixtensor.MixTensor object at 0x7f0f84d445b0>, <mixtensor.MixTensor object at 0x7f0f84d44a90>))
2023-10-30 12:29:54,448 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:54,449 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,471 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45120>, (<mixtensor.MixTensor object at 0x7f0f84d44100>, <mixtensor.MixTensor object at 0x7f0f84d44910>))
2023-10-30 12:29:54,471 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:54,471 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,473 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, (<mixtensor.MixTensor object at 0x7f0f84d4c880>, <mixtensor.MixTensor object at 0x7f0f84d44550>))
2023-10-30 12:29:54,473 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,474 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:54,474 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,482 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,487 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,487 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,488 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:54,488 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,492 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f4c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d446d0>))
2023-10-30 12:29:54,492 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:54,493 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,495 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44850>, (<mixtensor.MixTensor object at 0x7f0f84d45240>, <mixtensor.MixTensor object at 0x7f0f8588e710>))
2023-10-30 12:29:54,496 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:54,496 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,499 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1180>, (<mixtensor.MixTensor object at 0x7f0f857d18a0>, <mixtensor.MixTensor object at 0x7f0f857d0a30>))
2023-10-30 12:29:54,499 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:54,500 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,516 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d20b0>, (<mixtensor.MixTensor object at 0x7f0f857d3af0>, <mixtensor.MixTensor object at 0x7f0f857d3d00>))
2023-10-30 12:29:54,516 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,517 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:54,517 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,522 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,525 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,525 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,526 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:54,526 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,529 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f130>, (<mixtensor.MixTensor object at 0x7f0f84d4e680>, <mixtensor.MixTensor object at 0x7f0f84d4eb30>))
2023-10-30 12:29:54,529 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:54,530 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,531 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4f340>, <mixtensor.MixTensor object at 0x7f0f84d4f190>))
2023-10-30 12:29:54,532 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:54,532 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,534 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4db40>, <mixtensor.MixTensor object at 0x7f0f84d4e470>))
2023-10-30 12:29:54,534 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:54,535 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,538 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4eb30>, (<mixtensor.MixTensor object at 0x7f0f84d4e440>, <mixtensor.MixTensor object at 0x7f0f84d4ea10>))
2023-10-30 12:29:54,538 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,539 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:54,539 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,543 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,547 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,547 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,547 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:54,547 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,550 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f130>, (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, <mixtensor.MixTensor object at 0x7f0f84d4f910>))
2023-10-30 12:29:54,550 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:54,550 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,552 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f220>, (<mixtensor.MixTensor object at 0x7f0f84d4c190>, <mixtensor.MixTensor object at 0x7f0f84d4c4c0>))
2023-10-30 12:29:54,553 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:54,553 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,554 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d660>, (<mixtensor.MixTensor object at 0x7f0f84d4c970>, <mixtensor.MixTensor object at 0x7f0f84d4f8b0>))
2023-10-30 12:29:54,554 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:54,555 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,556 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f070>, (<mixtensor.MixTensor object at 0x7f0f84d4cfd0>, <mixtensor.MixTensor object at 0x7f0f84d4f850>))
2023-10-30 12:29:54,556 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,556 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:54,557 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,558 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,561 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,561 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,562 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:54,562 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,564 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d4f400>))
2023-10-30 12:29:54,565 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:54,565 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,567 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d447f0>, (<mixtensor.MixTensor object at 0x7f0f84d44e20>, <mixtensor.MixTensor object at 0x7f0f84d44070>))
2023-10-30 12:29:54,567 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:54,567 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,568 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d448e0>, (<mixtensor.MixTensor object at 0x7f0f84d45120>, <mixtensor.MixTensor object at 0x7f0f84d446d0>))
2023-10-30 12:29:54,569 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:54,569 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,570 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f040>, (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, <mixtensor.MixTensor object at 0x7f0f84d4e770>))
2023-10-30 12:29:54,571 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 12:29:54,571 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:54,571 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,575 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,576 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,576 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,576 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:54,576 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,577 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4da50>
2023-10-30 12:29:54,577 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:54,577 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,577 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ce20>
2023-10-30 12:29:54,577 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:54,577 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,578 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fdc0>
2023-10-30 12:29:54,578 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:54,578 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,578 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fc70>
2023-10-30 12:29:54,578 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,578 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:54,579 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,579 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,579 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,580 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,580 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:54,580 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,586 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d20b0>
2023-10-30 12:29:54,586 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:54,586 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,592 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d1540>
2023-10-30 12:29:54,593 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:54,593 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,599 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f130>
2023-10-30 12:29:54,600 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:54,600 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,606 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f070>
2023-10-30 12:29:54,607 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:54,608 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:54,609 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,609 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,610 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:54,610 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,610 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:54,610 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,611 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d210>
2023-10-30 12:29:54,611 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:54,611 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,611 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d3f0>
2023-10-30 12:29:54,611 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:54,611 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,612 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578f7f0>
2023-10-30 12:29:54,612 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:54,612 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,612 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578f310>
2023-10-30 12:29:54,612 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,613 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:54,613 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:54,617 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,617 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-30 12:29:54,618 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,618 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:54,618 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs_k: {}
2023-10-30 12:29:54,618 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d900>
2023-10-30 12:29:54,618 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:54,619 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs_k: {}
2023-10-30 12:29:54,619 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565b820>
2023-10-30 12:29:54,619 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:54,619 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs_k: {}
2023-10-30 12:29:54,619 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8565b610>
2023-10-30 12:29:54,619 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:54,619 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs_k: {}
2023-10-30 12:29:54,620 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578e0e0>
2023-10-30 12:29:54,620 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,621 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:54,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:54,625 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:54,628 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,629 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,629 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:54,629 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,632 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d600>, (<mixtensor.MixTensor object at 0x7f0f8572d660>, <mixtensor.MixTensor object at 0x7f0f8572dae0>))
2023-10-30 12:29:54,632 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:54,632 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,634 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d900>, (<mixtensor.MixTensor object at 0x7f0f8572dab0>, <mixtensor.MixTensor object at 0x7f0f8572d630>))
2023-10-30 12:29:54,634 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:54,634 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,636 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fcd0>, (<mixtensor.MixTensor object at 0x7f0f8572dd20>, <mixtensor.MixTensor object at 0x7f0f8572c310>))
2023-10-30 12:29:54,636 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:54,636 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,638 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c9d0>, (<mixtensor.MixTensor object at 0x7f0f8572ccd0>, <mixtensor.MixTensor object at 0x7f0f8572d360>))
2023-10-30 12:29:54,638 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,638 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:54,639 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:54,643 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:54,646 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,646 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,646 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:54,646 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,649 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fca0>, (<mixtensor.MixTensor object at 0x7f0f8572d3f0>, <mixtensor.MixTensor object at 0x7f0f8572ff40>))
2023-10-30 12:29:54,649 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:54,649 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,651 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d0c0>, (<mixtensor.MixTensor object at 0x7f0f8572c100>, <mixtensor.MixTensor object at 0x7f0f8572f9a0>))
2023-10-30 12:29:54,651 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:54,652 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,654 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa60>, (<mixtensor.MixTensor object at 0x7f0f8572d120>, <mixtensor.MixTensor object at 0x7f0f8572d420>))
2023-10-30 12:29:54,654 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:54,654 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,656 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca60>, (<mixtensor.MixTensor object at 0x7f0f8572fb50>, <mixtensor.MixTensor object at 0x7f0f8572cdc0>))
2023-10-30 12:29:54,656 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,656 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:54,733 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:54,739 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:54,744 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,744 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,744 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:54,745 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,748 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d540>, (<mixtensor.MixTensor object at 0x7f0f8579fd00>, <mixtensor.MixTensor object at 0x7f0f8579e590>))
2023-10-30 12:29:54,748 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:54,748 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,750 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3e50>, (<mixtensor.MixTensor object at 0x7f0f857d2740>, <mixtensor.MixTensor object at 0x7f0f857d2470>))
2023-10-30 12:29:54,750 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:54,751 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,753 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1e40>, (<mixtensor.MixTensor object at 0x7f0f857d29b0>, <mixtensor.MixTensor object at 0x7f0f857d2d40>))
2023-10-30 12:29:54,754 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:54,754 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,756 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579fd00>, (<mixtensor.MixTensor object at 0x7f0f8579e590>, <mixtensor.MixTensor object at 0x7f0f8579f9d0>))
2023-10-30 12:29:54,756 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,756 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:54,757 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:54,761 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:54,765 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,765 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,766 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:54,766 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,769 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2da0>, (<mixtensor.MixTensor object at 0x7f0f857d0670>, <mixtensor.MixTensor object at 0x7f0f857d12a0>))
2023-10-30 12:29:54,769 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:54,769 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,771 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da3e0>, (<mixtensor.MixTensor object at 0x7f0f857da5f0>, <mixtensor.MixTensor object at 0x7f0f857d83d0>))
2023-10-30 12:29:54,772 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:54,772 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,774 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbb50>, (<mixtensor.MixTensor object at 0x7f0f857da740>, <mixtensor.MixTensor object at 0x7f0f857da9e0>))
2023-10-30 12:29:54,774 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:54,774 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,776 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3ac0>, (<mixtensor.MixTensor object at 0x7f0f857d1540>, <mixtensor.MixTensor object at 0x7f0f857d20b0>))
2023-10-30 12:29:54,776 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,776 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:54,777 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:54,782 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:54,785 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,785 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,786 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:54,786 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,789 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8b80>, (<mixtensor.MixTensor object at 0x7f0f857d81f0>, <mixtensor.MixTensor object at 0x7f0f857d9e10>))
2023-10-30 12:29:54,789 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:54,790 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,792 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d80a0>, (<mixtensor.MixTensor object at 0x7f0f857d8eb0>, <mixtensor.MixTensor object at 0x7f0f857d9390>))
2023-10-30 12:29:54,792 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:54,792 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,794 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8e20>, (<mixtensor.MixTensor object at 0x7f0f857d8c40>, <mixtensor.MixTensor object at 0x7f0f857d8b20>))
2023-10-30 12:29:54,794 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:54,794 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,796 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db400>, (<mixtensor.MixTensor object at 0x7f0f857da890>, <mixtensor.MixTensor object at 0x7f0f857d8370>))
2023-10-30 12:29:54,796 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,797 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:54,797 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:54,802 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:54,805 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,806 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,806 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:54,806 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,809 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, <mixtensor.MixTensor object at 0x7f0f84d4d360>))
2023-10-30 12:29:54,810 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:54,810 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,812 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e140>, (<mixtensor.MixTensor object at 0x7f0f84d4ffa0>, <mixtensor.MixTensor object at 0x7f0f84d4fbe0>))
2023-10-30 12:29:54,812 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:54,812 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,814 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, (<mixtensor.MixTensor object at 0x7f0f84d4edd0>, <mixtensor.MixTensor object at 0x7f0f84d4dfc0>))
2023-10-30 12:29:54,814 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:54,814 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,816 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cac0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, <mixtensor.MixTensor object at 0x7f0f84d4ca90>))
2023-10-30 12:29:54,816 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,817 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:54,817 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,822 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:54,825 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,825 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,826 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:54,826 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,830 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, <mixtensor.MixTensor object at 0x7f0f84d4e8f0>))
2023-10-30 12:29:54,830 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:54,830 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,832 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fac0>, (<mixtensor.MixTensor object at 0x7f0f84d4da80>, <mixtensor.MixTensor object at 0x7f0f84d4fc70>))
2023-10-30 12:29:54,832 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:54,832 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,834 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4dd80>, (<mixtensor.MixTensor object at 0x7f0f84d4c1c0>, <mixtensor.MixTensor object at 0x7f0f84d4f040>))
2023-10-30 12:29:54,834 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:54,834 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,837 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d600>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:54,837 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,837 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:54,838 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,842 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:54,846 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,846 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,846 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:54,847 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,854 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, <mixtensor.MixTensor object at 0x7f0f84d4f070>))
2023-10-30 12:29:54,854 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:54,854 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,869 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ed40>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d45030>))
2023-10-30 12:29:54,869 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:54,869 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,871 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a60>, (<mixtensor.MixTensor object at 0x7f0f84d442e0>, <mixtensor.MixTensor object at 0x7f0f84d44a00>))
2023-10-30 12:29:54,872 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:54,872 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,874 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cac0>, (<mixtensor.MixTensor object at 0x7f0f84d4e860>, <mixtensor.MixTensor object at 0x7f0f84d4d240>))
2023-10-30 12:29:54,874 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,874 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:54,875 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,879 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:54,883 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,883 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,884 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:54,884 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,888 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, (<mixtensor.MixTensor object at 0x7f0f84d4f070>, <mixtensor.MixTensor object at 0x7f0f84d444f0>))
2023-10-30 12:29:54,888 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:54,888 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,890 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45510>, (<mixtensor.MixTensor object at 0x7f0f8578d540>, <mixtensor.MixTensor object at 0x7f0f8578dcf0>))
2023-10-30 12:29:54,891 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:54,891 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,892 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578caf0>, (<mixtensor.MixTensor object at 0x7f0f8578f490>, <mixtensor.MixTensor object at 0x7f0f8578d090>))
2023-10-30 12:29:54,892 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:54,893 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,894 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cd00>, (<mixtensor.MixTensor object at 0x7f0f8578dd80>, <mixtensor.MixTensor object at 0x7f0f8578ea40>))
2023-10-30 12:29:54,894 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,894 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:54,895 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,899 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:54,903 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,903 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,903 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:54,903 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,906 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da530>, (<mixtensor.MixTensor object at 0x7f0f857d9c90>, <mixtensor.MixTensor object at 0x7f0f857dab00>))
2023-10-30 12:29:54,907 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:54,907 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,908 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9870>, (<mixtensor.MixTensor object at 0x7f0f857d9780>, <mixtensor.MixTensor object at 0x7f0f8579fb20>))
2023-10-30 12:29:54,908 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:54,909 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,910 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8579f2b0>, (<mixtensor.MixTensor object at 0x7f0f8579f8e0>, <mixtensor.MixTensor object at 0x7f0f8579d000>))
2023-10-30 12:29:54,910 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:54,910 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,912 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da380>, (<mixtensor.MixTensor object at 0x7f0fe2f58e50>, <mixtensor.MixTensor object at 0x7f0fe2f58f10>))
2023-10-30 12:29:54,912 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,912 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:54,913 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,917 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:54,921 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,921 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,921 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:54,921 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,924 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fd00>, (<mixtensor.MixTensor object at 0x7f0f8572c1c0>, <mixtensor.MixTensor object at 0x7f0f8572f6a0>))
2023-10-30 12:29:54,924 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:54,924 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,926 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fa30>, (<mixtensor.MixTensor object at 0x7f0f8572d2d0>, <mixtensor.MixTensor object at 0x7f0f8572fbb0>))
2023-10-30 12:29:54,926 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:54,926 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,928 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cb50>, (<mixtensor.MixTensor object at 0x7f0f8572fdc0>, <mixtensor.MixTensor object at 0x7f0f8572d4b0>))
2023-10-30 12:29:54,928 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:54,928 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,930 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d6c0>, (<mixtensor.MixTensor object at 0x7f0f8572da20>, <mixtensor.MixTensor object at 0x7f0f8572cc10>))
2023-10-30 12:29:54,930 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,930 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:54,931 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,931 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:54,935 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,935 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,935 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:54,935 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,938 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fd00>, (<mixtensor.MixTensor object at 0x7f0f8572c1c0>, <mixtensor.MixTensor object at 0x7f0f8572d7b0>))
2023-10-30 12:29:54,938 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:54,938 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,940 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d45510>, (<mixtensor.MixTensor object at 0x7f0f84d44580>, <mixtensor.MixTensor object at 0x7f0f84d452d0>))
2023-10-30 12:29:54,940 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:54,940 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,942 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44430>, (<mixtensor.MixTensor object at 0x7f0f84d44670>, <mixtensor.MixTensor object at 0x7f0f84d44460>))
2023-10-30 12:29:54,942 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:54,942 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:54,944 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c1c0>, (<mixtensor.MixTensor object at 0x7f0f8572d7b0>, <mixtensor.MixTensor object at 0x7f0f8572d720>))
2023-10-30 12:29:54,944 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 12:29:54,944 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:54,945 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,948 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:54,949 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,949 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,949 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:54,949 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,950 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572fd00>
2023-10-30 12:29:54,950 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:54,950 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,950 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857da8f0>
2023-10-30 12:29:54,950 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:54,950 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d9c00>
2023-10-30 12:29:54,951 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:54,951 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,951 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857db0a0>
2023-10-30 12:29:54,951 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,952 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:54,952 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,952 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:54,953 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:54,953 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,953 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:54,953 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,961 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578d3f0>
2023-10-30 12:29:54,961 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:54,961 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,967 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578cee0>
2023-10-30 12:29:54,967 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:54,967 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,972 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8578f2b0>
2023-10-30 12:29:54,972 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:54,973 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:54,978 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d3100>
2023-10-30 12:29:54,980 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:54,981 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:54,981 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,982 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:54,982 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:54,982 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,983 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:54,983 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,983 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579f880>
2023-10-30 12:29:54,983 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:54,983 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,983 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579f0a0>
2023-10-30 12:29:54,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:54,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,984 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fbb0>
2023-10-30 12:29:54,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:54,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:54,984 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4df60>
2023-10-30 12:29:54,985 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,985 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:54,985 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:54,990 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:54,990 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-30 12:29:54,990 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:54,990 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:54,990 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs_k: {}
2023-10-30 12:29:54,991 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579d2a0>
2023-10-30 12:29:54,991 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:54,991 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs_k: {}
2023-10-30 12:29:54,991 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d780>
2023-10-30 12:29:54,992 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:54,992 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs_k: {}
2023-10-30 12:29:54,992 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c730>
2023-10-30 12:29:54,992 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:54,992 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs_k: {}
2023-10-30 12:29:54,993 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588e710>
2023-10-30 12:29:54,993 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:54,994 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:54,994 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:54,998 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:55,002 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,002 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,003 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:55,003 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,007 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c8b0>, (<mixtensor.MixTensor object at 0x7f0f84d4c040>, <mixtensor.MixTensor object at 0x7f0f84d4cee0>))
2023-10-30 12:29:55,007 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:55,007 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,009 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df00>, (<mixtensor.MixTensor object at 0x7f0f84d4d990>, <mixtensor.MixTensor object at 0x7f0f84d4d510>))
2023-10-30 12:29:55,010 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:55,010 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,012 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cca0>, (<mixtensor.MixTensor object at 0x7f0f84d4dc00>, <mixtensor.MixTensor object at 0x7f0f84d4ccd0>))
2023-10-30 12:29:55,012 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:55,013 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,015 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4d3c0>, <mixtensor.MixTensor object at 0x7f0f84d4c7f0>))
2023-10-30 12:29:55,015 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,015 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:55,016 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,021 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:55,024 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,024 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,025 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:55,025 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,029 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e440>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4e2c0>))
2023-10-30 12:29:55,029 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:55,029 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,032 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ea10>, (<mixtensor.MixTensor object at 0x7f0f84d4ec80>, <mixtensor.MixTensor object at 0x7f0f84d4ca00>))
2023-10-30 12:29:55,032 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:55,032 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,034 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c190>, (<mixtensor.MixTensor object at 0x7f0f84d4c970>, <mixtensor.MixTensor object at 0x7f0f84d4cfd0>))
2023-10-30 12:29:55,035 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:55,035 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,038 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ee60>, (<mixtensor.MixTensor object at 0x7f0f84d4f190>, <mixtensor.MixTensor object at 0x7f0f84d4e470>))
2023-10-30 12:29:55,038 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,038 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:55,039 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,044 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,048 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,048 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,048 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:55,049 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,052 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d210>, (<mixtensor.MixTensor object at 0x7f0f8578d990>, <mixtensor.MixTensor object at 0x7f0f8578cee0>))
2023-10-30 12:29:55,052 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:55,053 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,055 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e0e0>, (<mixtensor.MixTensor object at 0x7f0f8578f7f0>, <mixtensor.MixTensor object at 0x7f0f8578d180>))
2023-10-30 12:29:55,055 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:55,055 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,057 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d450>, (<mixtensor.MixTensor object at 0x7f0f8578d3f0>, <mixtensor.MixTensor object at 0x7f0f8572fca0>))
2023-10-30 12:29:55,057 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:55,058 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,060 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578de40>, (<mixtensor.MixTensor object at 0x7f0f8578f760>, <mixtensor.MixTensor object at 0x7f0f8578f2b0>))
2023-10-30 12:29:55,060 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,060 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:55,061 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,066 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,069 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,070 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,070 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:55,070 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,074 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fe50>, (<mixtensor.MixTensor object at 0x7f0f8572f790>, <mixtensor.MixTensor object at 0x7f0f8572cfa0>))
2023-10-30 12:29:55,074 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:55,074 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,076 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cd30>, (<mixtensor.MixTensor object at 0x7f0f8572cf70>, <mixtensor.MixTensor object at 0x7f0f8572d900>))
2023-10-30 12:29:55,077 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:55,077 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,079 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d840>, (<mixtensor.MixTensor object at 0x7f0f8572c2b0>, <mixtensor.MixTensor object at 0x7f0f8572d210>))
2023-10-30 12:29:55,079 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:55,079 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,081 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572ca60>, (<mixtensor.MixTensor object at 0x7f0f8572fa60>, <mixtensor.MixTensor object at 0x7f0f8572f940>))
2023-10-30 12:29:55,082 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,082 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:55,083 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,088 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,091 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,091 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,092 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:55,092 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,096 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dcc0>, (<mixtensor.MixTensor object at 0x7f0f8572dba0>, <mixtensor.MixTensor object at 0x7f0f8572cf10>))
2023-10-30 12:29:55,096 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:55,096 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,098 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d300>, (<mixtensor.MixTensor object at 0x7f0f8572c9a0>, <mixtensor.MixTensor object at 0x7f0f8572d060>))
2023-10-30 12:29:55,098 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:55,099 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,101 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d570>, (<mixtensor.MixTensor object at 0x7f0f8572c1c0>, <mixtensor.MixTensor object at 0x7f0f8572fd00>))
2023-10-30 12:29:55,101 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:55,101 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,104 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c940>, (<mixtensor.MixTensor object at 0x7f0f8572d810>, <mixtensor.MixTensor object at 0x7f0f8572db70>))
2023-10-30 12:29:55,105 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,105 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:55,106 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,111 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,114 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,114 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,115 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:55,115 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,119 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2aa0>, (<mixtensor.MixTensor object at 0x7f0f857d1030>, <mixtensor.MixTensor object at 0x7f0f857d18a0>))
2023-10-30 12:29:55,119 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:55,119 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,121 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0a30>, (<mixtensor.MixTensor object at 0x7f0f857d3d00>, <mixtensor.MixTensor object at 0x7f0f857d3100>))
2023-10-30 12:29:55,121 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:55,122 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,124 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1000>, (<mixtensor.MixTensor object at 0x7f0f857d9000>, <mixtensor.MixTensor object at 0x7f0f857d88e0>))
2023-10-30 12:29:55,124 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:55,124 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,126 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d2da0>, (<mixtensor.MixTensor object at 0x7f0f857d3e50>, <mixtensor.MixTensor object at 0x7f0f857d3af0>))
2023-10-30 12:29:55,127 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,127 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:55,127 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,132 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,136 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,136 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,137 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:55,137 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,140 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8e20>, (<mixtensor.MixTensor object at 0x7f0f857dacb0>, <mixtensor.MixTensor object at 0x7f0f857dbb50>))
2023-10-30 12:29:55,141 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:55,141 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,143 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db700>, (<mixtensor.MixTensor object at 0x7f0f857d9e10>, <mixtensor.MixTensor object at 0x7f0f857d9870>))
2023-10-30 12:29:55,143 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:55,144 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,146 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dab00>, (<mixtensor.MixTensor object at 0x7f0f857da530>, <mixtensor.MixTensor object at 0x7f0f857da380>))
2023-10-30 12:29:55,146 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:55,146 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,148 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db400>, (<mixtensor.MixTensor object at 0x7f0f857d80a0>, <mixtensor.MixTensor object at 0x7f0f857da3e0>))
2023-10-30 12:29:55,149 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,149 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:55,150 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,154 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,158 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,158 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,158 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:55,159 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,162 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8e20>, (<mixtensor.MixTensor object at 0x7f0f857dacb0>, <mixtensor.MixTensor object at 0x7f0f857dbc10>))
2023-10-30 12:29:55,163 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:55,163 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,165 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c90>, (<mixtensor.MixTensor object at 0x7f0f84d44220>, <mixtensor.MixTensor object at 0x7f0f84d449d0>))
2023-10-30 12:29:55,165 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:55,166 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,168 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d445b0>, (<mixtensor.MixTensor object at 0x7f0f84d44100>, <mixtensor.MixTensor object at 0x7f0f84d44f10>))
2023-10-30 12:29:55,168 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:55,168 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,172 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8f40>, (<mixtensor.MixTensor object at 0x7f0f857d9990>, <mixtensor.MixTensor object at 0x7f0f857dabc0>))
2023-10-30 12:29:55,173 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,173 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:55,174 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,178 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,181 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,182 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,182 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:55,182 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,186 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dacb0>, (<mixtensor.MixTensor object at 0x7f0f857dbc10>, <mixtensor.MixTensor object at 0x7f0f84d44a90>))
2023-10-30 12:29:55,186 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:55,187 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,190 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44550>, (<mixtensor.MixTensor object at 0x7f0f84d45240>, <mixtensor.MixTensor object at 0x7f0f857d9f00>))
2023-10-30 12:29:55,191 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:55,191 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,193 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dcc0>, (<mixtensor.MixTensor object at 0x7f0f8572f6a0>, <mixtensor.MixTensor object at 0x7f0f8579ed70>))
2023-10-30 12:29:55,193 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:55,194 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,196 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8070>, (<mixtensor.MixTensor object at 0x7f0f857d8b80>, <mixtensor.MixTensor object at 0x7f0f8579c700>))
2023-10-30 12:29:55,196 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,197 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:55,197 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,201 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,205 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,205 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,205 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:55,205 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,210 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578e0e0>, (<mixtensor.MixTensor object at 0x7f0f8578caf0>, <mixtensor.MixTensor object at 0x7f0f8578d750>))
2023-10-30 12:29:55,210 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:55,211 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,213 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e500>, (<mixtensor.MixTensor object at 0x7f0f84d4dd80>, <mixtensor.MixTensor object at 0x7f0f84d4f130>))
2023-10-30 12:29:55,213 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:55,213 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,215 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4f070>, <mixtensor.MixTensor object at 0x7f0f84d4c490>))
2023-10-30 12:29:55,215 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:55,216 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,217 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578de40>, (<mixtensor.MixTensor object at 0x7f0f8578cee0>, <mixtensor.MixTensor object at 0x7f0f8578e200>))
2023-10-30 12:29:55,218 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,218 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:55,219 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,223 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,227 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,227 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,227 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:55,227 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,231 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, (<mixtensor.MixTensor object at 0x7f0f84d4e2f0>, <mixtensor.MixTensor object at 0x7f0f84d4fe80>))
2023-10-30 12:29:55,231 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:55,231 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,233 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, (<mixtensor.MixTensor object at 0x7f0f84d4f8b0>, <mixtensor.MixTensor object at 0x7f0f84d4c4c0>))
2023-10-30 12:29:55,234 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:55,234 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,236 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f2b0>, (<mixtensor.MixTensor object at 0x7f0f84d4d1e0>, <mixtensor.MixTensor object at 0x7f0f84d4f4c0>))
2023-10-30 12:29:55,236 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:55,236 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,238 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e680>, (<mixtensor.MixTensor object at 0x7f0f84d4f0a0>, <mixtensor.MixTensor object at 0x7f0f84d4d600>))
2023-10-30 12:29:55,239 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,239 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:55,239 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,240 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,243 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,244 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,244 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:55,244 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,247 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e8f0>, (<mixtensor.MixTensor object at 0x7f0f84d4e2f0>, <mixtensor.MixTensor object at 0x7f0f84d4f430>))
2023-10-30 12:29:55,247 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:55,248 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,250 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, (<mixtensor.MixTensor object at 0x7f0f84d44550>, <mixtensor.MixTensor object at 0x7f0f84d45120>))
2023-10-30 12:29:55,250 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:55,251 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,253 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44340>, (<mixtensor.MixTensor object at 0x7f0f84d44a30>, <mixtensor.MixTensor object at 0x7f0f84d44070>))
2023-10-30 12:29:55,253 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:55,253 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,255 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c160>, (<mixtensor.MixTensor object at 0x7f0f84d4cac0>, <mixtensor.MixTensor object at 0x7f0f84d4d660>))
2023-10-30 12:29:55,255 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 12:29:55,255 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:55,256 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,259 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,260 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,260 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,260 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:55,260 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,261 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fe80>
2023-10-30 12:29:55,261 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:55,261 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,261 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4c3a0>
2023-10-30 12:29:55,261 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:55,262 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,262 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4e500>
2023-10-30 12:29:55,262 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:55,262 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,262 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4fe80>
2023-10-30 12:29:55,263 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,263 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:55,263 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:55,263 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,264 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,264 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,264 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:55,264 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,273 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579fd00>
2023-10-30 12:29:55,274 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:55,274 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,281 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579f070>
2023-10-30 12:29:55,281 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:55,281 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,288 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8579f880>
2023-10-30 12:29:55,289 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:55,289 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,294 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f430>
2023-10-30 12:29:55,296 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:55,297 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:55,297 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:55,298 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:55,299 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:55,299 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,299 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:55,299 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,300 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cd60>
2023-10-30 12:29:55,300 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:55,300 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,301 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ccd0>
2023-10-30 12:29:55,301 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:55,301 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,301 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c340>
2023-10-30 12:29:55,301 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:55,301 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,302 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572f9a0>
2023-10-30 12:29:55,302 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,302 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:55,302 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:55,307 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:55,307 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-30 12:29:55,307 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,308 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:55,308 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs_k: {}
2023-10-30 12:29:55,308 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d630>
2023-10-30 12:29:55,308 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:55,308 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs_k: {}
2023-10-30 12:29:55,309 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d720>
2023-10-30 12:29:55,309 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:55,309 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs_k: {}
2023-10-30 12:29:55,309 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572d7b0>
2023-10-30 12:29:55,309 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:55,309 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs_k: {}
2023-10-30 12:29:55,310 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8588e710>
2023-10-30 12:29:55,310 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,310 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:55,311 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:55,315 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:55,318 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,318 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,318 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:55,318 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,321 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ada0>, (<mixtensor.MixTensor object at 0x7f0f8565b820>, <mixtensor.MixTensor object at 0x7f0f8565ad70>))
2023-10-30 12:29:55,321 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:55,322 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,323 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578d540>, (<mixtensor.MixTensor object at 0x7f0f8578f490>, <mixtensor.MixTensor object at 0x7f0f8578dd80>))
2023-10-30 12:29:55,324 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:55,324 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,325 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578dcf0>, (<mixtensor.MixTensor object at 0x7f0f8578d090>, <mixtensor.MixTensor object at 0x7f0f8578ea40>))
2023-10-30 12:29:55,325 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:55,326 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,327 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f85659330>, (<mixtensor.MixTensor object at 0x7f0f8578caf0>, <mixtensor.MixTensor object at 0x7f0f8578d2a0>))
2023-10-30 12:29:55,328 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,328 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:55,328 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,333 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:55,336 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,336 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,337 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:55,337 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,340 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da740>, (<mixtensor.MixTensor object at 0x7f0f857db220>, <mixtensor.MixTensor object at 0x7f0f857d9300>))
2023-10-30 12:29:55,340 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:55,340 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,342 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8eb0>, (<mixtensor.MixTensor object at 0x7f0f857d8c40>, <mixtensor.MixTensor object at 0x7f0f857da890>))
2023-10-30 12:29:55,342 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:55,342 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,343 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9390>, (<mixtensor.MixTensor object at 0x7f0f857d8b20>, <mixtensor.MixTensor object at 0x7f0f857d8370>))
2023-10-30 12:29:55,344 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:55,344 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,345 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da4a0>, (<mixtensor.MixTensor object at 0x7f0f857d83d0>, <mixtensor.MixTensor object at 0x7f0f857da9e0>))
2023-10-30 12:29:55,346 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,346 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:55,346 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,351 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,354 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,355 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,355 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:55,355 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,358 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cca0>, (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, <mixtensor.MixTensor object at 0x7f0f84d4df00>))
2023-10-30 12:29:55,358 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:55,359 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,360 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e110>, (<mixtensor.MixTensor object at 0x7f0f84d4e950>, <mixtensor.MixTensor object at 0x7f0f84d4ef20>))
2023-10-30 12:29:55,360 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:55,361 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,362 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cee0>, (<mixtensor.MixTensor object at 0x7f0f84d4c040>, <mixtensor.MixTensor object at 0x7f0f84d4c580>))
2023-10-30 12:29:55,362 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:55,362 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,364 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e890>, (<mixtensor.MixTensor object at 0x7f0f84d4ead0>, <mixtensor.MixTensor object at 0x7f0f84d4f340>))
2023-10-30 12:29:55,364 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,364 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:55,365 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,369 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,373 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,373 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,373 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:55,373 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,376 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fbb0>, (<mixtensor.MixTensor object at 0x7f0f84d4df00>, <mixtensor.MixTensor object at 0x7f0f84d4ff10>))
2023-10-30 12:29:55,376 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:55,376 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,378 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df60>, (<mixtensor.MixTensor object at 0x7f0f84d4c610>, <mixtensor.MixTensor object at 0x7f0f84d4f010>))
2023-10-30 12:29:55,378 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:55,378 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,380 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e9b0>, (<mixtensor.MixTensor object at 0x7f0f84d4fa90>, <mixtensor.MixTensor object at 0x7f0f84d4f3d0>))
2023-10-30 12:29:55,380 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:55,380 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,382 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, (<mixtensor.MixTensor object at 0x7f0f84d4e440>, <mixtensor.MixTensor object at 0x7f0f84d4f820>))
2023-10-30 12:29:55,382 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,382 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:55,383 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,387 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,390 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,391 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,391 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:55,391 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,394 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df00>, (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, <mixtensor.MixTensor object at 0x7f0f84d4e1a0>))
2023-10-30 12:29:55,394 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:55,395 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,396 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f970>, (<mixtensor.MixTensor object at 0x7f0f84d4d8d0>, <mixtensor.MixTensor object at 0x7f0f84d4d960>))
2023-10-30 12:29:55,396 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:55,397 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,398 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4da50>, (<mixtensor.MixTensor object at 0x7f0f84d4eb30>, <mixtensor.MixTensor object at 0x7f0f84d4ffa0>))
2023-10-30 12:29:55,398 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:55,398 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,400 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ea10>, (<mixtensor.MixTensor object at 0x7f0f84d4c190>, <mixtensor.MixTensor object at 0x7f0f84d4ed40>))
2023-10-30 12:29:55,400 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,400 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:55,401 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,405 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,409 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,409 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,409 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:55,410 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,412 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, <mixtensor.MixTensor object at 0x7f0f84d4ff40>))
2023-10-30 12:29:55,413 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:55,413 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,415 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, (<mixtensor.MixTensor object at 0x7f0f84d4fbe0>, <mixtensor.MixTensor object at 0x7f0f84d4da80>))
2023-10-30 12:29:55,415 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:55,415 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,416 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4c1c0>, (<mixtensor.MixTensor object at 0x7f0f84d4d090>, <mixtensor.MixTensor object at 0x7f0f84d4f220>))
2023-10-30 12:29:55,417 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:55,417 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,418 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e110>, (<mixtensor.MixTensor object at 0x7f0f84d4fb80>, <mixtensor.MixTensor object at 0x7f0f84d4ce20>))
2023-10-30 12:29:55,418 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,419 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:55,419 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,423 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,427 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,427 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,427 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:55,427 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,431 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, <mixtensor.MixTensor object at 0x7f0f84d4fc70>))
2023-10-30 12:29:55,431 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:55,431 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,433 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f9a0>, (<mixtensor.MixTensor object at 0x7f0f84d4f0d0>, <mixtensor.MixTensor object at 0x7f0f84d4e500>))
2023-10-30 12:29:55,433 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:55,433 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,434 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fe80>, (<mixtensor.MixTensor object at 0x7f0f84d4fd60>, <mixtensor.MixTensor object at 0x7f0f84d4e680>))
2023-10-30 12:29:55,435 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:55,435 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,436 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4df60>, (<mixtensor.MixTensor object at 0x7f0f84d4db40>, <mixtensor.MixTensor object at 0x7f0f84d4f040>))
2023-10-30 12:29:55,437 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,437 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:55,437 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,442 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,445 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,445 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,446 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:55,446 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,449 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff40>, (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, <mixtensor.MixTensor object at 0x7f0f84d4ce50>))
2023-10-30 12:29:55,449 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:55,450 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,451 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4d780>, (<mixtensor.MixTensor object at 0x7f0f84d4c730>, <mixtensor.MixTensor object at 0x7f0f84d44040>))
2023-10-30 12:29:55,451 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:55,451 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,453 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44280>, (<mixtensor.MixTensor object at 0x7f0f84d442e0>, <mixtensor.MixTensor object at 0x7f0f84d44700>))
2023-10-30 12:29:55,453 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:55,453 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,455 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f970>, (<mixtensor.MixTensor object at 0x7f0f84d4ee60>, <mixtensor.MixTensor object at 0x7f0f84d4c160>))
2023-10-30 12:29:55,455 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,455 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:55,456 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,460 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,464 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,464 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,464 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:55,464 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,467 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4fc70>, (<mixtensor.MixTensor object at 0x7f0f84d4ce50>, <mixtensor.MixTensor object at 0x7f0f84d45030>))
2023-10-30 12:29:55,467 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:55,467 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,470 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a60>, (<mixtensor.MixTensor object at 0x7f0f84d444f0>, <mixtensor.MixTensor object at 0x7f0f8578d990>))
2023-10-30 12:29:55,470 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:55,470 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,472 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0fe297a020>, (<mixtensor.MixTensor object at 0x7f0f857fb160>, <mixtensor.MixTensor object at 0x7f0f8579f0d0>))
2023-10-30 12:29:55,472 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:55,472 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,473 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e890>, (<mixtensor.MixTensor object at 0x7f0f8578dcf0>, <mixtensor.MixTensor object at 0x7f0f8579cb20>))
2023-10-30 12:29:55,474 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,474 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:55,474 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,479 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,482 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,482 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,483 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:55,483 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,486 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0670>, (<mixtensor.MixTensor object at 0x7f0f857d18a0>, <mixtensor.MixTensor object at 0x7f0f857d3ac0>))
2023-10-30 12:29:55,486 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:55,487 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,488 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d0d00>, (<mixtensor.MixTensor object at 0x7f0f8565b640>, <mixtensor.MixTensor object at 0x7f0f85658d00>))
2023-10-30 12:29:55,489 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:55,489 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,490 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b220>, (<mixtensor.MixTensor object at 0x7f0f8565b4c0>, <mixtensor.MixTensor object at 0x7f0f8565b010>))
2023-10-30 12:29:55,490 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:55,491 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,492 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d18a0>, (<mixtensor.MixTensor object at 0x7f0f857d3ac0>, <mixtensor.MixTensor object at 0x7f0f857d2c20>))
2023-10-30 12:29:55,492 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,492 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:55,493 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,497 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,500 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,501 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,501 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:55,501 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,504 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ba00>, (<mixtensor.MixTensor object at 0x7f0f856581f0>, <mixtensor.MixTensor object at 0x7f0f85659330>))
2023-10-30 12:29:55,504 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:55,505 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,506 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b790>, (<mixtensor.MixTensor object at 0x7f0f8572d690>, <mixtensor.MixTensor object at 0x7f0f8572cfd0>))
2023-10-30 12:29:55,507 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:55,507 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,508 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f7c0>, (<mixtensor.MixTensor object at 0x7f0f8572d240>, <mixtensor.MixTensor object at 0x7f0f8572db10>))
2023-10-30 12:29:55,508 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:55,509 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,510 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f856581f0>, (<mixtensor.MixTensor object at 0x7f0f85659330>, <mixtensor.MixTensor object at 0x7f0f8565b310>))
2023-10-30 12:29:55,510 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,510 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:55,511 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,512 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,515 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,515 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,515 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:55,516 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,518 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db550>, (<mixtensor.MixTensor object at 0x7f0f857d8f10>, <mixtensor.MixTensor object at 0x7f0f8572ff40>))
2023-10-30 12:29:55,518 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:55,519 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,520 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dba0>, (<mixtensor.MixTensor object at 0x7f0f84d44a60>, <mixtensor.MixTensor object at 0x7f0f84d44580>))
2023-10-30 12:29:55,520 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:55,521 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,522 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44670>, (<mixtensor.MixTensor object at 0x7f0f84d45060>, <mixtensor.MixTensor object at 0x7f0f84d45180>))
2023-10-30 12:29:55,522 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:55,522 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,524 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9390>, (<mixtensor.MixTensor object at 0x7f0f8572ff40>, <mixtensor.MixTensor object at 0x7f0f8572dcc0>))
2023-10-30 12:29:55,524 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 12:29:55,524 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:55,525 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,528 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,529 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,529 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,529 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:55,529 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,529 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8eb0>
2023-10-30 12:29:55,530 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:55,530 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,530 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d9780>
2023-10-30 12:29:55,530 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:55,530 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,530 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8ac0>
2023-10-30 12:29:55,531 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:55,531 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,531 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f857d8eb0>
2023-10-30 12:29:55,531 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,531 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:55,532 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:55,532 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,532 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,533 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,533 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:55,533 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,540 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572ccd0>
2023-10-30 12:29:55,540 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:55,540 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,547 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572c310>
2023-10-30 12:29:55,547 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:55,547 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,552 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8572cd60>
2023-10-30 12:29:55,552 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:55,552 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,559 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f8576dea0>
2023-10-30 12:29:55,560 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:55,561 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 12:29:55,562 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:55,562 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:55,563 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 12:29:55,563 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,563 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-30 12:29:55,563 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,563 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f910>
2023-10-30 12:29:55,563 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-30 12:29:55,564 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,564 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d3c0>
2023-10-30 12:29:55,564 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-30 12:29:55,564 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,564 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ca00>
2023-10-30 12:29:55,564 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-30 12:29:55,565 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs_k: {}
2023-10-30 12:29:55,565 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4cfd0>
2023-10-30 12:29:55,565 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,565 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 12:29:55,566 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:55,570 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 12:29:55,570 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-30 12:29:55,570 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,570 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-30 12:29:55,571 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs_k: {}
2023-10-30 12:29:55,571 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d510>
2023-10-30 12:29:55,571 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-30 12:29:55,571 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs_k: {}
2023-10-30 12:29:55,572 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f4c0>
2023-10-30 12:29:55,572 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-30 12:29:55,572 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs_k: {}
2023-10-30 12:29:55,572 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f0a0>
2023-10-30 12:29:55,572 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-30 12:29:55,572 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs_k: {}
2023-10-30 12:29:55,573 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d990>
2023-10-30 12:29:55,573 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,573 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 12:29:55,574 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:55,578 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 12:29:55,581 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,581 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,581 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-30 12:29:55,582 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,586 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1150>, (<mixtensor.MixTensor object at 0x7f0f857d0d00>, <mixtensor.MixTensor object at 0x7f0f857d3910>))
2023-10-30 12:29:55,586 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-30 12:29:55,586 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,588 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1e40>, (<mixtensor.MixTensor object at 0x7f0f857d3d00>, <mixtensor.MixTensor object at 0x7f0f857d3e50>))
2023-10-30 12:29:55,588 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-30 12:29:55,588 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,590 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d3100>, (<mixtensor.MixTensor object at 0x7f0f857d3af0>, <mixtensor.MixTensor object at 0x7f0f857d18a0>))
2023-10-30 12:29:55,590 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-30 12:29:55,590 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,593 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d1540>, (<mixtensor.MixTensor object at 0x7f0f857d1c30>, <mixtensor.MixTensor object at 0x7f0f857d2c50>))
2023-10-30 12:29:55,593 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,594 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 12:29:55,594 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,598 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 12:29:55,601 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,601 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,602 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-30 12:29:55,602 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,605 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d420>, (<mixtensor.MixTensor object at 0x7f0f8572c9d0>, <mixtensor.MixTensor object at 0x7f0f8572dc60>))
2023-10-30 12:29:55,605 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-30 12:29:55,605 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,607 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572fca0>, (<mixtensor.MixTensor object at 0x7f0f8572d600>, <mixtensor.MixTensor object at 0x7f0f8572c5b0>))
2023-10-30 12:29:55,607 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-30 12:29:55,607 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,609 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572cf70>, (<mixtensor.MixTensor object at 0x7f0f8572c2b0>, <mixtensor.MixTensor object at 0x7f0f8572fa60>))
2023-10-30 12:29:55,609 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-30 12:29:55,609 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,611 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d2d0>, (<mixtensor.MixTensor object at 0x7f0f8572d270>, <mixtensor.MixTensor object at 0x7f0f8572fcd0>))
2023-10-30 12:29:55,611 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,611 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 12:29:55,611 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,615 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 12:29:55,619 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,619 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,619 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-30 12:29:55,619 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,622 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f940>, (<mixtensor.MixTensor object at 0x7f0f8572fd00>, <mixtensor.MixTensor object at 0x7f0f8572db70>))
2023-10-30 12:29:55,623 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-30 12:29:55,623 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,624 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f7c0>, (<mixtensor.MixTensor object at 0x7f0f8572c310>, <mixtensor.MixTensor object at 0x7f0f8572cd60>))
2023-10-30 12:29:55,625 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-30 12:29:55,625 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,626 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d720>, (<mixtensor.MixTensor object at 0x7f0f8572c340>, <mixtensor.MixTensor object at 0x7f0f8572dab0>))
2023-10-30 12:29:55,626 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-30 12:29:55,627 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,628 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572dd80>, (<mixtensor.MixTensor object at 0x7f0f8572d210>, <mixtensor.MixTensor object at 0x7f0f8572f6a0>))
2023-10-30 12:29:55,628 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,628 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 12:29:55,629 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,633 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 12:29:55,636 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,636 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,637 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-30 12:29:55,637 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,640 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572db70>, (<mixtensor.MixTensor object at 0x7f0f8565af20>, <mixtensor.MixTensor object at 0x7f0f8565ad70>))
2023-10-30 12:29:55,640 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-30 12:29:55,641 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,642 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565ae60>, (<mixtensor.MixTensor object at 0x7f0f8565b910>, <mixtensor.MixTensor object at 0x7f0f8565aef0>))
2023-10-30 12:29:55,642 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-30 12:29:55,643 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,644 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b3a0>, (<mixtensor.MixTensor object at 0x7f0f8565ba60>, <mixtensor.MixTensor object at 0x7f0f8565b610>))
2023-10-30 12:29:55,644 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-30 12:29:55,644 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,646 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565af20>, (<mixtensor.MixTensor object at 0x7f0f8565ad70>, <mixtensor.MixTensor object at 0x7f0f8565b820>))
2023-10-30 12:29:55,646 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,646 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 12:29:55,647 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,651 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 12:29:55,654 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,654 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,819 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-30 12:29:55,820 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,823 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572f940>, (<mixtensor.MixTensor object at 0x7f0f8565b220>, <mixtensor.MixTensor object at 0x7f0f856581f0>))
2023-10-30 12:29:55,823 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-30 12:29:55,823 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,825 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8576ef80>, (<mixtensor.MixTensor object at 0x7f0f8576fe50>, <mixtensor.MixTensor object at 0x7f0f8576cb50>))
2023-10-30 12:29:55,825 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-30 12:29:55,825 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,827 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db220>, (<mixtensor.MixTensor object at 0x7f0f857da740>, <mixtensor.MixTensor object at 0x7f0f857da5f0>))
2023-10-30 12:29:55,827 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-30 12:29:55,827 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,828 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8565b220>, (<mixtensor.MixTensor object at 0x7f0f856581f0>, <mixtensor.MixTensor object at 0x7f0f8565b790>))
2023-10-30 12:29:55,828 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,829 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 12:29:55,829 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,834 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 12:29:55,837 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,837 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,838 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-30 12:29:55,838 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,841 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572c430>, (<mixtensor.MixTensor object at 0x7f0f8572db70>, <mixtensor.MixTensor object at 0x7f0f857da8f0>))
2023-10-30 12:29:55,841 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-30 12:29:55,841 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,844 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c00>, (<mixtensor.MixTensor object at 0x7f0f857db0a0>, <mixtensor.MixTensor object at 0x7f0f857d9870>))
2023-10-30 12:29:55,844 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-30 12:29:55,844 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,846 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857da380>, (<mixtensor.MixTensor object at 0x7f0f857da3e0>, <mixtensor.MixTensor object at 0x7f0f857d9990>))
2023-10-30 12:29:55,846 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-30 12:29:55,846 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,847 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8572d720>, (<mixtensor.MixTensor object at 0x7f0f857da8f0>, <mixtensor.MixTensor object at 0x7f0f857d9000>))
2023-10-30 12:29:55,848 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,848 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 12:29:55,848 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,853 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 12:29:55,856 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,856 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,856 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-30 12:29:55,857 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,860 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8160>, (<mixtensor.MixTensor object at 0x7f0f857db220>, <mixtensor.MixTensor object at 0x7f0f857d8b80>))
2023-10-30 12:29:55,860 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-30 12:29:55,860 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,862 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8f40>, (<mixtensor.MixTensor object at 0x7f0f857d9780>, <mixtensor.MixTensor object at 0x7f0f857db940>))
2023-10-30 12:29:55,862 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-30 12:29:55,862 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,863 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9390>, (<mixtensor.MixTensor object at 0x7f0f857d9150>, <mixtensor.MixTensor object at 0x7f0f857d8c70>))
2023-10-30 12:29:55,864 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-30 12:29:55,864 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,865 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db220>, (<mixtensor.MixTensor object at 0x7f0f857d8b80>, <mixtensor.MixTensor object at 0x7f0f857d9f00>))
2023-10-30 12:29:55,866 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,866 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 12:29:55,866 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,871 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 12:29:55,874 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,874 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,874 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-30 12:29:55,875 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,877 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9c00>, (<mixtensor.MixTensor object at 0x7f0f857da380>, <mixtensor.MixTensor object at 0x7f0f857dbb50>))
2023-10-30 12:29:55,878 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-30 12:29:55,878 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,880 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d454e0>, (<mixtensor.MixTensor object at 0x7f0f84d449a0>, <mixtensor.MixTensor object at 0x7f0f84d44220>))
2023-10-30 12:29:55,880 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-30 12:29:55,880 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,881 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44100>, (<mixtensor.MixTensor object at 0x7f0f84d44400>, <mixtensor.MixTensor object at 0x7f0f84d44e20>))
2023-10-30 12:29:55,881 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-30 12:29:55,882 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,883 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857dbb50>, (<mixtensor.MixTensor object at 0x7f0f857dabc0>, <mixtensor.MixTensor object at 0x7f0f857d8f10>))
2023-10-30 12:29:55,883 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,883 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 12:29:55,884 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,888 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 12:29:55,891 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,891 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,892 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-30 12:29:55,892 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,895 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db220>, (<mixtensor.MixTensor object at 0x7f0f857d9c00>, <mixtensor.MixTensor object at 0x7f0f84d44f10>))
2023-10-30 12:29:55,895 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-30 12:29:55,895 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,898 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d445b0>, <mixtensor.MixTensor object at 0x7f0f8565af20>))
2023-10-30 12:29:55,898 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-30 12:29:55,898 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,900 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0fe2f58f10>, (<mixtensor.MixTensor object at 0x7f0f8578cb20>, <mixtensor.MixTensor object at 0x7f0f8578cfd0>))
2023-10-30 12:29:55,900 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-30 12:29:55,900 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,902 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f8578f280>, (<mixtensor.MixTensor object at 0x7f0f8578c220>, <mixtensor.MixTensor object at 0x7f0f8578f310>))
2023-10-30 12:29:55,902 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,902 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 12:29:55,903 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,910 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 12:29:55,915 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,916 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,916 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-30 12:29:55,917 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,933 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d8f40>, (<mixtensor.MixTensor object at 0x7f0f857db550>, <mixtensor.MixTensor object at 0x7f0f857dacb0>))
2023-10-30 12:29:55,933 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-30 12:29:55,934 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,937 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857d9390>, (<mixtensor.MixTensor object at 0x7f0f857d9cc0>, <mixtensor.MixTensor object at 0x7f0f84d4fe80>))
2023-10-30 12:29:55,937 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-30 12:29:55,937 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,941 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f430>, (<mixtensor.MixTensor object at 0x7f0f84d4fdc0>, <mixtensor.MixTensor object at 0x7f0f84d4ff40>))
2023-10-30 12:29:55,941 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-30 12:29:55,942 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,945 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f857db220>, (<mixtensor.MixTensor object at 0x7f0f857dab00>, <mixtensor.MixTensor object at 0x7f0f857dbc10>))
2023-10-30 12:29:55,945 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,946 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 12:29:55,946 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,951 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 12:29:55,955 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,955 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,955 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-30 12:29:55,956 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,959 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4f9a0>))
2023-10-30 12:29:55,959 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-30 12:29:55,959 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,961 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4cc10>, (<mixtensor.MixTensor object at 0x7f0f84d4cc70>, <mixtensor.MixTensor object at 0x7f0f84d4cac0>))
2023-10-30 12:29:55,961 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-30 12:29:55,961 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,962 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e4a0>, (<mixtensor.MixTensor object at 0x7f0f84d4e770>, <mixtensor.MixTensor object at 0x7f0f84d4dba0>))
2023-10-30 12:29:55,963 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-30 12:29:55,963 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,964 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4e560>, (<mixtensor.MixTensor object at 0x7f0f84d4e890>, <mixtensor.MixTensor object at 0x7f0f84d4e9e0>))
2023-10-30 12:29:55,964 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,965 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 12:29:55,965 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,966 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 12:29:55,969 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,970 [block.py:132 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,970 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-30 12:29:55,970 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,973 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4ff10>, (<mixtensor.MixTensor object at 0x7f0f84d4f8e0>, <mixtensor.MixTensor object at 0x7f0f84d4d780>))
2023-10-30 12:29:55,973 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-30 12:29:55,973 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,975 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d452a0>, (<mixtensor.MixTensor object at 0x7f0f84d449d0>, <mixtensor.MixTensor object at 0x7f0f84d44550>))
2023-10-30 12:29:55,975 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-30 12:29:55,975 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,977 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d44a30>, (<mixtensor.MixTensor object at 0x7f0f84d44b20>, <mixtensor.MixTensor object at 0x7f0f84d45390>))
2023-10-30 12:29:55,977 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-30 12:29:55,977 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 12:29:55,978 [block.py:196 in flexgen_forward] DEBUG - output: (<mixtensor.MixTensor object at 0x7f0f84d4f430>, (<mixtensor.MixTensor object at 0x7f0f84d4e1a0>, <mixtensor.MixTensor object at 0x7f0f84d4fc70>))
2023-10-30 12:29:55,979 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.Batches'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 12:29:55,979 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 12:29:55,979 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,983 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 12:29:55,984 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,984 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,984 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-30 12:29:55,984 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,985 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ea10>
2023-10-30 12:29:55,985 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-30 12:29:55,985 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,985 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4f9a0>
2023-10-30 12:29:55,985 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-30 12:29:55,985 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,986 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ce50>
2023-10-30 12:29:55,986 [block.py:178 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-30 12:29:55,986 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,986 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ea10>
2023-10-30 12:29:55,986 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.Batches'>: torch.Size([4, 1, 768])
2023-10-30 12:29:55,987 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 12:29:55,987 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 12:29:55,987 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 12:29:55,988 [block.py:131 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.Batches'>: torch.Size([4, 1, 768])",)
2023-10-30 12:29:55,988 [block.py:132 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 12:29:55,988 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 0
2023-10-30 12:29:55,988 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:55,995 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d780>
2023-10-30 12:29:55,995 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 1
2023-10-30 12:29:55,995 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:56,002 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4ccd0>
2023-10-30 12:29:56,002 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 2
2023-10-30 12:29:56,002 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:56,008 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d990>
2023-10-30 12:29:56,008 [block.py:178 in flexgen_forward] DEBUG - layer: lm_head, batch: 3
2023-10-30 12:29:56,008 [block.py:179 in flexgen_forward] DEBUG - args_k: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs_k: {}
2023-10-30 12:29:56,014 [block.py:196 in flexgen_forward] DEBUG - output: <mixtensor.MixTensor object at 0x7f0f84d4d780>
2023-10-30 12:29:56,016 [block.py:210 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 12:29:56,020 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-30 12:29:56,020 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 12:29:56,020 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-30 12:29:56,020 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 12:29:56,020 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-30 12:29:56,020 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 12:29:56,020 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-30 12:29:56,020 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 12:29:56,029 [block.py:85 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-30 12:29:56,029 [block.py:85 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-30 12:29:56,029 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-30 12:29:56,029 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-30 12:29:56,030 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-30 12:29:56,031 [block.py:85 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-30 12:29:56,031 [block.py:85 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-30 12:29:56,031 [block.py:85 in layer_reset] DEBUG - lm_head from flexgen to old.
