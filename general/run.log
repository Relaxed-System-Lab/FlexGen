2023-10-09 05:41:33,431 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmptgnhe6mg
2023-10-09 05:41:33,432 [instantiator.py:76 in _write] INFO - Writing /tmp/tmptgnhe6mg/_remote_module_non_scriptable.py
2023-10-09 05:41:33,563 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-09 05:41:33,656 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:41:35,516 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-09 05:41:35,868 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 05:41:35,868 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 05:41:35,868 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 05:41:35,868 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 05:41:36,765 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:41:36,868 [model.py:159 in check_disk] INFO - [], ['lm_head.weight']
2023-10-09 05:41:36,915 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:41:37,011 [model.py:159 in check_disk] INFO - [], ['lm_head.weight']
2023-10-09 05:41:37,012 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-09 05:41:37,020 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-09 05:41:37,020 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-09 05:41:37,021 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-09 05:41:37,022 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-09 05:41:37,023 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-09 05:41:37,024 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-09 05:41:37,025 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-09 05:41:37,026 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-09 05:41:37,027 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-09 05:41:37,028 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-09 05:41:37,029 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-09 05:41:37,030 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-09 05:41:37,031 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-09 05:41:37,032 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-09 05:41:37,033 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-09 05:41:37,034 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-09 05:41:37,034 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-09 05:41:37,036 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-09 05:41:37,037 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-09 05:41:37,066 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-09 05:41:37,066 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-09 05:41:37,066 [forward.py:46 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-09 05:41:37,066 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-09 05:41:37,066 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-09 05:41:37,067 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-09 05:41:37,068 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-09 05:41:37,068 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-09 05:41:37,068 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-09 05:41:37,068 [forward.py:46 in to_test_forward] DEBUG - lm_head to test forward
2023-10-09 05:41:37,127 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 05:41:37,327 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:37,328 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:37,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:37,330 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:37,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:37,344 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:37,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:37,355 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:37,357 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:37,364 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:37,366 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:37,373 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:37,375 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:37,383 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:37,385 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:37,392 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:37,394 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:37,400 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:37,402 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:37,409 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:37,411 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:37,418 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:37,419 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:37,426 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:37,427 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:37,434 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:37,435 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:37,443 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:37,445 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:37,445 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:37,446 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:37,454 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:37,459 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-09 05:41:37,459 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:37,468 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-09 05:41:37,468 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-09 05:41:37,468 [forward.py:26 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-09 05:41:37,468 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-09 05:41:37,468 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-09 05:41:37,469 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-09 05:41:37,470 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-09 05:41:37,470 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-09 05:41:37,470 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-09 05:41:37,470 [forward.py:26 in reset_forward] DEBUG - lm_head from test to old.
2023-10-09 05:41:37,470 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-09 05:41:37,470 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-09 05:41:37,470 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-09 05:41:37,470 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-09 05:41:37,471 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-09 05:41:37,472 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-09 05:41:37,472 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-09 05:41:37,472 [forward.py:117 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-09 05:41:37,472 [forward.py:117 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-09 05:41:37,519 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 05:41:37,691 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:37,692 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:37,692 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9]),)
2023-10-09 05:41:37,693 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:37,693 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9]),)
2023-10-09 05:41:37,693 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:37,693 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:37,694 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:37,694 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:37,694 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:37,694 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:41:37,695 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:41:37,695 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:37,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:37,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:37,702 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9]), 0)
2023-10-09 05:41:37,702 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:37,702 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9]), 0)
2023-10-09 05:41:37,703 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:37,703 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:37,703 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:37,704 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:37,704 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:37,704 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:41:37,705 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:41:37,705 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:37,705 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:37,710 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:37,716 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,716 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,716 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:37,724 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:37,729 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:37,733 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:37,738 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,738 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,738 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:37,741 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:37,744 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:37,748 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,748 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,749 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,749 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,749 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:37,755 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:37,759 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:37,763 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:37,767 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,768 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,768 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:37,770 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:37,774 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:37,778 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,778 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,778 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,778 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,779 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:37,783 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:37,787 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:37,791 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:37,794 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,794 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,795 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:37,796 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:37,800 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:37,803 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,804 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,804 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,804 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:37,808 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:37,811 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:37,815 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:37,818 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,818 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,819 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:37,820 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:37,824 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:37,827 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,828 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,828 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,828 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,828 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:37,831 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:37,834 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:37,837 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:37,840 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,840 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,840 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:37,842 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:37,845 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:37,849 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,849 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,849 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,850 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,850 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:37,853 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:37,856 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:37,859 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:37,862 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,863 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,863 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:37,864 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:37,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:37,872 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,872 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,872 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,872 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,872 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:37,876 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:37,880 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:37,884 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:37,887 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,888 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,888 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:37,890 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:37,893 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:37,897 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,897 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,897 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,897 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,897 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:37,902 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:37,906 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:37,909 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:37,913 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,913 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,913 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:37,915 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:37,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:37,922 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,922 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,922 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,922 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,923 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:37,927 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:37,931 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:37,935 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:37,938 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,939 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,939 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:37,941 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:37,944 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:37,948 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,948 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,948 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,948 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:37,953 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:37,957 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:37,961 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:37,965 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,965 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,965 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:37,967 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:37,970 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:37,974 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:37,974 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,975 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:37,975 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:37,975 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:37,979 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:37,983 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:37,987 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:37,991 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:37,992 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:37,992 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:37,995 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,000 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,001 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:38,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,002 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:38,002 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,002 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:38,023 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:38,027 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:38,031 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:38,035 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:41:38,035 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:41:38,036 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:38,037 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,038 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,039 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:38,039 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,039 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:38,039 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,039 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:38,039 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:38,040 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:38,040 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:38,040 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:41:38,040 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:41:38,040 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:38,041 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,041 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,042 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:41:38,042 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,042 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:41:38,042 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,042 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:38,058 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:38,072 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:38,086 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:38,101 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 50272])
2023-10-09 05:41:38,106 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 50272])
2023-10-09 05:41:38,106 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:38,111 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,112 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,113 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:38,113 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,113 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:38,114 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,114 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:38,114 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:38,114 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:38,115 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:38,115 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,115 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,115 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:38,116 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,117 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,120 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 10]), 9)
2023-10-09 05:41:38,121 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,121 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 10]), 9)
2023-10-09 05:41:38,121 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,121 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:38,122 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:38,122 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:38,122 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:38,122 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,123 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,123 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:38,123 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,127 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,130 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,131 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,131 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,131 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,131 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:38,134 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:38,137 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:38,139 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:38,141 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,142 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,142 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:38,144 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,147 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,151 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,152 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,152 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,152 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,152 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:38,156 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:38,159 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:38,161 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:38,163 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,164 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,164 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:38,166 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,169 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,173 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,173 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,174 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,174 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,174 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:38,180 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:38,183 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:38,185 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:38,188 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,188 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,189 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:38,190 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,194 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,198 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,198 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,198 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,198 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,198 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:38,202 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:38,204 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:38,206 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:38,208 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,208 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,209 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:38,210 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,218 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,218 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,219 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,219 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,219 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:38,222 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:38,224 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:38,226 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:38,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,229 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,229 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:38,231 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,234 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,238 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,238 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,238 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,238 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,239 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:38,242 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:38,244 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:38,246 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:38,248 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,248 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,248 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:38,249 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,253 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,257 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,257 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,257 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:38,261 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:38,263 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:38,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:38,267 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,268 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,268 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:38,269 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,273 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,277 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,277 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,277 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,277 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:38,284 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:38,286 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:38,288 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:38,290 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,291 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,291 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:38,292 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,296 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,300 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,300 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,300 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,301 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,301 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:38,304 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:38,306 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:38,308 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:38,310 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,310 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,311 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:38,312 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,316 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,320 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,320 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,320 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,320 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,320 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:38,324 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:38,326 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:38,328 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:38,330 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,331 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,331 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:38,332 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,336 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,340 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,340 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,341 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,341 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,341 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:38,345 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:38,347 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:38,349 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:38,351 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,351 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,351 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:38,353 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,357 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,357 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,357 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,358 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,358 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,358 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:38,361 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:38,363 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:38,366 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:38,368 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:41:38,368 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:41:38,368 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:38,369 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,370 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,370 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,370 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,371 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,371 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:38,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:38,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:38,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:38,372 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,372 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,372 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:38,372 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,373 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,373 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,373 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,373 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,373 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,374 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:38,384 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:38,393 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:38,401 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:38,410 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:38,410 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:38,411 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:38,417 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,417 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,418 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:38,418 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,418 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:38,418 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,419 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:38,419 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:38,419 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:38,419 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:38,419 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,420 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,420 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:38,420 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,421 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,425 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 11]), 10)
2023-10-09 05:41:38,425 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,425 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 11]), 10)
2023-10-09 05:41:38,425 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,425 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:38,426 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:38,426 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:38,426 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:38,426 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,427 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,427 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:38,427 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,431 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,435 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,435 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,435 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,435 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,435 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:38,438 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:38,441 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:38,443 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:38,445 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,446 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,446 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:38,448 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,451 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,455 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,455 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,456 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,456 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,456 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:38,459 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:38,461 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:38,463 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:38,465 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,465 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,465 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:38,467 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,470 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,474 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,474 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,474 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,474 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,475 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:38,477 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:38,479 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:38,481 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:38,483 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,483 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,483 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:38,485 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,488 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,492 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,492 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,492 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,492 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,493 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:38,496 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:38,498 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:38,499 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:38,501 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,502 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,502 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:38,503 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,507 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,511 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,511 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,511 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,511 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,511 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:38,514 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:38,516 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:38,518 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:38,520 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,520 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,520 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:38,522 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,525 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,529 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,529 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,529 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,530 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,530 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:38,533 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:38,535 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:38,536 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:38,538 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,538 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,538 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:38,540 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,544 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,548 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,548 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,548 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,548 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,548 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:38,551 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:38,553 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:38,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:38,557 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,557 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,557 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:38,558 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,562 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,566 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,566 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,566 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,566 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,566 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:38,570 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:38,572 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:38,573 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:38,575 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,575 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,576 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:38,577 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,580 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,585 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,585 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,585 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,585 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,586 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:38,589 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:38,590 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:38,592 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:38,594 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,594 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,595 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:38,596 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,599 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,603 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,604 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,604 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,604 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,604 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:38,607 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:38,610 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:38,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:38,614 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,614 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,614 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:38,615 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,619 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,623 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,623 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,623 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,624 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,624 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:38,639 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:38,641 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:38,643 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:38,645 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,646 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,646 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:38,647 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,651 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,651 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,651 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,652 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,652 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,652 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:38,655 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:38,658 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:38,660 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:38,662 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:41:38,662 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:41:38,662 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:38,664 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,665 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,666 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,666 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,666 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,666 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,666 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:38,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:38,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:38,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:38,667 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,668 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,668 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:38,668 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,668 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,669 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,669 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,669 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,669 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,670 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:38,680 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:38,689 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:38,698 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:38,707 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:38,707 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:38,707 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:38,715 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,716 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,716 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:38,716 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,717 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:38,717 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:38,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:38,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:38,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:38,718 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,718 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,718 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:38,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,719 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,723 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 12]), 11)
2023-10-09 05:41:38,723 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,723 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 12]), 11)
2023-10-09 05:41:38,723 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,723 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:38,723 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:38,724 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:38,724 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:38,724 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,724 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,724 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:38,725 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,728 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,732 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,732 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,732 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,732 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,732 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:38,736 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:38,738 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:38,740 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:38,742 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,742 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,742 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:38,744 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:38,748 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,751 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,752 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,752 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,752 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,752 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:38,755 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:38,757 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:38,759 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:38,761 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,761 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,761 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:38,763 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:38,766 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,770 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,770 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,770 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,771 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,771 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:38,773 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:38,775 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:38,777 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:38,779 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,779 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,780 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:38,781 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:38,785 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,788 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,789 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,789 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,789 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,789 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:38,792 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:38,794 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:38,796 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:38,798 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,798 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,799 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:38,800 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:38,804 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,808 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,808 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,808 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,809 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:38,813 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:38,815 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:38,817 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:38,818 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,819 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,819 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:38,821 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:38,824 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,828 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,828 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,828 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,828 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,828 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:38,831 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:38,833 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:38,835 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:38,837 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,837 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,838 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:38,839 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:38,842 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,846 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,846 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,847 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,847 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,847 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:38,850 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:38,852 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:38,853 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:38,855 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,855 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,856 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:38,857 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:38,860 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,864 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,864 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,865 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,865 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,865 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:38,868 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:38,870 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:38,871 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:38,873 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,873 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,873 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:38,875 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:38,878 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,882 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,882 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,882 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,883 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,883 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:38,886 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:38,887 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:38,889 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:38,891 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,892 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,892 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:38,893 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:38,897 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,901 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,901 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,901 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,901 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,902 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:38,905 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:38,907 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:38,908 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:38,910 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,911 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,911 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:38,912 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:38,916 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,920 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,920 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,920 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,920 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,920 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:38,924 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:38,925 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:38,927 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:38,929 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,929 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,929 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:38,931 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:38,935 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,935 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,935 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,935 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,936 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:38,936 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:38,938 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:38,940 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:38,942 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:38,944 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:41:38,945 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:41:38,945 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:38,946 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:38,947 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,947 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,947 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,947 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,947 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:38,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:38,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:38,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:38,948 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,949 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,949 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:38,949 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:38,949 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,950 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:38,950 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,950 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:38,950 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,950 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:38,958 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:38,964 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:38,972 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:38,979 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:38,980 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:38,980 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:38,984 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:38,985 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,985 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:38,985 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:38,986 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,986 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:38,986 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:38,986 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:38,986 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:38,986 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,987 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,987 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:38,987 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:38,988 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,992 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 13]), 12)
2023-10-09 05:41:38,992 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:38,992 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 13]), 12)
2023-10-09 05:41:38,992 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:38,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:38,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:38,993 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:38,993 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:38,993 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:38,993 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:38,994 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:38,994 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:38,997 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:39,001 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,001 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,001 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,002 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:39,005 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:39,007 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:39,008 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:39,010 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,011 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,011 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:39,012 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:39,016 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:39,020 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,020 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,020 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,020 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,020 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:39,023 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:39,025 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:39,027 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:39,029 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,029 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,029 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:39,031 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:39,034 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:39,038 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,038 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,038 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,039 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,039 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:39,042 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:39,044 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:39,046 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:39,047 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,048 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,048 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:39,049 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:39,054 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:39,058 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,058 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,058 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,058 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,058 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:39,062 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:39,064 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:39,065 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:39,067 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,068 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,068 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:39,069 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:39,073 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:39,077 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,077 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,077 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,077 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,078 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:39,080 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:39,082 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:39,083 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:39,085 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,085 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,086 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:39,087 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:39,091 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:39,094 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,094 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,095 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,095 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,095 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:39,099 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:39,101 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:39,103 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:39,105 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,105 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,105 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:39,106 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:39,110 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:39,114 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,114 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,114 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,114 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,115 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:39,117 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:39,119 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:39,122 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:39,123 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,124 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,124 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:39,125 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:39,129 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:39,132 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,133 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,133 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,133 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,133 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:39,136 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:39,138 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:39,139 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:39,141 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,141 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,141 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:39,142 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:39,146 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:39,150 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,150 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,150 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,150 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:39,153 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:39,156 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:39,157 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:39,159 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,159 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,159 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:39,161 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:39,164 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:39,168 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,168 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,168 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,169 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,169 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:39,171 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:39,173 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:39,175 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:39,176 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,177 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:39,178 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:39,182 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:39,186 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,186 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,186 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,186 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,186 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:39,189 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:39,191 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:39,193 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:39,194 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,195 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,195 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:39,197 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:39,200 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:39,201 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,201 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,201 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,201 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,201 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:39,204 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:39,206 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:39,208 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:39,209 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:41:39,210 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:41:39,210 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:39,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:39,212 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:39,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,212 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:39,213 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,213 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:39,213 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:39,213 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:39,213 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:39,213 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:39,214 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:39,214 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:39,214 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:39,214 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:39,215 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:39,215 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,215 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:39,215 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,215 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:39,215 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:39,223 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:39,230 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:39,237 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:39,244 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:39,245 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:39,245 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:39,249 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:39,250 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:39,250 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:39,250 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:39,250 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:39,251 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:39,251 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:39,251 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:39,251 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:39,251 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:39,251 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:39,252 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:39,252 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:39,252 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:39,253 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:39,256 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 14]), 13)
2023-10-09 05:41:39,256 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:39,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 14]), 13)
2023-10-09 05:41:39,257 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:39,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:39,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:39,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:39,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:39,258 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:39,258 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:39,258 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:39,259 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:39,262 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:39,266 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,266 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,266 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,266 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,266 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:39,269 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:39,271 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:39,272 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:39,274 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,275 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,275 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:39,276 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:39,280 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:39,284 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,284 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,284 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,284 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,284 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:39,287 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:39,289 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:39,291 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:39,292 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,293 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,293 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:39,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:39,298 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:39,302 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,302 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,302 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,302 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,302 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:39,305 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:39,307 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:39,308 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:39,310 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,310 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,310 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:39,312 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:39,315 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:39,319 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,319 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,319 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,319 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,320 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:39,323 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:39,325 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:39,326 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:39,328 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,328 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,328 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:39,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:39,333 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:39,338 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,338 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,338 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,338 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,338 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:39,341 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:39,343 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:39,344 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:39,346 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,346 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,347 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:39,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:39,352 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:39,356 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,356 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,356 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,356 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,356 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:39,359 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:39,361 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:39,362 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:39,364 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,364 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,365 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:39,366 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:39,370 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:39,374 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:39,375 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,375 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:39,375 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:39,375 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:39,379 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:39,382 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:39,384 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:39,386 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:39,387 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:39,387 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:39,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:39,392 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:40,650 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,650 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,650 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,651 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,651 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:40,657 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:40,660 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:40,664 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:40,667 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:40,668 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:40,668 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:40,670 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:40,674 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:40,680 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,680 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,681 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,681 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,681 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:40,685 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:40,688 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:40,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:40,691 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:40,691 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:40,691 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:40,693 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:40,697 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:40,701 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,701 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,701 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,701 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,702 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:40,705 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:40,707 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:40,710 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:40,712 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:40,712 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:40,713 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:40,715 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:40,722 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:40,728 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,728 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,728 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,728 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,729 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:40,732 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:40,735 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:40,737 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:40,739 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:40,740 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:40,740 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:40,743 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:40,746 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:40,747 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,747 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,748 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,748 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:40,752 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:40,754 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:40,755 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:40,757 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:41:40,758 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:41:40,758 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:40,759 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:40,760 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:40,761 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,761 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:40,761 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,761 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:40,761 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:40,762 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:40,762 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:40,762 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:40,763 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:40,763 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:40,763 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:40,764 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:40,764 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:40,765 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,765 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:40,765 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,765 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:40,766 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:40,776 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:40,785 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:40,794 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:40,803 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:40,804 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:40,804 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:40,809 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:40,810 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:40,810 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:40,810 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:40,811 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:40,811 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:40,811 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:40,811 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:40,812 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:40,812 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:40,812 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:40,813 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:40,813 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:40,813 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:40,814 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:40,817 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 15]), 14)
2023-10-09 05:41:40,818 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:40,818 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 15]), 14)
2023-10-09 05:41:40,818 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:40,818 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:40,818 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:40,819 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:40,819 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:40,819 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:40,819 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:40,819 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:40,820 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:40,823 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:40,827 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,827 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,828 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,828 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,828 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:40,832 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:40,836 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:40,838 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:40,840 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,841 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,841 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:40,844 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:40,848 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:40,853 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,853 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,853 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,853 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,853 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:40,856 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:40,860 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:40,862 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:40,864 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,864 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,865 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:40,867 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:40,873 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:40,879 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,879 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,879 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,879 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,880 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:40,883 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:40,887 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:40,890 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:40,894 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,894 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,895 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:40,896 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:40,900 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:40,905 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,905 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,905 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,905 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,906 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:40,909 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:40,912 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:40,914 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:40,916 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,916 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,917 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:40,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:40,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:40,928 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,928 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,928 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,928 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,929 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:40,932 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:40,935 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:40,938 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:40,940 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,941 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,941 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:40,943 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:40,947 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:40,951 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,951 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,952 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:40,956 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:40,958 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:40,961 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:40,963 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,964 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,964 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:40,965 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:40,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:40,975 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:40,975 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,976 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:40,976 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:40,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:40,979 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:40,981 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:40,983 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:40,986 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:40,986 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:40,986 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:40,989 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:40,995 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,001 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,002 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,002 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,002 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:41,005 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:41,007 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:41,009 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:41,011 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:41,011 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:41,011 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:41,013 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,017 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,023 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,023 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,024 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,024 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,024 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:41,027 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:41,030 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:41,033 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:41,034 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:41,035 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:41,035 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:41,037 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,040 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,045 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,045 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,045 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,045 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,045 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:41,049 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:41,051 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:41,054 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:41,057 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:41,057 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:41,058 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:41,059 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,064 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,069 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,069 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,070 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,070 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,070 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:41,073 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:41,074 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:41,077 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:41,079 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:41,080 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:41,080 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:41,082 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,087 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,088 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,088 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,088 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,088 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,088 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:41,091 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:41,094 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:41,096 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:41,098 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:41:41,098 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:41:41,098 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:41,099 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,100 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,101 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,101 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,101 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,101 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,101 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:41,101 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:41,101 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:41,102 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:41,102 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,102 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,102 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:41,103 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,103 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,103 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,103 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,104 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,104 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,104 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:41,113 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:41,121 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:41,130 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:41,138 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:41,139 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:41,139 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:41,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,146 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:41,146 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,146 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:41,147 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,147 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:41,147 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:41,147 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:41,148 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:41,148 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,148 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,148 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:41,149 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,149 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,153 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 16]), 15)
2023-10-09 05:41:41,153 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,153 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 16]), 15)
2023-10-09 05:41:41,154 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,154 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:41,154 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:41,155 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:41,155 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:41,155 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,156 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,156 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:41,156 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,160 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,165 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,165 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,166 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,166 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,166 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:41,169 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:41,172 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:41,174 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:41,176 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,177 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,177 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:41,180 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,184 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,189 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,189 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,189 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,189 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,189 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:41,192 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:41,194 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:41,196 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:41,198 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,198 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,199 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:41,200 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,205 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,210 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,211 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,211 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,211 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,212 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:41,215 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:41,218 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:41,221 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:41,223 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,224 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,224 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:41,225 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,229 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,233 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,233 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,233 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,233 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,233 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:41,236 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:41,238 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:41,240 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:41,242 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,243 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,243 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:41,244 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,248 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,252 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,252 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,252 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,253 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,253 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:41,256 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:41,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:41,260 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:41,261 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,262 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,262 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:41,263 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,267 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,270 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,270 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,271 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,271 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,271 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:41,274 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:41,276 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:41,278 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:41,280 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,281 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,281 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:41,282 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,289 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,290 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,290 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,290 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,290 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:41,293 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:41,295 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:41,297 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:41,299 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,299 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,299 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:41,301 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,305 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,308 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,309 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,309 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,309 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,309 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:41,312 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:41,314 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:41,316 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:41,318 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,318 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,318 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:41,320 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,323 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,327 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,327 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,328 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,328 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,328 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:41,331 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:41,333 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:41,336 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:41,339 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,339 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,340 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:41,341 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,344 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,348 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,348 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,348 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,349 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,349 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:41,353 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:41,355 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:41,356 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:41,358 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,359 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,359 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:41,360 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,368 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,368 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,368 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,368 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:41,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:41,373 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:41,376 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:41,378 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,379 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,379 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:41,380 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,385 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,385 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,385 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,385 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,385 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:41,388 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:41,391 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:41,393 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:41,394 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:41:41,395 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:41:41,395 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:41,396 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,397 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,397 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,397 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,398 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,398 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,398 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:41,398 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:41,398 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:41,398 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:41,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,399 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:41,399 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,399 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,400 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,400 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,400 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,400 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,400 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:41,410 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:41,417 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:41,425 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:41,434 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:41,434 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:41,435 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:41,439 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,440 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,440 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:41,441 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,441 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:41,441 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,441 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:41,442 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:41,442 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:41,442 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:41,442 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,443 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,443 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:41,443 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,444 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,448 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 17]), 16)
2023-10-09 05:41:41,448 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,448 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 17]), 16)
2023-10-09 05:41:41,448 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,449 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:41,449 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:41,449 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:41,450 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:41,450 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,450 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,450 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:41,451 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,458 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,458 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,459 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,459 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,459 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:41,463 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:41,465 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:41,466 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:41,469 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,469 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,470 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:41,471 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,475 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,479 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,479 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,479 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,479 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,479 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:41,483 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:41,485 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:41,487 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:41,489 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,489 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,489 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:41,491 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,494 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,498 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,498 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,498 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,499 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,499 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:41,502 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:41,504 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:41,506 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:41,508 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,508 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,509 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:41,510 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,514 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,517 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,518 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,518 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,518 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,518 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:41,521 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:41,523 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:41,526 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:41,528 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,528 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,528 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:41,530 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,533 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,537 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,537 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,538 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,538 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,538 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:41,542 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:41,544 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:41,546 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:41,549 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,549 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,549 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:41,551 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,555 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,558 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,558 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,559 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,559 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,559 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:41,562 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:41,565 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:41,567 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:41,570 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,570 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,570 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:41,572 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,575 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,579 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,579 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,579 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,579 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,580 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:41,584 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:41,586 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:41,588 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:41,591 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,592 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,592 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:41,593 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,597 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,600 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,601 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,601 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,601 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,601 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:41,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:41,607 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:41,610 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:41,612 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,612 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,613 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:41,614 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,617 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,621 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,622 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,622 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,622 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,622 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:41,626 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:41,629 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:41,631 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:41,633 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,634 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,634 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:41,635 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,639 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,642 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,643 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,643 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,643 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,643 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:41,646 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:41,649 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:41,651 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:41,653 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,654 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,654 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:41,655 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,659 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,663 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,663 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,663 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,663 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,663 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:41,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:41,670 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:41,672 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:41,674 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,674 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,674 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:41,676 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,679 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,680 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,680 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,680 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,680 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,681 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:41,684 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:41,687 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:41,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:41,691 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:41:41,691 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:41:41,692 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:41,693 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,694 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,694 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,694 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,694 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,694 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,694 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:41,695 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:41,695 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:41,695 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:41,695 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,695 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,696 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:41,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,697 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,697 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,697 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,697 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,697 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:41,708 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:41,720 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:41,732 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:41,741 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:41,741 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:41,742 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:41,746 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,747 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,747 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:41,747 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,747 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:41,747 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:41,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:41,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:41,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:41,749 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,749 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,749 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:41,750 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:41,750 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,755 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 18]), 17)
2023-10-09 05:41:41,755 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,755 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 18]), 17)
2023-10-09 05:41:41,755 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,755 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:41,756 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:41,756 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:41,756 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:41,756 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,757 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,757 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:41,757 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:41,761 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,765 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,765 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,765 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,765 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,765 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:41,772 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:41,775 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:41,777 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:41,779 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,780 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,780 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:41,781 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:41,785 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,789 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,789 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,789 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,790 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,790 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:41,794 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:41,796 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:41,798 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:41,801 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,801 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,802 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:41,803 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:41,807 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,810 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,811 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,811 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,811 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,811 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:41,815 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:41,817 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:41,819 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:41,821 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,821 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,821 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:41,823 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:41,826 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,830 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,830 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,830 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,831 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:41,834 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:41,836 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:41,838 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:41,841 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,841 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,841 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:41,843 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:41,846 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,850 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,850 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,850 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,850 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,851 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:41,854 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:41,856 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:41,858 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:41,860 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,860 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,860 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:41,862 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:41,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,869 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,869 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,869 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,870 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,870 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:41,873 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:41,875 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:41,877 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:41,880 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,880 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,880 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:41,882 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:41,885 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,889 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,889 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,889 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,890 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,890 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:41,893 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:41,895 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:41,897 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:41,899 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,900 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,900 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:41,901 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:41,905 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,908 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,909 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,909 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,909 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,909 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:41,912 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:41,914 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:41,916 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:41,918 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,918 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,918 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:41,919 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:41,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,927 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,927 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,927 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,927 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,927 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:41,930 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:41,932 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:41,934 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:41,935 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,936 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,936 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:41,937 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:41,941 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,945 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,945 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,945 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,945 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,945 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:41,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:41,950 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:41,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:41,953 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,954 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,954 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:41,955 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:41,959 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,963 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,963 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,963 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,963 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,963 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:41,966 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:41,969 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:41,971 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:41,973 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,973 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,973 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:41,975 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:41,979 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,979 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,979 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,980 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,980 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:41,980 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:41,983 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:41,985 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:41,986 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:41,988 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:41:41,988 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:41:41,989 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:41,990 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:41,991 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,991 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,991 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,991 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,991 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,991 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:41,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:41,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:41,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:41,992 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:41,992 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:41,993 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:41,993 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:41,993 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:41,994 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:41,994 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:41,994 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:41,994 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:41,994 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:42,003 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:42,011 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:42,019 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:42,027 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:42,028 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:42,028 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:42,032 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:42,033 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,033 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:42,033 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,033 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:42,034 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,034 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:42,034 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:42,034 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:42,034 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:42,034 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,035 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,035 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:42,035 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,036 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,040 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 19]), 18)
2023-10-09 05:41:42,040 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,040 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 19]), 18)
2023-10-09 05:41:42,040 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,040 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:42,040 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:42,040 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:42,041 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:42,041 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,041 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,041 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:42,042 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,045 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:42,049 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,049 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,049 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,049 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,049 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:42,053 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:42,055 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:42,057 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:42,059 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,059 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,059 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:42,062 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:42,066 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:42,070 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,070 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,070 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,070 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,070 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:42,074 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:42,075 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:42,077 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:42,079 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,079 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,079 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:42,081 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:42,084 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:42,088 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,088 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,088 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,089 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,089 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:42,092 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:42,094 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:42,096 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:42,098 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,099 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,099 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:42,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:42,106 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:42,110 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,110 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,110 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,110 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,110 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:42,113 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:42,115 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:42,117 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:42,119 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,119 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,119 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:42,121 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:42,125 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:42,129 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,129 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,129 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,129 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,129 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:42,132 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:42,134 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:42,136 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:42,138 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,138 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,138 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:42,140 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:42,143 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:42,147 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,147 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,147 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,147 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,148 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:42,151 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:42,153 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:42,154 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:42,156 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,156 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,157 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:42,158 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:42,162 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:42,166 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,166 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,166 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,166 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,166 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:42,178 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:42,180 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:42,182 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:42,183 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,184 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,184 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:42,185 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:42,189 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:42,193 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,193 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,193 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,193 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,193 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:42,197 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:42,198 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:42,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:42,202 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,202 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,203 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:42,204 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:42,207 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:42,211 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,211 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,212 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,212 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:42,215 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:42,217 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:42,219 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:42,221 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,221 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,221 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:42,223 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:42,227 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:42,231 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,231 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,231 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,231 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,231 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:42,234 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:42,236 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:42,238 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:42,239 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,240 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,240 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:42,241 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:42,245 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:42,249 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,249 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,249 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,249 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:42,252 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:42,254 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:42,256 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:42,258 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,258 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,258 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:42,260 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:42,263 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:42,264 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,264 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,264 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,264 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:42,268 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:42,269 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:42,271 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:42,273 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:41:42,273 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:41:42,274 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:42,275 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:42,276 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:42,276 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,276 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,276 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,276 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,276 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:42,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:42,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:42,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:42,277 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,277 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,277 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:42,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:42,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:42,279 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,279 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,279 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,279 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,279 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:42,287 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:42,299 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:42,316 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:42,331 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:42,344 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:42,344 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:42,349 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:42,350 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,350 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:42,350 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,351 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:42,351 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,351 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:42,352 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:42,352 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:42,352 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:42,353 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,353 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,353 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:42,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,354 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,358 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 20]), 19)
2023-10-09 05:41:42,358 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,359 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 20]), 19)
2023-10-09 05:41:42,359 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,359 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:42,359 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:42,360 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:42,360 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:42,360 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,361 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,361 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:42,361 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,365 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:42,369 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,369 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,369 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,369 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,369 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:42,372 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:42,374 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:42,376 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:42,378 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,378 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,378 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:42,380 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:42,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:42,388 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,388 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,388 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,388 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,388 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:42,391 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:42,393 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:42,395 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:42,396 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,397 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,397 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:42,399 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:42,403 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:42,406 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,406 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,407 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,407 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,407 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:42,410 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:42,412 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:42,414 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:42,415 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,416 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,416 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:42,418 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:42,421 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:42,425 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,425 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,425 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,425 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,426 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:42,429 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:42,430 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:42,432 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:42,434 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,434 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,434 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:42,436 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:42,439 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:42,443 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,444 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,444 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,444 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,444 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:42,447 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:42,448 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:42,450 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:42,452 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,452 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,453 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:42,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:42,458 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:42,461 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,462 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,462 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,462 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,462 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:42,466 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:42,468 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:42,469 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:42,471 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,471 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,472 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:42,473 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:42,476 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:42,480 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,481 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,481 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,481 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,481 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:42,484 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:42,486 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:42,488 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:42,490 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,490 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,491 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:42,492 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:42,496 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:42,499 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,500 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,500 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,500 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:42,503 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:42,505 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:42,506 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:42,508 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,508 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,508 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:42,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:42,513 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:42,517 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,517 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,517 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,518 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,518 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:42,521 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:42,523 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:42,525 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:42,527 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,527 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,528 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:42,529 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:42,533 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:42,536 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,536 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,537 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,537 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,537 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:42,540 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:42,541 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:42,543 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:42,545 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,545 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,545 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:42,546 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:42,550 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:42,554 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,554 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,554 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,555 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:42,557 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:42,559 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:42,561 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:42,563 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,564 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,564 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:42,565 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:42,569 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:42,570 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,570 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,570 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,909 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,910 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:42,913 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:42,918 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:42,920 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:42,921 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:41:42,922 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:41:42,922 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:42,924 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:42,924 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:42,925 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,925 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,925 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,925 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,925 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:42,926 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:42,926 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:42,926 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:42,926 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,927 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,927 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:42,927 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:42,928 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:42,928 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,928 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,929 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,929 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,929 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:42,938 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:42,946 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:42,954 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:42,961 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:42,962 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:42,963 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:42,967 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:42,968 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,969 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:42,969 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,969 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:42,969 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,969 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:42,969 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:42,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:42,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:42,970 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,970 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,970 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:42,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:42,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,975 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 21]), 20)
2023-10-09 05:41:42,975 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:42,975 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 21]), 20)
2023-10-09 05:41:42,976 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:42,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:42,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:42,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:42,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:42,977 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:42,977 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:42,977 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:42,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:42,981 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:42,985 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:42,985 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,985 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:42,985 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:42,985 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:42,989 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:42,991 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:42,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:42,994 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:42,995 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:42,995 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:42,997 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,000 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,005 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,005 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,006 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,006 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,006 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:43,009 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:43,011 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:43,013 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:43,015 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,015 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,015 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:43,017 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,021 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,024 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,025 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,025 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,025 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,025 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:43,028 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:43,030 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:43,032 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:43,034 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,035 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,035 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:43,036 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,040 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,044 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,044 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,044 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,044 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,045 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:43,048 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:43,050 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:43,052 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:43,054 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,055 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,055 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:43,057 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,060 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,064 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,064 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,064 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,064 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,065 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:43,067 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:43,069 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:43,071 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:43,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,073 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:43,075 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,079 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,083 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,083 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,083 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,083 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:43,086 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:43,088 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:43,090 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:43,092 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,092 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,093 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:43,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,098 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,102 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,103 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,103 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,103 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,103 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:43,106 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:43,108 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:43,110 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:43,111 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,112 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,112 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:43,113 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,117 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,121 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,121 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,121 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,121 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,121 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:43,124 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:43,126 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:43,128 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:43,130 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,131 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,131 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:43,132 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,136 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,140 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,140 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,141 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,141 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,141 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:43,144 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:43,146 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:43,149 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:43,151 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,151 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,152 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:43,153 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,157 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,160 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,161 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,161 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,161 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,161 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:43,164 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:43,166 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:43,168 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:43,170 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,171 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,171 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:43,172 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,175 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,179 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,179 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,180 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,180 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,180 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:43,183 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:43,185 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:43,187 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:43,188 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,189 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,189 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:43,190 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,194 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,194 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,195 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,195 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,195 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,195 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:43,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:43,202 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:43,204 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:43,206 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:41:43,206 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:41:43,206 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:43,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,209 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,209 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,209 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,209 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,209 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:43,209 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:43,210 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:43,210 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:43,210 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,210 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,210 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:43,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,211 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,211 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,212 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,212 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:43,220 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:43,227 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:43,234 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:43,241 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:43,242 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:43,242 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:43,248 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,248 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,249 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:43,249 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,249 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:43,249 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:43,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:43,250 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:43,250 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:43,250 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,250 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,250 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:43,251 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,251 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,255 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 22]), 21)
2023-10-09 05:41:43,255 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,255 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 22]), 21)
2023-10-09 05:41:43,255 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,255 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:43,256 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:43,256 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:43,256 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:43,257 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,257 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,257 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:43,257 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,261 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,264 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,265 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,265 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,265 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:43,275 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:43,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:43,279 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:43,281 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,282 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,282 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:43,283 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,287 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,291 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,291 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,292 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,292 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,292 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:43,295 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:43,298 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:43,300 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:43,302 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,302 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,302 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:43,304 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,307 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,312 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,312 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,312 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,312 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,313 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:43,316 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:43,318 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:43,320 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:43,322 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,323 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,323 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:43,325 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,328 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,332 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,332 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,332 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,332 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,333 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:43,336 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:43,337 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:43,340 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:43,343 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,344 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,344 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:43,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,350 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,354 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,354 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,354 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,354 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,354 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:43,357 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:43,359 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:43,362 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:43,363 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,364 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,364 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:43,366 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,369 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,373 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,373 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,373 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,373 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,374 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:43,377 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:43,378 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:43,381 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:43,382 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,383 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,383 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:43,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,388 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,392 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,392 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,393 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,393 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:43,397 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:43,399 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:43,401 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:43,402 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,404 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,404 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:43,405 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,409 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,413 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,413 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,413 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,413 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,413 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:43,417 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:43,418 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:43,420 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:43,422 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,423 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,423 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:43,424 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,428 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,432 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,432 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,432 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,432 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,433 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:43,436 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:43,438 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:43,441 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:43,444 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,444 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,444 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:43,446 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,449 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,453 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,453 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,453 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,453 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,453 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:43,457 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:43,459 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:43,461 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:43,463 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,463 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,463 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:43,464 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,468 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,472 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,472 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,472 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,472 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,472 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:43,476 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:43,477 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:43,479 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:43,481 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,481 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,481 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:43,483 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,486 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,487 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,487 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,487 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,487 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,487 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:43,490 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:43,493 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:43,495 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:43,496 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:41:43,497 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:41:43,497 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:43,498 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,499 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,499 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,499 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,499 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,500 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:43,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:43,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:43,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:43,501 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,501 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,501 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:43,501 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,502 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,502 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,503 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,503 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,503 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,503 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:43,512 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:43,520 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:43,528 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:43,535 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:43,536 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:43,536 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:43,541 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,541 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,542 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:43,542 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,542 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:43,542 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,542 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:43,543 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:43,543 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:43,543 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:43,543 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,543 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,543 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:43,544 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,544 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,548 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 23]), 22)
2023-10-09 05:41:43,548 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,548 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 23]), 22)
2023-10-09 05:41:43,548 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,548 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:43,549 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:43,549 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:43,549 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:43,549 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,550 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,550 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:43,550 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,553 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,557 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,557 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,557 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,557 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,557 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:43,561 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:43,563 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:43,564 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:43,566 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,567 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,567 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:43,569 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,572 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,576 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,576 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,576 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,576 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,576 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:43,580 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:43,581 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:43,583 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:43,589 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,590 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,590 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:43,592 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,595 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,599 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,599 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,599 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,600 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,600 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:43,603 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:43,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:43,607 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:43,609 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,610 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,610 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:43,611 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,615 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,619 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,619 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,619 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,619 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,619 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:43,624 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:43,626 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:43,628 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:43,631 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,632 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,632 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:43,634 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,637 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,641 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,641 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,641 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,642 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,642 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:43,645 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:43,647 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:43,649 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:43,651 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,652 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,652 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:43,654 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,658 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,661 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,661 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,662 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,662 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,662 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:43,665 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:43,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:43,669 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:43,671 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,671 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,671 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:43,673 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,677 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,681 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,681 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,681 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,681 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,681 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:43,684 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:43,687 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:43,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:43,690 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,691 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,691 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:43,693 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,696 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,700 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,700 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,700 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,701 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,701 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:43,704 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:43,708 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:43,710 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:43,712 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,713 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,713 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:43,714 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:43,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,722 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,722 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,722 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,722 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,722 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:43,729 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:43,731 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:43,733 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:43,734 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,735 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,735 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:43,737 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:43,741 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,744 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,744 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,744 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,745 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,745 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:43,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:43,750 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:43,751 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:43,753 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,754 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,754 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:43,755 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:43,759 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,763 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,763 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,763 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,763 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,764 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:43,767 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:43,769 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:43,771 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:43,773 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,773 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,773 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:43,775 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:43,779 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,779 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,779 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,780 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,780 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,780 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:43,783 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:43,785 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:43,787 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:43,789 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:41:43,789 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:41:43,789 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:43,791 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:43,791 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,792 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,792 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,792 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,792 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,792 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:43,792 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:43,792 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:43,793 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:43,793 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,793 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,793 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:43,793 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:43,794 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,794 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,794 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,795 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,795 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,795 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:43,805 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:43,813 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:43,821 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:43,830 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:43,831 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:43,831 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:43,836 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:43,836 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,837 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:43,837 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,837 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:43,837 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,838 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:43,838 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:43,839 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:43,839 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:43,839 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,840 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,840 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:43,841 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:43,841 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,845 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 24]), 23)
2023-10-09 05:41:43,845 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:43,846 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 24]), 23)
2023-10-09 05:41:43,846 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:43,846 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:43,846 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:43,846 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:43,847 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:43,847 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:43,847 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:43,847 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:43,848 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:43,851 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,855 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,855 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,855 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,855 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,856 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:43,860 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:43,863 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:43,866 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:43,870 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,870 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,870 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:43,872 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:43,876 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,879 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,880 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,880 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,880 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,880 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:43,883 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:43,885 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:43,887 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:43,889 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,889 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,890 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:43,891 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:43,895 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,898 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,898 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,899 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,899 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,899 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:43,907 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:43,909 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:43,911 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:43,913 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,914 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,914 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:43,915 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:43,919 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,923 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,923 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,923 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,923 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,923 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:43,927 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:43,929 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:43,930 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:43,932 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,933 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,933 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:43,934 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:43,938 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,942 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,942 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,942 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,942 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,942 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:43,945 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:43,948 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:43,951 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:43,953 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,953 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,953 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:43,955 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:43,958 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,962 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,962 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,962 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,962 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,962 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:43,966 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:43,968 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:43,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:43,972 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,972 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,972 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:43,973 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:43,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,981 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:43,982 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,982 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:43,982 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:43,982 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:43,985 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:43,987 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:43,989 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:43,991 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:43,991 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:43,991 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:43,993 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:43,996 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,000 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,000 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,000 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,000 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,001 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:44,011 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:44,013 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:44,015 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:44,017 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:44,018 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:44,018 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:44,019 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,023 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,027 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,027 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,027 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,027 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,027 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:44,031 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:44,032 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:44,034 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:44,036 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:44,037 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:44,037 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:44,038 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,042 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,046 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,046 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,046 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,046 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,046 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:44,050 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:44,051 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:44,053 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:44,055 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:44,056 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:44,056 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:44,057 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,061 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,064 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,065 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,065 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,065 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,065 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:44,068 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:44,070 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:44,072 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:44,073 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:44,074 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:44,074 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:44,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,079 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,079 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,080 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,080 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,080 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,080 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:44,084 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:44,086 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:44,088 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:44,089 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:41:44,090 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:41:44,090 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:44,091 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,092 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,092 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,092 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,092 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,092 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,092 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:44,093 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:44,093 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:44,093 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:44,093 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,093 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,094 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:44,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,094 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,095 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,095 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,095 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,095 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,095 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:44,103 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:44,110 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:44,117 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:44,125 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:44,125 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:44,125 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:44,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,130 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,131 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:44,131 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,131 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:44,131 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,131 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:44,132 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:44,132 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:44,132 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:44,132 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,132 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,132 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:44,133 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,133 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,138 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 25]), 24)
2023-10-09 05:41:44,139 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,139 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 25]), 24)
2023-10-09 05:41:44,139 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,139 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:44,139 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:44,140 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:44,140 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:44,140 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,141 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,141 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:44,141 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,149 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,150 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,150 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,150 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:44,153 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:44,155 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:44,157 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:44,158 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,159 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,159 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:44,161 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,164 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,168 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,168 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,169 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,169 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,169 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:44,172 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:44,174 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:44,175 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:44,177 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,178 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,178 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:44,180 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,183 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,187 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,187 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,188 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,188 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:44,191 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:44,192 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:44,194 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:44,196 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,196 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,196 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:44,198 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,201 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,204 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,205 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,205 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,205 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,205 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:44,208 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:44,210 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:44,212 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:44,213 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,214 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,214 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:44,215 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,219 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,223 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,223 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,223 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,223 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,223 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:44,226 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:44,228 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:44,230 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:44,232 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,232 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,232 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:44,234 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,237 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,241 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,241 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,241 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,242 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,242 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:44,245 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:44,247 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:44,248 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:44,250 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,251 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,251 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:44,252 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,256 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,260 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,260 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,260 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,260 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,260 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:44,263 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:44,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:44,267 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:44,269 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,269 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,269 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:44,271 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,274 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,278 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,278 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,278 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,278 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,279 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:44,282 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:44,283 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:44,285 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:44,287 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,287 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,287 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:44,288 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,292 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,296 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,296 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,296 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,296 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,296 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:44,300 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:44,302 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:44,303 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:44,305 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,306 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,306 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:44,308 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,311 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,315 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,315 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,315 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,315 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,316 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:44,321 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:44,323 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:44,325 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:44,327 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,327 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,327 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:44,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,332 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,337 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,337 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,337 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,337 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,337 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:44,341 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:44,343 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:44,345 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:44,346 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,347 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,347 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:44,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,352 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,352 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,353 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,353 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,353 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,353 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:44,356 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:44,358 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:44,359 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:44,361 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:41:44,362 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:41:44,362 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:44,363 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,364 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,364 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,364 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,365 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,365 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:44,365 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:44,365 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:44,365 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:44,366 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,366 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,366 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:44,366 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,367 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,367 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,367 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,367 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,367 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,367 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:44,375 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:44,383 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:44,391 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:44,397 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:44,398 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:44,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:44,403 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,403 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,404 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:44,404 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,404 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:44,404 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,404 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:44,404 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:44,405 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:44,405 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:44,405 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,405 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,405 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:44,406 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,406 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,410 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 26]), 25)
2023-10-09 05:41:44,410 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,410 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 26]), 25)
2023-10-09 05:41:44,410 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,410 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:44,410 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:44,411 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:44,411 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:44,411 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,411 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,411 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:44,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,415 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,419 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,419 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,419 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,419 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,419 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:44,423 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:44,425 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:44,427 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:44,429 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,430 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,430 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:44,432 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,435 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,439 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,439 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,439 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,439 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,439 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:44,443 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:44,445 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:44,446 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:44,448 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,449 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,449 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:44,450 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,454 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,457 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,458 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,458 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,458 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,458 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:44,462 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:44,464 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:44,465 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:44,467 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,468 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,468 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:44,469 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,473 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,476 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,476 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,477 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,477 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,477 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:44,480 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:44,481 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:44,483 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:44,485 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,485 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,486 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:44,487 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,494 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,494 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,495 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,495 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,495 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:44,498 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:44,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:44,502 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:44,504 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,504 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,504 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:44,506 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,509 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,513 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,513 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,513 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,513 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,514 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:44,517 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:44,518 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:44,520 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:44,522 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,523 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,523 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:44,524 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,528 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,531 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,531 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,532 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,532 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,532 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:44,535 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:44,537 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:44,539 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:44,540 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,541 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,541 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:44,542 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,546 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,550 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,550 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,550 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,550 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,550 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:44,554 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:44,556 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:44,557 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:44,559 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,559 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,559 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:44,561 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,564 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,568 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,568 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,568 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,568 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,568 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:44,573 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:44,575 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:44,576 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:44,579 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,580 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,580 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:44,582 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,585 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,590 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,590 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,590 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,590 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,590 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:44,594 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:44,597 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:44,598 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:44,600 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,600 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,601 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:44,602 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,606 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,609 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,609 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,610 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,610 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,610 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:44,613 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:44,615 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:44,617 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:44,619 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,619 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,619 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:44,621 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,625 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,625 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,625 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,626 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,626 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,626 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:44,629 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:44,631 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:44,633 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:44,635 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:41:44,635 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:41:44,635 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:44,636 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,637 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,638 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,638 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,638 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,638 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,638 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:44,638 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:44,638 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:44,639 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:44,639 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,639 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,639 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:44,639 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,640 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,640 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,640 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,641 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,641 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,641 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:44,650 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:44,659 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:44,666 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:44,674 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:44,675 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:44,675 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:44,680 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,680 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,681 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:44,681 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,681 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:44,681 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,681 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:44,682 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:44,682 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:44,682 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:44,683 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,683 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,683 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:44,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,688 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 27]), 26)
2023-10-09 05:41:44,688 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,688 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 27]), 26)
2023-10-09 05:41:44,688 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:44,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:44,689 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:44,690 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:44,690 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,690 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,690 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:44,691 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,694 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,697 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,698 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,698 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,698 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,698 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:44,701 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:44,704 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:44,705 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:44,708 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,708 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,708 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:44,710 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,714 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,717 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,718 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,718 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,718 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,718 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:44,724 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:44,726 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:44,727 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:44,729 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,729 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,730 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:44,731 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,734 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,738 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,738 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,738 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,738 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,739 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:44,742 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:44,744 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:44,745 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:44,747 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,748 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,748 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:44,749 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:44,753 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,756 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,756 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,757 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,757 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,757 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:44,761 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:44,763 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:44,764 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:44,766 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,767 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,767 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:44,769 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:44,772 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,776 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,776 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,776 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,776 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:44,780 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:44,782 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:44,784 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:44,786 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,787 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,787 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:44,788 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:44,792 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,795 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,795 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,796 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,796 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,796 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:44,799 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:44,801 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:44,803 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:44,804 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,805 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,805 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:44,807 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:44,810 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,814 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,814 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,815 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,815 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,815 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:44,818 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:44,821 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:44,823 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:44,825 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,826 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,826 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:44,827 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:44,831 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,835 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,835 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,835 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,835 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,835 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:44,839 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:44,841 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:44,842 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:44,844 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,845 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,845 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:44,846 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:44,849 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,853 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,853 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,854 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,854 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,854 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:44,857 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:44,858 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:44,861 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:44,862 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,863 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,863 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:44,864 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:44,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,872 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,872 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,872 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,872 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,872 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:44,876 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:44,878 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:44,879 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:44,881 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,882 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,882 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:44,883 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:44,887 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,892 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,893 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,893 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,893 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:44,896 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:44,899 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:44,901 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:44,902 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,903 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,903 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:44,905 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:44,908 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,909 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,909 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,909 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,909 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,909 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:44,912 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:44,914 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:44,916 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:44,918 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:41:44,918 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:41:44,918 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:44,920 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:44,920 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,921 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,921 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,921 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,921 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,921 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:44,922 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:44,922 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:44,922 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:44,923 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,923 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,923 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:44,923 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:44,924 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,924 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,924 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,924 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,925 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,925 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:44,932 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:44,940 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:44,947 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:44,954 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:44,956 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:44,956 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:44,962 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:44,963 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,963 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:44,963 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,963 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:44,964 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,964 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:44,964 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:44,964 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:44,964 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:44,964 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,965 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,965 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:44,965 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:44,966 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,969 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 28]), 27)
2023-10-09 05:41:44,969 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:44,970 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 28]), 27)
2023-10-09 05:41:44,970 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:44,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:44,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:44,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:44,971 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:44,971 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:44,971 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:44,971 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:44,971 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:44,975 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,978 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,979 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,979 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,979 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,979 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:44,982 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:44,984 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:44,987 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:44,988 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:44,989 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:44,989 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:44,991 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:44,994 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:44,998 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:44,998 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,998 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:44,998 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:44,998 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:45,002 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:45,004 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:45,005 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:45,007 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,008 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,008 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:45,009 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,013 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,016 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,017 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,017 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,017 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,017 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:45,021 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:45,023 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:45,025 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:45,027 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,027 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,027 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:45,029 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,032 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,036 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,036 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,036 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,036 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,036 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:45,039 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:45,042 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:45,044 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:45,046 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,046 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,047 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:45,048 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,052 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,056 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,056 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,056 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,056 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:45,059 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:45,061 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:45,063 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:45,065 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,065 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,065 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:45,067 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,070 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,074 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,074 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,074 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,074 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,074 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:45,077 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:45,080 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:45,082 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:45,084 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,085 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,085 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:45,087 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,090 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,094 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,095 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,095 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,095 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,095 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:45,099 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:45,103 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:45,105 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:45,107 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,108 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,108 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:45,110 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,113 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,116 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,117 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,117 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,117 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,117 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:45,120 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:45,122 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:45,125 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:45,127 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,127 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,127 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:45,129 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,132 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,136 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,136 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,136 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,136 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,136 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:45,140 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:45,142 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:45,145 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:45,149 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,149 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,150 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:45,151 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,155 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,158 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,159 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,159 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,159 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,159 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:45,162 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:45,164 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:45,166 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:45,167 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,168 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,168 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:45,169 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,173 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,177 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,177 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,177 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,177 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,177 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:45,180 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:45,182 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:45,184 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:45,186 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,186 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,187 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:45,188 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,192 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,192 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,192 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,193 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,193 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,193 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:45,196 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:45,198 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:45,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:45,202 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:41:45,203 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:41:45,204 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:45,205 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,206 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,206 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,206 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,206 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,206 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,206 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:45,207 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:45,207 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:45,207 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:45,207 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,207 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,208 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:45,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,208 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,209 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,209 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,209 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,209 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,209 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:45,218 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:45,225 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:45,231 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:45,246 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:45,250 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:45,250 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:45,255 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,256 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,256 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:45,256 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:45,257 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:45,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:45,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:45,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:45,258 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,258 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,259 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:45,259 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,260 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,264 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 29]), 28)
2023-10-09 05:41:45,264 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,264 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 29]), 28)
2023-10-09 05:41:45,264 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,264 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:45,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:45,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:45,265 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:45,266 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,266 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,266 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:45,266 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,270 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,273 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,273 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,274 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,274 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,274 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:45,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:45,279 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:45,281 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:45,283 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,284 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,284 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:45,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,289 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,293 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,293 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,293 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,293 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,293 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:45,297 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:45,299 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:45,301 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:45,303 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,304 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,304 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:45,306 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,309 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,313 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,313 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,313 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,313 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,313 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:45,317 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:45,319 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:45,321 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:45,323 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,324 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,324 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:45,326 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,329 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,334 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,334 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,334 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,334 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,334 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:45,338 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:45,341 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:45,343 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:45,346 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,346 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,346 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:45,348 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,352 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,356 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,356 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,356 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,356 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,356 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:45,368 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:45,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:45,373 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:45,376 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,377 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,377 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:45,379 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,383 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,387 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,387 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,387 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,387 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,387 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:45,390 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:45,393 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:45,396 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:45,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,398 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:45,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,408 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,408 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,408 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,408 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,408 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:45,412 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:45,413 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:45,415 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:45,417 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,417 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,418 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:45,419 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,423 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,427 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,427 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,427 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,427 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,427 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:45,436 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:45,438 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:45,441 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:45,443 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,444 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,444 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:45,445 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,449 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,453 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,453 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,453 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,454 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,454 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:45,460 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:45,462 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:45,464 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:45,466 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,467 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,467 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:45,469 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,472 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,476 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,476 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,476 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,476 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,476 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:45,480 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:45,482 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:45,485 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:45,487 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,488 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,488 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:45,491 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,497 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,503 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,503 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,503 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,504 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,504 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:45,514 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:45,519 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:45,524 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:45,528 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,528 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,529 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:45,531 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,535 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,536 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,536 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,536 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,537 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,537 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:45,541 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:45,544 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:45,546 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:45,549 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:41:45,550 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:41:45,550 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:45,552 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,553 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,553 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,554 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,554 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,554 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,554 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:45,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:45,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:45,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:45,556 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,556 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,556 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:45,556 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,557 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,558 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,558 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,558 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,558 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,558 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:45,569 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:45,578 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:45,587 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:45,596 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:45,597 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:45,597 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:45,603 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,604 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,604 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:45,604 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,604 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:45,605 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:45,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:45,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:45,606 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:45,606 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,606 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,606 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:45,607 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,607 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,611 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 30]), 29)
2023-10-09 05:41:45,612 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,612 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 30]), 29)
2023-10-09 05:41:45,612 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:45,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:45,613 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:45,613 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:45,614 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,614 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,614 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:45,614 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,618 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,622 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,622 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,623 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,623 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,623 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:45,627 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:45,630 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:45,633 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:45,635 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,636 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,636 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:45,638 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,642 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,646 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,646 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,646 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,647 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,647 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:45,651 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:45,653 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:45,655 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:45,658 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,659 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,659 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:45,661 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,665 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,669 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,669 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,669 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,669 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,669 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:45,673 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:45,676 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:45,679 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:45,682 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,683 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,683 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:45,685 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,689 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,693 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,693 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,693 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,693 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,693 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:45,697 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:45,700 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:45,702 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:45,706 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,706 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,706 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:45,708 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:45,712 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,716 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,716 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,717 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,717 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:45,720 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:45,723 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:45,725 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:45,728 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,728 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,728 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:45,730 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:45,734 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,738 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,738 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,738 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,738 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,738 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:45,742 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:45,745 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:45,747 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:45,750 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,750 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,750 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:45,752 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:45,756 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,760 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,760 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,760 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,760 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,760 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:45,764 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:45,767 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:45,769 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:45,771 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,772 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,772 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:45,773 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:45,777 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,781 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,781 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,782 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,782 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,782 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:45,785 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:45,787 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:45,789 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:45,791 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,791 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,792 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:45,793 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:45,797 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,802 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,802 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,802 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,802 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,802 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:45,806 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:45,808 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:45,810 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:45,812 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,812 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,812 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:45,814 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:45,818 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,822 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,822 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,822 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,822 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,823 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:45,826 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:45,828 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:45,831 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:45,833 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,834 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,834 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:45,835 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:45,839 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,843 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,843 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,843 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,844 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,844 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:45,848 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:45,850 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:45,852 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:45,855 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,855 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,856 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:45,857 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:45,861 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,862 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,862 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,862 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,862 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,862 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:45,866 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:45,869 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:45,871 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:45,873 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:41:45,873 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:41:45,874 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:45,875 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:45,876 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,876 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,876 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,876 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,876 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,877 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:45,877 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:45,877 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:45,877 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:45,877 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,878 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,878 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:45,878 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:45,879 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,879 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,879 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,879 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,880 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,880 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:45,891 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:45,901 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:45,911 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:45,921 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:45,922 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:45,922 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:45,927 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:45,928 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,928 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:45,928 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,929 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:45,929 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,929 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:45,929 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:45,929 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:45,930 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:45,930 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,930 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,930 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:45,931 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:45,931 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,935 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 31]), 30)
2023-10-09 05:41:45,935 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:45,936 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 31]), 30)
2023-10-09 05:41:45,936 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:45,936 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:45,936 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:45,936 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:45,937 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:45,937 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:45,937 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:45,937 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:45,938 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:45,941 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,945 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,946 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,946 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,946 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,946 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:45,950 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:45,953 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:45,956 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:45,958 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:45,959 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:45,959 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:45,961 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:45,965 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,969 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,969 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,970 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,970 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,970 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:45,974 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:45,978 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:45,980 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:45,983 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:45,984 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:45,984 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:45,986 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:45,990 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:45,994 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:45,994 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,994 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:45,994 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:45,995 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:45,999 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:46,001 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:46,004 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:46,006 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,007 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,007 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:46,009 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:46,013 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,017 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,017 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,017 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,017 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,018 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:46,022 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:46,025 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:46,027 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:46,029 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,030 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,030 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:46,032 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,036 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,040 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,040 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,040 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,041 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,041 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:46,044 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:46,047 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:46,050 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:46,052 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,054 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,054 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:46,056 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,060 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,064 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,064 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,064 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,064 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,065 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:46,069 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:46,071 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:46,073 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:46,076 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,076 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,076 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:46,078 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,081 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,086 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,086 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,086 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,086 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,086 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:46,090 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:46,094 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:46,096 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:46,098 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,099 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,099 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:46,101 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,104 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,108 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,109 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,109 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,109 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,109 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:46,113 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:46,116 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:46,118 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:46,120 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,121 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,121 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:46,122 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,126 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,130 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,131 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,131 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,131 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,131 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:46,135 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:46,137 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:46,140 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:46,143 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,143 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,144 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:46,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,149 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,153 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,153 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,154 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,154 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,154 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:46,160 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:46,163 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:46,166 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:46,170 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,171 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,172 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:46,175 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,181 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,188 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,188 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,188 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,188 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,189 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:46,193 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:46,196 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:46,199 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:46,201 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,202 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,202 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:46,205 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,212 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,212 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,212 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,212 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,212 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:46,215 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:46,218 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:46,221 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:46,223 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:41:46,224 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:41:46,224 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:46,225 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,226 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,226 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,226 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,227 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,227 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,227 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:46,227 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:46,227 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:46,227 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:46,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,228 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,228 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:46,228 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,229 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,229 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,229 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,229 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,229 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,230 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:46,241 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:46,249 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:46,259 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:46,269 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:46,269 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:46,270 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:46,274 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,275 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,276 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:46,276 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,276 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:46,276 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,276 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:46,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:46,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:46,277 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:46,277 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,278 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,278 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:46,278 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,279 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,283 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 32]), 31)
2023-10-09 05:41:46,283 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,283 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 32]), 31)
2023-10-09 05:41:46,283 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,284 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:46,284 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:46,284 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:46,285 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:46,285 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,285 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,285 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:46,286 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,289 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,292 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,293 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,293 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,293 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,293 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:46,297 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:46,300 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:46,302 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:46,305 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,306 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,306 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:46,308 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,311 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:46,315 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,315 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,315 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,315 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,315 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:46,318 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:46,320 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:46,322 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:46,324 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,324 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,324 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:46,326 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:46,330 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:46,333 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,333 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,334 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,334 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,334 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:46,337 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:46,339 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:46,341 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:46,343 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,344 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,344 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:46,346 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:46,349 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,353 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,353 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,353 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,354 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,354 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:46,357 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:46,361 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:46,364 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:46,366 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,367 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,367 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:46,368 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,372 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,376 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,377 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,377 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,377 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,377 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:46,381 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:46,383 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:46,385 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:46,386 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,387 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,387 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:46,389 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,395 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,399 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,400 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,400 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,400 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,400 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:46,403 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:46,406 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:46,408 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:46,410 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,410 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,411 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:46,412 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,415 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,420 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,420 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,420 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,420 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,421 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:46,425 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:46,429 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:46,431 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:46,433 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,434 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,434 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:46,436 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,441 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,446 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,446 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,447 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,447 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,447 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:46,450 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:46,454 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:46,457 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:46,460 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,460 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,461 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:46,462 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,466 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,471 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,472 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,472 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,472 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,472 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:46,475 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:46,477 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:46,479 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:46,482 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,483 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,483 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:46,486 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,491 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,496 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,496 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,496 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,496 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,496 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:46,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:46,504 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:46,506 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:46,509 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,509 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,509 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:46,511 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,515 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,521 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,522 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,522 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,522 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,522 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:46,526 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:46,529 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:46,531 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:46,534 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,534 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,534 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:46,536 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,539 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,540 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,540 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,540 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,540 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,541 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:46,543 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:46,546 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:46,549 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:46,551 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:41:46,552 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:41:46,552 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:46,554 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,555 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,556 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,556 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,556 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,556 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,556 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:46,556 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:46,556 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:46,557 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:46,557 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,557 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,557 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:46,558 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,558 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,558 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,558 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,559 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,559 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,559 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:46,568 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:46,576 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:46,585 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:46,595 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:46,596 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:46,596 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:46,600 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,601 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,601 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:46,601 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,602 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:46,602 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,602 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:46,602 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:46,603 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:46,603 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:46,604 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,604 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,604 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:46,605 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,605 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,611 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 33]), 32)
2023-10-09 05:41:46,611 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,611 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 33]), 32)
2023-10-09 05:41:46,611 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:46,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:46,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:46,613 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:46,613 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,613 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,613 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:46,614 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,618 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,622 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,622 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,622 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,622 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,623 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:46,626 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:46,628 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:46,630 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:46,632 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,633 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,633 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:46,634 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,638 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:46,644 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,644 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,645 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,645 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,645 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:46,649 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:46,651 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:46,653 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:46,656 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,657 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,657 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:46,659 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:46,662 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:46,666 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,666 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,666 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:46,670 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:46,674 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:46,678 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:46,681 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,682 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,682 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:46,684 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:46,689 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,692 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,693 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,693 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,693 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,693 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:46,696 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:46,698 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:46,700 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:46,702 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,703 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,703 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:46,706 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:46,711 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,715 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,715 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,715 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,716 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,716 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:46,720 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:46,722 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:46,725 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:46,727 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,728 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,728 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:46,731 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:46,737 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,742 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,742 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,742 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,742 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,743 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:46,746 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:46,748 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:46,750 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:46,753 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,753 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,754 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:46,756 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:46,759 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,765 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,766 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,766 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,766 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,766 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:46,770 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:46,774 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:46,777 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:46,779 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,780 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,780 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:46,783 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:46,786 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,790 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,790 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,790 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,790 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,790 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:46,793 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:46,796 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:46,799 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:46,802 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,803 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,803 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:46,805 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:46,808 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,813 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,813 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,814 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,814 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,814 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:46,817 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:46,819 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:46,821 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:46,823 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,823 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,823 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:46,826 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:46,832 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,837 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,837 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,837 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,838 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,838 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:46,841 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:46,845 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:46,847 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:46,849 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,850 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,850 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:46,851 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:46,854 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,859 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,860 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,860 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,860 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,860 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:46,865 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:46,868 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:46,871 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:46,874 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,875 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,875 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:46,878 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:46,884 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,885 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,885 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,885 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,885 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,885 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:46,889 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:46,892 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:46,896 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:46,899 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:41:46,900 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:41:46,900 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:46,903 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:46,904 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,904 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,904 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,905 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,905 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,905 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:46,905 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:46,906 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:46,906 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:46,906 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,907 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,907 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:46,907 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:46,908 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,908 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,908 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,909 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,909 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,909 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:46,919 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:46,927 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:46,936 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:46,944 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:46,945 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:46,945 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:46,949 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:46,950 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,951 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:46,951 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,951 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:46,951 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,951 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:46,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:46,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:46,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:46,953 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,953 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,953 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:46,954 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:46,955 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,960 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 34]), 33)
2023-10-09 05:41:46,960 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:46,960 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 34]), 33)
2023-10-09 05:41:46,960 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:46,960 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:46,961 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:46,961 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:46,961 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:46,962 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:46,962 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:46,962 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:46,962 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:46,966 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,971 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,972 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,972 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,972 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:46,976 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:46,979 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:46,981 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:46,983 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:46,984 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:46,984 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:46,987 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:46,991 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:46,996 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:46,997 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,997 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:46,997 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:46,997 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:47,001 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:47,004 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:47,007 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:47,010 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,011 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,011 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:47,014 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,020 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,024 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,024 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,025 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,025 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,025 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:47,028 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:47,030 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:47,033 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:47,036 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,037 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,037 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:47,040 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,046 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,052 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,052 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,053 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,053 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,053 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:47,058 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:47,061 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:47,063 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:47,066 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,067 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,067 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:47,069 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,073 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,078 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,078 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,078 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,078 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,079 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:47,082 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:47,086 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:47,088 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:47,090 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,090 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,090 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:47,093 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,097 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,101 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,101 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,101 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,101 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,102 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:47,105 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:47,107 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:47,110 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:47,112 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,113 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,113 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:47,116 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,120 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,125 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,125 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,126 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,126 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,126 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:47,130 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:47,133 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:47,135 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:47,136 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,137 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,137 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:47,139 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,145 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,149 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,150 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,150 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,150 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:47,153 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:47,156 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:47,158 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:47,160 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,160 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,160 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:47,161 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,165 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,169 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,169 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,170 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,170 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,170 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:47,174 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:47,177 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:47,180 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:47,182 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,183 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,183 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:47,185 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,190 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,193 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,193 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,194 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,194 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,194 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:47,197 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:47,199 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:47,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:47,203 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,209 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,209 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:47,211 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,215 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,219 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,219 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,219 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,219 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,220 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:47,223 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:47,225 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:47,227 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:47,228 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,229 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,229 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:47,231 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,234 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,235 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,235 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,235 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,236 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,236 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:47,239 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:47,241 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:47,243 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:47,245 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:41:47,246 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:41:47,246 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:47,247 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,248 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,248 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,248 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,248 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,249 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:47,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:47,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:47,249 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:47,249 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,250 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,250 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:47,250 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,250 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,251 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,251 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,251 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,251 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,251 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:47,260 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:47,269 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:47,277 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:47,285 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:47,286 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:47,286 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:47,291 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,291 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,292 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:47,292 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,292 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:47,292 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,292 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:47,292 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:47,293 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:47,293 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:47,293 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,293 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,293 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:47,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,294 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,298 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 35]), 34)
2023-10-09 05:41:47,299 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,299 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 35]), 34)
2023-10-09 05:41:47,299 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,299 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:47,299 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:47,300 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:47,300 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:47,300 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,300 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,300 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:47,301 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,304 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,308 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,309 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,309 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,309 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,309 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:47,320 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:47,323 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:47,326 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:47,329 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,332 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,332 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:47,334 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,338 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,342 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,342 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,342 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,342 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:47,346 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:47,348 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:47,350 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:47,352 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,353 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,353 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:47,355 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,359 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,363 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,363 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,363 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,363 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,363 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:47,366 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:47,370 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:47,372 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:47,374 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,375 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,375 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:47,377 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,380 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,384 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,384 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,384 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,384 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,385 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:47,388 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:47,390 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:47,392 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:47,395 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,395 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,395 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:47,397 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,400 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,404 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,404 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,404 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,404 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,404 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:47,408 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:47,410 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:47,412 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:47,413 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,414 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,414 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:47,416 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,419 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,423 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,423 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,423 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,423 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,423 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:47,426 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:47,428 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:47,431 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:47,432 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,433 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,433 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:47,434 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,437 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,442 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,442 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,442 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,442 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,442 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:47,445 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:47,447 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:47,449 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:47,451 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,452 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,452 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:47,453 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,457 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,460 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,460 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,460 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,461 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,461 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:47,464 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:47,466 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:47,469 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:47,471 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,471 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,471 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:47,473 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,476 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,480 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,480 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,480 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,480 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,480 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:47,484 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:47,486 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:47,489 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:47,491 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,492 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,492 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:47,493 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,497 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,500 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,501 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,501 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,501 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,501 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:47,505 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:47,507 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:47,509 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:47,511 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,511 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,511 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:47,513 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,516 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,520 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,520 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,520 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,520 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,520 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:47,524 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:47,526 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:47,528 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:47,530 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,531 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,531 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:47,533 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,536 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,536 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,537 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,537 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,537 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,537 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:47,545 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:47,547 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:47,549 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:47,552 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:41:47,552 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:41:47,552 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:47,553 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,554 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,555 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,555 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,555 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,555 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:47,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:47,555 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:47,556 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:47,556 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,556 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,556 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:47,556 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,557 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,557 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,557 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,557 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,557 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,558 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:47,568 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:47,578 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:47,587 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:47,596 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:47,597 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:47,597 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:47,602 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,603 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,603 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:47,603 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,603 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:47,604 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,604 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:47,604 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:47,604 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:47,605 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:47,605 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,605 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,605 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:47,606 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,606 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,610 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 36]), 35)
2023-10-09 05:41:47,610 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,610 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 36]), 35)
2023-10-09 05:41:47,611 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,611 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:47,611 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:47,611 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:47,612 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:47,612 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,612 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,612 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:47,613 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,616 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,620 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,620 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,620 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,620 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,620 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:47,624 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:47,626 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:47,629 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:47,631 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,632 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,632 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:47,634 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,637 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,641 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,641 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,641 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,641 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,642 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:47,646 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:47,648 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:47,650 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:47,653 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,653 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,653 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:47,655 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,658 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,662 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,662 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,662 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,662 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,662 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:47,667 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:47,669 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:47,671 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:47,675 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,675 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,675 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:47,677 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,680 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,684 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,684 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,684 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,684 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,684 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:47,688 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:47,691 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:47,692 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:47,694 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,695 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,695 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:47,697 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:47,700 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,704 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,704 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,704 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,705 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,705 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:47,709 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:47,712 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:47,714 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:47,716 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,717 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,717 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:47,718 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:47,722 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,725 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,725 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,725 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,726 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,726 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:47,732 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:47,735 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:47,738 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:47,741 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,742 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,742 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:47,744 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:47,747 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,751 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,751 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,751 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,751 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,751 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:47,755 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:47,757 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:47,760 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:47,762 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,763 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,763 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:47,765 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:47,768 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,772 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,772 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,772 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,772 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,772 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:47,776 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:47,779 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:47,781 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:47,783 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,784 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,784 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:47,785 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:47,789 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,792 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,793 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,793 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,793 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,793 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:47,797 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:47,800 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:47,802 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:47,804 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,805 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,805 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:47,807 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:47,810 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,814 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,814 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,814 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,814 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,815 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:47,819 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:47,821 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:47,823 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:47,826 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,827 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,827 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:47,828 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:47,831 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,835 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,836 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,836 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,836 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,836 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:47,840 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:47,842 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:47,844 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:47,847 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,847 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,847 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:47,849 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:47,852 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,853 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,853 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,853 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,853 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,853 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:47,857 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:47,859 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:47,861 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:47,864 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:41:47,864 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:41:47,864 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:47,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:47,866 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,867 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,867 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,867 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,867 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,867 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:47,867 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:47,867 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:47,868 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:47,868 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,868 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,868 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:47,868 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:47,869 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,869 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,869 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,869 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,870 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,870 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:47,881 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:47,891 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:47,899 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:47,907 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:47,909 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:47,909 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:47,913 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:47,914 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,914 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:47,914 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,915 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:47,915 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,915 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:47,915 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:47,916 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:47,916 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:47,916 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,916 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,917 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:47,917 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:47,918 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,923 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 37]), 36)
2023-10-09 05:41:47,923 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:47,924 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 37]), 36)
2023-10-09 05:41:47,924 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:47,924 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:47,924 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:47,924 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:47,925 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:47,925 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:47,925 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:47,925 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:47,926 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:47,929 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,932 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,933 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,933 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,933 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,933 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:47,937 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:47,939 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:47,940 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:47,942 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:47,943 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:47,943 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:47,944 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:47,948 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,951 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,952 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,952 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,952 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,952 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:47,956 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:47,959 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:47,962 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:47,965 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:47,966 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:47,967 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:47,970 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:47,977 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:47,982 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:47,982 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,982 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:47,983 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:47,983 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:47,987 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:47,990 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:47,992 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:47,995 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:47,996 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:47,996 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:47,999 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:48,004 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:48,010 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,010 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,011 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,011 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,011 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:48,015 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:48,018 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:48,021 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:48,024 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,025 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,025 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:48,028 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:48,033 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:48,038 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,038 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,038 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,038 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,038 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:48,042 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:48,044 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:48,046 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:48,048 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,049 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,049 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:48,051 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:48,054 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:48,058 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,058 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,058 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,058 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,058 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:48,063 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:48,066 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:48,068 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:48,070 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,071 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,071 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:48,072 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:48,076 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:48,079 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,080 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,080 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,080 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,080 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:48,084 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:48,086 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:48,088 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:48,091 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,091 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,091 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:48,093 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:48,096 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:48,100 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,100 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,100 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,100 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,101 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:48,104 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:48,107 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:48,109 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:48,112 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,112 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,112 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:48,114 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:48,117 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:48,121 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,121 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,121 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,121 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,121 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:48,125 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:48,127 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:48,129 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:48,131 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,132 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,132 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:48,134 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:48,137 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:48,141 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,141 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,141 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,141 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,141 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:48,145 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:48,147 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:48,149 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:48,152 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,153 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,153 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:48,155 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:48,158 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:48,162 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,162 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,162 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,163 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:48,167 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:48,169 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:48,171 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:48,174 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,175 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,175 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:48,177 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:48,180 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:48,180 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,181 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,181 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,181 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,181 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:48,189 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:48,191 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:48,193 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:48,195 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:41:48,196 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:41:48,196 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:48,198 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:48,198 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:48,199 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,199 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,199 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,199 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,199 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:48,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:48,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:48,200 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:48,200 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:48,200 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:48,201 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:48,201 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:48,201 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:48,202 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,202 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,202 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,202 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,202 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:48,213 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:48,224 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:48,233 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:48,242 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:48,243 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:48,243 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:48,248 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:48,249 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:48,249 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:41:48,249 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,249 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:41:48,250 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,250 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:41:48,250 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:41:48,250 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:41:48,251 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:41:48,251 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:48,251 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:48,251 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:41:48,252 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:41:48,253 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:48,256 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 38]), 37)
2023-10-09 05:41:48,256 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,257 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 38]), 37)
2023-10-09 05:41:48,257 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:41:48,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:41:48,257 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:41:48,258 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:41:48,258 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:48,258 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:48,258 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:41:48,259 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:41:48,262 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:48,265 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,266 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,266 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,266 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,266 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:41:48,269 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:41:48,272 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:41:48,274 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:41:48,277 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,277 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,278 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:41:48,279 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:41:48,283 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:48,286 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,286 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,287 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,287 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,287 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:41:48,290 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:41:48,292 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:41:48,294 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:41:48,296 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,297 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,297 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:41:48,299 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:41:48,302 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:48,306 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,306 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,306 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,306 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,306 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:41:48,310 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:41:48,312 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:41:48,314 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:41:48,317 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,318 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,318 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:41:48,319 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:41:48,323 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:48,326 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,327 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,327 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,327 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,327 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:41:48,331 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:41:48,333 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:41:48,335 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:41:48,338 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,339 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,339 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:41:48,341 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:41:48,344 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:48,348 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,348 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,348 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,348 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,349 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:41:48,352 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:41:48,354 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:41:48,356 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:41:48,358 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,359 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,359 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:41:48,360 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:41:48,364 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:48,367 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,368 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,368 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,368 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,368 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:41:48,371 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:41:48,374 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:41:48,377 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:41:48,379 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,379 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,379 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:41:48,381 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:41:48,384 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:48,388 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,388 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,388 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,388 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,389 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:41:48,392 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:41:48,394 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:41:48,396 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:41:48,398 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,399 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,399 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:41:48,401 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:41:48,404 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:48,408 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,408 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,408 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,408 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,408 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:41:48,411 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:41:48,414 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:41:48,417 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:41:48,419 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,420 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,420 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:41:48,421 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:41:48,425 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:48,428 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,429 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,429 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,429 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,429 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:41:48,432 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:41:48,435 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:41:48,439 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:41:48,442 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,443 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,443 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:41:48,444 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:41:48,448 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:48,452 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,452 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,452 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,452 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,452 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:41:48,457 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:41:48,459 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:41:48,462 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:41:48,464 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,465 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,465 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:41:48,467 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:41:48,470 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:48,474 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,474 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,474 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,474 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,475 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:41:48,479 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:41:48,481 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:41:48,482 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:41:48,484 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,485 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,485 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:41:48,487 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:41:48,490 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:48,491 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,491 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,491 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,492 [forward.py:90 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:41:48,492 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:41:48,495 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:41:48,498 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:41:48,500 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:41:48,502 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:41:48,503 [forward.py:110 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:41:48,503 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:41:48,505 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:41:48,505 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:48,506 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,506 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,506 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,506 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,506 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:41:48,509 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:41:48,509 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:41:48,510 [forward.py:96 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:41:48,510 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:41:48,510 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:41:48,511 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:41:48,511 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:41:48,512 [model.py:247 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:41:48,512 [forward.py:82 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:41:48,513 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:41:48,513 [forward.py:89 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:41:48,513 [forward.py:90 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:41:48,513 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:41:48,524 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:41:48,533 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:41:48,542 [forward.py:96 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:41:48,551 [forward.py:108 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:41:48,552 [forward.py:110 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:41:48,552 [model.py:257 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:41:48,559 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-09 05:41:48,559 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,559 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-09 05:41:48,559 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,559 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-09 05:41:48,559 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,559 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-09 05:41:48,560 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,560 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-09 05:41:48,560 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,560 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-09 05:41:48,560 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,560 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-09 05:41:48,560 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,560 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-09 05:41:48,560 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:41:48,569 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-09 05:41:48,570 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-09 05:41:48,571 [forward.py:21 in reset_forward] DEBUG - lm_head from flexgen to old.
