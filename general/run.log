2023-10-31 12:39:00,574 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp7aq5kcf9
2023-10-31 12:39:00,574 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp7aq5kcf9/_remote_module_non_scriptable.py
2023-10-31 12:39:01,045 [connectionpool.py:957 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 12:39:01,274 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 12:39:01,542 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 12:39:01,609 [model.py:132 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 12:39:01,609 [model.py:71 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 12:39:01,744 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 12:39:01,812 [model.py:81 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 12:39:01,817 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 12:39:01,818 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-31 12:39:01,818 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-31 12:39:01,819 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-31 12:39:01,820 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-31 12:39:01,821 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-31 12:39:01,822 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-31 12:39:01,823 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-31 12:39:01,824 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-31 12:39:01,824 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-31 12:39:01,825 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-31 12:39:01,826 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-31 12:39:01,827 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-31 12:39:01,828 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-31 12:39:01,829 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 12:39:01,829 [model.py:263 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 12:39:01,829 [model.py:269 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 12:39:01,831 [model.py:305 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-31 12:39:01,968 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 12:39:02,277 [model.py:395 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 12:39:02,278 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 12:39:02,279 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 12:39:02,280 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 12:39:02,280 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 12:39:02,280 [model.py:395 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 12:39:02,283 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:02,285 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 12:39:02,286 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:02,286 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 12:39:02,287 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:02,294 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 12:39:02,296 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:02,302 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 12:39:02,304 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:02,310 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 12:39:02,312 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:02,317 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 12:39:02,320 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:02,325 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 12:39:02,327 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:02,332 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 12:39:02,335 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:02,340 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 12:39:02,342 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:02,347 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 12:39:02,350 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:02,355 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 12:39:02,357 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:02,362 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 12:39:02,364 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:02,370 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 12:39:02,372 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:02,377 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 12:39:02,379 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:02,380 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 12:39:02,380 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:02,393 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 12:39:02,397 [model.py:403 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 12:39:02,398 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 12:39:02,399 [model.py:403 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 12:39:02,409 [model.py:518 in init_all_weights] DEBUG - init all weights...
2023-10-31 12:39:03,854 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 12:39:03,855 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 12:39:03,855 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 12:39:03,855 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 12:39:03,855 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 12:39:03,855 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 12:39:03,856 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 12:39:03,857 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 12:39:03,857 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 12:39:03,857 [flexgen.py:250 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 12:39:03,857 [flexgen.py:250 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 12:39:03,996 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 12:39:04,165 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:04,165 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 9), dtype=torch.int64)',)
2023-10-31 12:39:04,165 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,166 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:04,166 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:04,167 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:04,167 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 9), dtype=torch.int64)',)
2023-10-31 12:39:04,168 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,168 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9), dtype=torch.int64)',), {})
2023-10-31 12:39:04,169 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,169 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,170 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,170 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,171 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,171 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,172 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,172 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)


2023-10-31 12:39:04,172 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,172 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:04,173 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 9), dtype=torch.int64)', '0')
2023-10-31 12:39:04,173 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,173 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:04,173 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:04,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:04,176 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 9), dtype=torch.int64)', '0')
2023-10-31 12:39:04,176 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,177 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9), dtype=torch.int64)', '0'), {})
2023-10-31 12:39:04,177 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,178 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,178 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,179 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,179 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,180 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,180 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,180 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)


2023-10-31 12:39:04,180 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,182 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:04,183 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,183 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,183 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:04,183 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:04,186 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:04,188 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,188 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,189 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,243 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,245 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,249 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,250 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,288 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,289 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,294 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,294 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,294 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,295 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:04,295 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,295 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,295 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:04,297 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:04,299 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:04,302 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,302 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,303 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,309 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,314 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,316 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,319 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,325 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,326 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,326 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,326 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:04,326 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,326 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,326 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:04,328 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:04,331 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:04,333 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,333 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,334 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,341 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,345 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,347 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,351 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,352 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,356 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,357 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,357 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,357 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:04,357 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,357 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,357 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:04,359 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:04,361 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:04,364 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,364 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,365 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,370 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,372 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,377 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,379 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,384 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,386 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,391 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,392 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,392 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,392 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:04,392 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,392 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,393 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:04,394 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:04,397 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:04,399 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,400 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,400 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,407 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,409 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,413 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,415 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,420 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,421 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,426 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,427 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,427 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,427 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:04,427 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,428 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,428 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:04,429 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:04,432 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:04,435 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,435 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,436 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,442 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,444 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,448 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,450 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,455 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,456 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,461 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,462 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,462 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,462 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:04,462 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,463 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,463 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:04,464 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:04,467 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:04,469 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,469 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,470 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,476 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,478 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,483 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,484 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,489 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,496 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,496 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,496 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,497 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:04,497 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,497 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,497 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:04,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:04,501 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:04,504 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,504 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,505 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,511 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,524 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,526 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,531 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,531 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,531 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,531 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:04,532 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,532 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,532 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:04,533 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:04,536 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:04,540 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,540 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,541 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,547 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,549 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,554 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,555 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,560 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,562 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,567 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,567 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,568 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,568 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:04,568 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,568 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,568 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:04,570 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:04,572 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:04,575 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,575 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,576 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,582 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,584 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,590 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,595 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,597 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,602 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,602 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,603 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,603 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:04,603 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,603 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,603 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:04,604 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:04,607 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:04,610 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,610 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,611 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,617 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,619 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,623 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,625 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,630 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,631 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,636 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,637 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,637 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,637 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:04,638 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,638 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,638 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:04,639 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:04,640 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:04,643 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,643 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,644 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,650 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,651 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,656 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,663 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,669 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,670 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,670 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,670 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:04,670 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,671 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,671 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:04,672 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:04,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:04,673 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,673 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,674 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:04,675 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,675 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,676 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,676 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,677 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 9, 768), dtype=torch.float32)
2023-10-31 12:39:04,678 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,678 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)


2023-10-31 12:39:04,678 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,679 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:04,679 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,679 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,679 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:04,679 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:04,680 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:04,680 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,680 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,681 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 9, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:04,710 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,711 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 9, 50272), dtype=torch.float32)
2023-10-31 12:39:04,734 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,736 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 9, 50272), dtype=torch.float32)
2023-10-31 12:39:04,759 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,760 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 9, 50272), dtype=torch.float32)
2023-10-31 12:39:04,784 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,788 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 9, 50272), dtype=torch.float32)


2023-10-31 12:39:04,788 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,789 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:04,789 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:04,790 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,790 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:04,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:04,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:04,795 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:04,795 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,796 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:04,796 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,797 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,797 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,798 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,798 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,799 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,799 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,800 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:04,800 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,800 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:04,800 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 10), dtype=torch.int64)', '9')
2023-10-31 12:39:04,800 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:04,801 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:04,801 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:04,804 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:04,804 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 10), dtype=torch.int64)', '9')
2023-10-31 12:39:04,804 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:04,805 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 10), dtype=torch.int64)', '9'), {})
2023-10-31 12:39:04,806 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,806 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,807 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,807 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,808 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,808 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:04,809 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,809 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:04,809 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,811 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:04,811 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,811 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,811 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:04,812 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:04,815 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:04,817 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,817 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,819 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,832 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,833 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,836 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,838 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,841 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,843 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,845 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,846 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,846 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,846 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:04,846 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,847 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,847 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:04,848 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:04,851 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:04,854 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,854 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,856 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,860 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,861 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,864 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,866 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,869 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,871 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,874 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,874 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,874 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,874 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:04,875 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,875 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,875 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:04,877 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:04,880 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:04,882 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,882 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,884 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,888 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,889 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,892 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,894 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,897 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,898 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,901 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,901 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,901 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,902 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:04,902 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,902 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,902 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:04,904 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:04,906 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:04,909 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,909 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,911 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,914 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,915 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,918 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,922 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,926 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,927 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,927 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,927 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:04,927 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,927 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,928 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:04,929 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:04,932 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:04,935 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,935 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,937 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,940 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,942 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,944 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,946 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,950 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,952 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,953 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,953 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,953 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:04,953 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,953 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,954 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:04,955 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:04,958 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:04,961 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,961 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,962 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,965 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,967 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,970 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,971 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,974 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,978 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,978 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:04,978 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:04,979 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:04,979 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,979 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,979 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:04,980 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:04,983 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:04,986 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:04,986 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:04,988 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:04,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,992 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,995 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:04,996 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:04,999 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,001 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,003 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,003 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,003 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,004 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:05,004 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,004 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,004 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:05,005 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:05,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:05,011 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,011 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,013 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,016 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,017 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,020 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,021 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,024 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,026 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,028 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,028 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,029 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,029 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:05,029 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,029 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,029 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:05,030 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:05,036 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:05,039 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,039 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,040 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,044 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,048 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,049 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,052 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,053 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,056 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,056 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,056 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,056 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:05,057 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,057 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,057 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:05,058 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:05,061 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:05,064 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,064 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,066 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,069 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,073 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,075 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,078 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,079 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,082 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,082 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,082 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,082 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:05,083 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,083 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,083 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:05,084 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:05,087 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:05,090 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,090 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,092 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,095 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,097 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,100 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,104 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,108 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,109 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,109 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,109 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:05,109 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,109 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,110 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:05,111 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:05,112 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:05,114 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,114 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,116 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,119 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,121 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,124 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,125 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,128 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,130 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,132 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,133 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,133 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,133 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:05,133 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,133 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:05,133 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:05,135 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:05,135 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:05,136 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,136 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:05,136 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:05,137 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,138 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,138 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,139 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,139 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,140 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,140 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,141 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:05,141 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,141 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:05,141 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,141 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:05,141 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:05,142 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:05,142 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:05,143 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,143 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:05,143 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:05,153 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:05,160 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:05,167 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,168 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:05,175 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,176 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:05,176 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,177 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:05,177 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:05,177 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:05,177 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:05,181 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:05,182 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:05,182 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:05,182 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:05,183 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:05,183 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,184 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,184 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,185 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,185 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,186 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,186 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,187 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:05,187 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,187 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:05,187 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 11), dtype=torch.int64)', '10')
2023-10-31 12:39:05,187 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:05,187 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:05,188 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:05,191 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:05,191 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 11), dtype=torch.int64)', '10')
2023-10-31 12:39:05,191 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:05,192 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 11), dtype=torch.int64)', '10'), {})
2023-10-31 12:39:05,192 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,193 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,193 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,194 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,195 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,195 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:05,196 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,196 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:05,196 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,197 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:05,198 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,198 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,198 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:05,198 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:05,201 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:05,204 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,204 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,206 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,210 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,211 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,214 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,219 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,221 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,223 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,224 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,224 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,224 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:05,224 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,224 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,225 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:05,226 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:05,229 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:05,232 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,232 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,234 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,237 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,239 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,242 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,244 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,247 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,249 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,252 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,252 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,252 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:05,253 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,253 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,253 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:05,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:05,258 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:05,260 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,260 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,262 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,266 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,268 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,271 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,272 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,275 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,277 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,280 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,280 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,280 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,280 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:05,281 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,281 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,281 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:05,282 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:05,285 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:05,288 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,288 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,290 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,293 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,295 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,298 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,299 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,302 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,304 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,306 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,307 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,307 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,307 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:05,307 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,307 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,307 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:05,309 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:05,312 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:05,315 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,315 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,317 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,320 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,322 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,324 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,326 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,329 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,331 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,333 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,333 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,333 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,334 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:05,334 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,334 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,334 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:05,335 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:05,339 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:05,341 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,342 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,344 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,347 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,349 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,351 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,356 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,358 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,360 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,360 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,361 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,361 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:05,361 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,361 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,361 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:05,362 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:05,366 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:05,368 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,368 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,370 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,374 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,375 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,378 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,380 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,383 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,384 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,387 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,387 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,387 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,387 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:05,387 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,387 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,388 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:05,389 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:05,392 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:05,394 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,395 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,397 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,400 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,404 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,406 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,409 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,410 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,414 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,415 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,415 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,415 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:05,415 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,415 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,415 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:05,416 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:05,420 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:05,422 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:05,423 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:05,425 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:05,428 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,430 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,432 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,434 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,436 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,438 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:05,441 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:05,441 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:05,441 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:05,441 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:06,797 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,797 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,797 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:06,799 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:06,802 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:06,805 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,805 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,807 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:06,811 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,813 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,816 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,817 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,820 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,822 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,824 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,824 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:06,825 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,825 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:06,825 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,825 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,825 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:06,826 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:06,830 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:06,832 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,832 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,834 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:06,838 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,840 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,842 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,844 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,846 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,848 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,852 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,852 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:06,852 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,852 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:06,852 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,853 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,853 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:06,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:06,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:06,859 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,859 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,861 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:06,865 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,867 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,870 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,871 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,874 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,876 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,879 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,879 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'))


2023-10-31 12:39:06,879 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,879 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:06,879 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,879 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:06,880 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:06,881 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:06,881 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:06,882 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,882 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:06,883 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:06,883 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,884 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,885 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,885 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,886 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,886 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,887 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,887 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:06,887 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,888 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:06,888 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,888 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:06,888 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:06,888 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:06,889 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:06,889 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,889 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:06,890 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:06,899 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,900 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:06,907 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,908 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:06,914 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,915 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:06,921 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,922 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:06,922 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,923 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:06,923 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:06,923 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:06,923 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:06,927 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:06,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:06,928 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:06,928 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:06,929 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:06,929 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,930 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,930 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,931 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,932 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,932 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:06,932 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,933 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:06,933 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 12), dtype=torch.int64)', '11')
2023-10-31 12:39:06,933 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:06,933 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:06,933 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:06,936 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:06,937 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 12), dtype=torch.int64)', '11')
2023-10-31 12:39:06,937 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:06,937 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 12), dtype=torch.int64)', '11'), {})
2023-10-31 12:39:06,938 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,938 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,939 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,939 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,940 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,940 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:06,940 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,941 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:06,941 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,942 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:06,942 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,942 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,942 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:06,943 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:06,946 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:06,948 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,948 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,950 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:06,953 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,955 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,958 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,959 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,962 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,963 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,966 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,966 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:06,966 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,966 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:06,966 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,966 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,967 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:06,968 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:06,971 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:06,973 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,973 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,975 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:06,979 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,980 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,983 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,984 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,987 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,988 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:06,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:06,991 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:06,992 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:06,992 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:06,992 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,992 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:06,992 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:06,994 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:06,996 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:06,999 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:06,999 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,000 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,004 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,005 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,008 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,010 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,012 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,014 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,016 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,017 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,017 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,017 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:07,017 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,017 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,017 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:07,019 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:07,021 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:07,024 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,024 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,026 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,029 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,030 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,033 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,034 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,037 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,038 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,041 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,041 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,041 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,041 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:07,041 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,041 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,041 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:07,043 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:07,046 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:07,049 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,049 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,051 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,054 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,055 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,058 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,059 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,062 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,063 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,066 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,067 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,067 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,067 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:07,067 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,067 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,067 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:07,068 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:07,071 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:07,074 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,074 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,076 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,079 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,081 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,084 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,086 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,088 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,090 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,092 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,093 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,093 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,093 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:07,093 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,093 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,093 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:07,094 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:07,098 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:07,100 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,100 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,102 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,105 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,107 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,110 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,111 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,114 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,118 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,118 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,119 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,119 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:07,119 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,119 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,119 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:07,120 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:07,123 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:07,126 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,126 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,128 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,131 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,133 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,136 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,137 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,141 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,143 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,145 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,146 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,146 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,146 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:07,146 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,146 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,146 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:07,147 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:07,151 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:07,153 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,153 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,155 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,160 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,162 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,164 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,166 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,169 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,171 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,174 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,174 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,174 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,175 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:07,175 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,175 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,175 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:07,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:07,179 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:07,182 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,182 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,184 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,188 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,190 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,192 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,195 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,198 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,200 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,202 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,203 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,203 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,203 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:07,203 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,203 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,203 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:07,205 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:07,208 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:07,210 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,210 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,212 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,216 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,218 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,221 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,223 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,226 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,228 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,231 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,231 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,231 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,232 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:07,232 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,232 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,232 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:07,233 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:07,234 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:07,236 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,237 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,239 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,242 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,244 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,247 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,249 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,254 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,257 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,257 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,257 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,257 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:07,257 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,257 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:07,258 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:07,259 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:07,259 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:07,260 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,260 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:07,260 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:07,261 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,262 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,262 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,263 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,263 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,264 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,264 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,264 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:07,264 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,265 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:07,265 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,265 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:07,265 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:07,265 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:07,266 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:07,266 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,266 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:07,267 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:07,277 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,278 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:07,285 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,286 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:07,292 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,293 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:07,300 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,301 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:07,301 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,302 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:07,302 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:07,302 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:07,302 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:07,306 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:07,307 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:07,307 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:07,307 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:07,308 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:07,308 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,309 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,309 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,310 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,311 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,311 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,311 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:07,311 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,312 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:07,312 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 13), dtype=torch.int64)', '12')
2023-10-31 12:39:07,312 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:07,312 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:07,312 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:07,315 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:07,316 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 13), dtype=torch.int64)', '12')
2023-10-31 12:39:07,316 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:07,316 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 13), dtype=torch.int64)', '12'), {})
2023-10-31 12:39:07,317 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,317 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,318 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,318 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,319 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,320 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:07,320 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,320 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:07,320 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,322 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:07,322 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,322 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,322 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:07,322 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:07,325 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:07,328 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,328 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,330 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,334 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,336 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,339 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,341 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,344 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,346 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,349 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,349 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,350 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,350 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:07,350 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,350 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,350 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:07,352 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:07,355 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:07,357 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,357 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,359 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,363 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,365 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,368 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,370 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,373 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,375 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,378 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,379 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,379 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,379 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:07,379 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,379 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,379 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:07,381 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:07,384 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:07,386 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,387 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,389 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,393 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,398 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,400 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,403 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,405 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,408 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,408 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,408 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,408 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:07,409 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,409 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,409 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:07,410 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:07,413 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:07,416 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,416 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,418 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,422 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,424 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,427 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,429 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,431 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,433 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,436 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,437 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,437 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,437 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:07,437 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,437 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,437 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:07,439 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:07,442 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:07,444 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,445 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,447 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,450 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,452 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,455 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,457 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,460 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,462 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,464 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,465 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,465 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,465 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:07,465 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,465 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,466 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:07,467 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:07,470 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:07,472 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,473 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,475 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,479 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,481 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,483 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,485 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,488 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,490 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,493 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,493 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,493 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,494 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:07,494 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,494 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,494 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:07,495 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:07,498 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:07,501 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,501 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,503 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,506 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,508 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,511 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,516 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,518 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,520 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,521 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,521 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,521 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:07,521 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,521 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,522 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:07,523 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:07,526 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:07,528 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,529 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,530 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:07,534 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,536 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,539 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,541 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,544 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,546 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:07,549 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:07,549 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:07,549 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:07,549 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:07,550 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,550 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:07,550 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:07,551 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:07,554 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:07,557 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:07,557 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,096 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,100 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,104 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,109 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,111 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,114 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,115 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,115 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,115 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:09,115 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,115 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,115 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:09,117 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:09,120 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:09,122 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,123 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,125 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,128 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,130 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,133 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,135 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,141 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,143 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,145 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,146 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,146 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,146 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:09,146 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,146 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,146 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:09,148 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:09,151 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:09,153 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,154 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,156 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,159 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,164 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,166 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,169 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,171 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,174 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,174 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,174 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,174 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:09,175 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,175 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,175 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:09,177 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:09,177 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:09,180 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,180 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,182 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,186 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,187 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,190 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,192 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,195 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,197 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,200 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,200 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,200 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,201 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:09,201 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,201 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,201 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:09,202 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:09,203 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:09,203 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,203 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,204 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:09,204 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,205 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,205 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,206 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,206 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,207 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,208 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,208 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,208 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,208 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:09,208 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,208 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,208 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:09,209 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:09,209 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:09,210 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,210 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,210 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:09,220 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,221 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,228 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,229 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,235 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,236 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,243 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,244 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:09,244 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,245 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:09,245 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:09,245 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,245 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:09,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:09,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:09,250 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:09,251 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,251 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:09,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,252 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,253 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,253 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,254 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,254 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,255 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,255 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,255 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:09,255 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 14), dtype=torch.int64)', '13')
2023-10-31 12:39:09,255 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,255 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:09,256 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:09,259 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:09,259 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 14), dtype=torch.int64)', '13')
2023-10-31 12:39:09,259 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,260 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 14), dtype=torch.int64)', '13'), {})
2023-10-31 12:39:09,260 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,261 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,261 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,262 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,262 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,263 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,263 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,263 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,264 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,265 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:09,265 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,265 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,266 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:09,266 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:09,269 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:09,271 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,272 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,274 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,278 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,279 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,283 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,285 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,288 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,290 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,292 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,293 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,293 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,293 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:09,293 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,293 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,294 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:09,295 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:09,298 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:09,301 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,301 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,303 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,307 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,309 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,312 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,314 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,317 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,319 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,322 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,322 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,322 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,322 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:09,322 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,323 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,323 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:09,324 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:09,327 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:09,330 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,330 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,332 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,336 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,338 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,341 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,343 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,346 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,348 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,351 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,351 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,351 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,352 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:09,352 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,352 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,352 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:09,355 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:09,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:09,360 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,360 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,362 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,366 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,368 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,371 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,373 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,376 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,378 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,381 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,381 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,381 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,381 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:09,382 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,382 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,382 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:09,383 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:09,387 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:09,389 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,389 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,391 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,395 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,397 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,400 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,405 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,406 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,409 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,410 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,410 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,410 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:09,410 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,410 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,410 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:09,412 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:09,415 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:09,417 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,418 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,420 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,423 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,425 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,428 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,430 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,433 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,435 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,438 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,438 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,438 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,438 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:09,438 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,438 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,439 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:09,440 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:09,443 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:09,446 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,446 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,448 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,451 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,453 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,456 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,458 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,461 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,463 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,466 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,466 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,466 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,466 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:09,466 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,467 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,467 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:09,468 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:09,471 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:09,474 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,474 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,476 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,479 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,481 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,484 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,486 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,489 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,494 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,494 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,495 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,495 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:09,495 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,495 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,495 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:09,496 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:09,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:09,502 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,502 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,504 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,508 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,510 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,512 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,514 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,522 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,522 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,523 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,523 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:09,523 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,523 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,523 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:09,525 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:09,527 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:09,530 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,530 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,532 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,536 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,538 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,541 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,542 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,545 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,547 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,550 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,550 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,551 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,551 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:09,551 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,551 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,551 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:09,552 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:09,555 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:09,558 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,558 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,560 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,564 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,566 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,568 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,570 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,575 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,578 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,578 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,579 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,579 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:09,579 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,579 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,579 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:09,581 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:09,581 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:09,584 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,584 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,586 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,591 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,594 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,596 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,599 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,601 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,604 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,604 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,604 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,605 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:09,605 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,605 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,605 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:09,606 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:09,607 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:09,607 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,607 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,608 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:09,608 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,609 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,609 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,610 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,610 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,611 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,611 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,612 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,612 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,612 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:09,612 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,612 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,612 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:09,612 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:09,613 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:09,613 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,613 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,614 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:09,625 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,626 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,633 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,634 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,640 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,641 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:09,648 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,649 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:09,649 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,650 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:09,650 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:09,650 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,650 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:09,655 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:09,655 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:09,655 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:09,656 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,656 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:09,656 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,657 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,657 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,658 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,659 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,659 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,659 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,660 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,660 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:09,660 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 15), dtype=torch.int64)', '14')
2023-10-31 12:39:09,660 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:09,660 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:09,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:09,663 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:09,664 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 15), dtype=torch.int64)', '14')
2023-10-31 12:39:09,664 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:09,664 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 15), dtype=torch.int64)', '14'), {})
2023-10-31 12:39:09,665 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,665 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,666 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,666 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,667 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,667 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:09,668 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,668 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:09,668 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,669 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:09,670 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,670 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,670 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:09,670 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:09,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:09,675 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,676 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,678 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,682 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,684 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,687 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,689 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,692 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,694 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,697 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,697 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,697 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,697 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:09,698 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,698 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,698 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:09,699 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:09,702 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:09,705 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,705 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,707 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,711 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,713 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,716 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,718 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,721 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,723 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,726 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,726 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,726 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,727 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:09,727 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,727 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,727 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:09,729 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:09,731 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:09,734 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,734 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,736 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,740 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,742 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,745 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,747 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,751 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,753 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,756 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,756 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,756 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,757 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:09,757 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,757 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,757 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:09,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:09,761 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:09,764 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,764 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,766 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,770 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,772 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,775 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,777 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,779 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,784 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,784 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,785 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,785 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:09,785 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,785 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,785 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:09,787 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:09,790 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:09,792 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,792 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,794 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,798 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,800 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,803 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,805 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,808 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,810 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,812 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,813 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,813 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,813 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:09,813 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,813 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,813 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:09,815 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:09,818 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:09,820 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,820 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,822 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,826 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,828 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,831 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,833 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,836 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,838 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,840 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,841 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,841 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,841 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:09,841 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,841 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,841 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:09,843 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:09,846 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:09,848 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,848 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,850 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,854 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,856 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,859 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,861 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,864 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,866 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,868 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,869 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,869 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,869 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:09,869 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,869 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,869 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:09,871 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:09,874 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:09,876 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,876 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,879 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:09,882 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,884 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,887 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,889 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,892 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,894 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:09,897 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:09,897 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:09,897 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:09,897 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:09,898 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,898 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:09,898 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:09,899 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:09,903 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:09,905 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:09,905 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,522 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,526 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,527 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,530 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,532 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,536 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,538 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,541 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,541 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,541 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,541 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:11,541 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,542 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,542 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:11,543 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:11,546 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:11,549 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,549 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,551 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,555 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,557 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,560 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,562 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,565 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,567 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,570 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,570 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,570 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,570 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:11,570 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,571 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,571 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:11,572 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:11,575 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:11,578 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,578 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,580 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,584 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,586 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,591 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,593 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,595 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,598 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,599 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,599 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,599 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:11,599 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,599 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,599 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:11,601 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:11,601 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:11,604 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,604 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,606 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,610 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,612 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,615 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,617 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,620 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,622 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,624 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,625 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,625 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,625 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:11,625 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,625 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:11,625 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:11,627 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:11,627 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:11,627 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,628 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:11,628 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:11,629 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,629 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,630 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,630 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,631 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,632 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,632 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,632 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:11,632 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,633 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:11,633 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,633 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:11,633 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:11,633 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:11,633 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:11,634 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,634 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:11,635 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:11,644 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,645 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:11,652 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:11,659 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,660 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:11,667 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,668 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:11,668 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,669 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:11,669 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:11,669 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:11,670 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:11,674 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:11,674 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:11,675 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:11,675 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:11,675 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:11,676 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,676 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,676 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,677 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,677 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,678 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,679 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:11,679 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,679 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:11,679 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 16), dtype=torch.int64)', '15')
2023-10-31 12:39:11,679 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:11,679 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:11,679 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:11,682 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:11,683 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 16), dtype=torch.int64)', '15')
2023-10-31 12:39:11,683 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:11,683 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 16), dtype=torch.int64)', '15'), {})
2023-10-31 12:39:11,684 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,684 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,685 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,685 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,686 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,686 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:11,687 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,687 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:11,687 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,689 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:11,689 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,689 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,689 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:11,689 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:11,692 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:11,695 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,695 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,697 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,701 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,703 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,706 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,708 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,711 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,713 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,716 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,717 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,717 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,717 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:11,717 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,717 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,717 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:11,719 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:11,722 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:11,724 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,725 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,727 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,731 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,733 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,736 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,738 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,741 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,743 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,746 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,746 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,746 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,746 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:11,747 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,747 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,747 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:11,749 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:11,751 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:11,754 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,754 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,756 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,760 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,762 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,765 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,767 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,771 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,773 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,776 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,776 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,776 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,776 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:11,776 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,776 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,777 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:11,778 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:11,781 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:11,783 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,784 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,786 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,789 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,791 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,794 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,796 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,799 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,801 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,804 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,804 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,804 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,804 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:11,804 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,805 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,805 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:11,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:11,812 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:11,814 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,814 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,816 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,820 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,822 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,825 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,827 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,830 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,835 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,835 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,835 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,835 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:11,835 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,836 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,836 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:11,838 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:11,840 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:11,843 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,843 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,845 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,849 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,851 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,854 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,855 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,858 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,860 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,863 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,864 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,864 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,864 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:11,864 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,864 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,864 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:11,865 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:11,869 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:11,871 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,871 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,873 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,877 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,879 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,882 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,884 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,887 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,889 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,892 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,892 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,892 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,892 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:11,892 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,893 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,893 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:11,894 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:11,897 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:11,900 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,900 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,902 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,905 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,907 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,910 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,912 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,915 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,917 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,920 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,920 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,920 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,921 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:11,921 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,921 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,921 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:11,922 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:11,925 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:11,928 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,928 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,930 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,934 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,939 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,941 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,943 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,945 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,949 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,949 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,949 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:11,949 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,949 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,949 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:11,951 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:11,954 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:11,956 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,956 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,958 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,962 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,964 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,967 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,969 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,972 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,974 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,977 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,977 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:11,977 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:11,977 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:11,978 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,978 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,978 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:11,979 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:11,982 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:11,985 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:11,985 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:11,987 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:11,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,993 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:11,995 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:11,997 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,000 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,002 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,005 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,005 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,006 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,006 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:12,006 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,006 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,006 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:12,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:12,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:12,011 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,011 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,013 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,016 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,019 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,021 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,023 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,026 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,028 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,031 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,032 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,032 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,032 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:12,032 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,032 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,032 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:12,033 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:12,034 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:12,034 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,034 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,035 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:12,035 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,036 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,037 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,037 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,038 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,038 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,039 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,039 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,039 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,039 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:12,039 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,039 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,039 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:12,040 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:12,040 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:12,041 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,041 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,041 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:12,051 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,052 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,059 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,060 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,066 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,067 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,074 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,075 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:12,075 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,076 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:12,076 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:12,076 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,076 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:12,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:12,081 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:12,081 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:12,081 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,082 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:12,082 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,083 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,083 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,084 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,084 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,085 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,085 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,085 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,085 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,086 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:12,086 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 17), dtype=torch.int64)', '16')
2023-10-31 12:39:12,086 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,086 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:12,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:12,089 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:12,090 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 17), dtype=torch.int64)', '16')
2023-10-31 12:39:12,090 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,090 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 17), dtype=torch.int64)', '16'), {})
2023-10-31 12:39:12,091 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,091 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,091 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,092 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,093 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,094 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,094 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,094 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,095 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:12,095 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,096 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,096 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:12,096 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:12,099 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:12,101 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,101 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,104 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,107 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,113 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,114 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,118 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,120 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,123 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,123 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,124 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,124 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:12,124 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,124 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,124 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:12,126 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:12,128 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:12,131 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,131 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,133 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,137 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,139 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,142 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,144 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,147 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,152 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,153 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,153 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,153 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:12,153 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,153 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,153 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:12,155 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:12,158 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:12,160 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,160 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,162 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,166 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,168 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,171 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,173 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,177 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,179 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,182 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,182 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,182 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,182 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:12,182 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,182 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,183 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:12,184 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:12,187 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:12,189 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,190 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,192 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,195 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,197 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,200 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,202 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,205 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,207 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,210 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,210 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,210 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,210 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:12,210 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,210 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,210 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:12,212 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:12,215 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:12,218 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,218 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,220 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,223 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,225 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,228 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,230 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,233 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,235 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,238 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,238 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,238 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,239 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:12,239 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,239 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,239 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:12,240 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:12,243 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:12,246 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,246 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,248 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,251 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,254 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,257 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,259 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,262 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,264 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,267 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,267 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,267 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,267 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:12,267 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,268 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,268 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:12,269 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:12,272 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:12,275 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,275 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,277 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,280 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,282 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,285 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,287 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,290 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,292 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,295 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,295 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,295 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,296 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:12,296 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,296 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,296 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:12,297 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:12,300 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:12,303 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,303 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,305 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,308 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,311 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,313 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,316 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,319 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,320 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,323 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,324 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,324 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,324 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:12,814 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,814 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,814 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:12,816 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:12,819 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:12,822 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,822 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,824 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,828 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,830 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,833 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,835 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,838 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,840 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,843 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,843 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,844 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,844 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:12,844 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,844 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,844 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:12,846 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:12,849 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:12,851 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,851 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,853 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,857 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,859 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,862 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,864 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,867 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,869 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,872 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,872 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,872 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,872 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:12,872 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,873 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,873 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:12,874 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:12,877 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:12,880 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,880 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,882 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,885 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,890 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,892 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,895 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,897 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,900 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,901 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,901 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,901 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:12,901 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,901 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,901 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:12,903 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:12,903 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:12,906 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,906 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,908 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:12,912 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,914 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,917 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,919 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,922 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 12:39:12,926 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,927 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'))


2023-10-31 12:39:12,927 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,927 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:12,927 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,927 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,927 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:12,929 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:12,929 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:12,930 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,930 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,930 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:12,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,931 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,932 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,933 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,933 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,934 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,934 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,934 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,934 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,935 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:12,935 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,935 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,935 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:12,935 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:12,936 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:12,936 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,936 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,937 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:12,947 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,948 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,954 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,955 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,962 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,963 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:12,970 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,971 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:12,971 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,972 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:12,972 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:12,972 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,972 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:12,976 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:12,977 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:12,977 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:12,977 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,978 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:12,978 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,979 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,980 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,980 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,981 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,981 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,981 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,981 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,982 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:12,982 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 18), dtype=torch.int64)', '17')
2023-10-31 12:39:12,982 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:12,982 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:12,982 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:12,985 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:12,985 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 18), dtype=torch.int64)', '17')
2023-10-31 12:39:12,986 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:12,986 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 18), dtype=torch.int64)', '17'), {})
2023-10-31 12:39:12,986 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,987 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,987 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,988 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,988 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,989 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:12,990 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:12,990 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:12,990 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:12,991 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:12,991 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,991 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,992 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:12,992 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:12,995 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:12,997 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:12,997 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:12,999 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,003 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,005 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,009 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,011 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,014 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,016 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,019 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,019 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,019 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,019 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:13,020 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,020 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,020 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:13,021 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:13,024 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:13,027 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,027 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,029 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,033 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,035 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,038 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,040 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,043 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,048 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,049 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,049 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,049 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:13,049 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,049 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,049 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:13,051 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:13,054 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:13,056 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,057 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,059 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,063 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,064 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,068 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,070 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,073 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,075 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,078 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,078 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,078 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,079 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:13,079 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,079 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,079 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:13,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:13,083 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:13,086 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,086 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,088 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,092 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,094 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,097 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,099 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,101 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,103 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,106 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,107 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,107 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,107 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:13,107 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,107 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,107 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:13,109 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:13,112 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:13,115 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,115 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,117 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,121 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,123 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,126 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,127 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,130 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,135 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,136 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,136 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,136 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:13,136 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,136 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,136 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:13,138 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:13,141 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:13,143 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,144 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,146 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,149 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,151 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,154 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,156 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,159 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,164 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,164 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,165 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,165 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:13,165 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,165 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,165 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:13,166 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:13,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:13,172 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,172 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,174 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,178 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,180 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,183 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,185 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,188 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,190 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,192 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,193 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,193 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,193 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:13,193 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,193 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,193 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:13,195 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:13,198 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:13,200 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,200 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,202 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,206 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,208 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,211 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,213 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,216 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,218 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,221 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,221 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,221 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,221 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:13,221 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,221 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,222 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:13,223 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:13,226 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:13,229 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,229 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,231 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,234 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,236 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,239 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,241 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,244 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,246 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,249 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,250 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,250 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,250 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:13,250 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,250 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,250 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:13,252 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:13,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:13,257 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,257 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,259 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,263 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,265 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,268 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,270 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,273 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,275 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,277 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,278 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,278 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,278 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:13,278 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,278 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,278 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:13,280 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:13,283 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:13,285 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,285 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,287 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,291 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,293 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,296 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,298 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,301 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,303 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,306 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,306 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,306 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,306 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:13,306 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,306 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,307 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:13,308 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:13,309 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:13,311 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,311 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,314 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,317 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,319 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,322 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,324 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,327 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,332 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,333 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,333 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,333 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:13,333 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,333 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:13,333 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:13,335 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:13,335 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:13,336 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,336 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:13,337 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:13,337 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,338 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,338 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,339 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,340 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,340 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:13,341 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,341 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:13,341 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,341 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:13,341 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:13,342 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:13,342 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:13,342 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,343 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:13,343 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:13,353 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,356 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:13,363 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,364 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:13,370 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,371 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:13,378 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,379 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:13,379 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,380 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:13,380 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:13,380 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:13,381 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:13,385 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:13,385 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:13,386 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:13,386 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:13,387 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:13,387 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,388 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,388 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,389 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,389 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,390 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,390 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,390 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:13,390 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,391 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:13,391 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 19), dtype=torch.int64)', '18')
2023-10-31 12:39:13,391 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:13,391 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:13,391 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:13,394 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:13,395 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 19), dtype=torch.int64)', '18')
2023-10-31 12:39:13,395 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:13,396 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 19), dtype=torch.int64)', '18'), {})
2023-10-31 12:39:13,396 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,397 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,397 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,398 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,398 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,399 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:13,399 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,399 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:13,400 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,401 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:13,401 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,401 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,402 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:13,402 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:13,405 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:13,407 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,407 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,410 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,413 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,415 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,419 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,421 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,424 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,426 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,429 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,429 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,429 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,430 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:13,430 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,430 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,430 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:13,432 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:13,435 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:13,437 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,437 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,439 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,443 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,445 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,449 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,451 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,454 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,456 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,459 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,459 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,460 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,460 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:13,460 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,460 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,460 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:13,462 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:13,465 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:13,467 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,468 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,470 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,474 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,476 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,479 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,481 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,484 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,486 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,489 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,490 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,490 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,490 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:13,490 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,490 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,490 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:13,492 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:13,495 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:13,497 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,497 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,500 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,503 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,505 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,508 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,510 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,513 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,515 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,518 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,518 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,518 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,518 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:13,518 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,519 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,519 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:13,520 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:13,523 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:13,526 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,526 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,528 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,532 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,534 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,537 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,539 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,542 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,544 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,546 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,547 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,547 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,547 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:13,547 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,547 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,548 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:13,549 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:13,552 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:13,554 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,555 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,557 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,560 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,562 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,565 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,567 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,570 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,572 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,575 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,576 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,576 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,576 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:13,576 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,576 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,576 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:13,577 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:13,581 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:13,583 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,583 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,585 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:13,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,591 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,594 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,596 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,599 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,601 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:13,604 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:13,604 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:13,604 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:13,604 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:13,604 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,604 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:13,605 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:13,606 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:13,609 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:13,611 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:13,612 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,320 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,324 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,327 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,330 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,332 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,335 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,337 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,340 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,341 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,341 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:14,341 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,341 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,341 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:14,342 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:14,345 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:14,348 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,348 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,350 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,354 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,356 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,359 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,361 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,364 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,366 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,369 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,369 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,369 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,370 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:14,370 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,370 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,370 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:14,371 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:14,374 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:14,377 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,377 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,379 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,383 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,385 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,388 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,390 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,393 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,395 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,398 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,398 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,398 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,398 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:14,398 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,398 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,399 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:14,400 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:14,403 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:14,406 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,406 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,408 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,412 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,414 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,417 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,419 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,422 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,424 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,427 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,427 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,427 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,428 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:14,428 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,428 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,428 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:14,429 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:14,430 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:14,433 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,433 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,435 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 19), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 18, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,438 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,440 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,443 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,445 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,449 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,451 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,454 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,454 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,454 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,454 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:14,454 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,454 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,455 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:14,456 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:14,456 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:14,457 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,457 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,457 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:14,458 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,459 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,459 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,460 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,460 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,461 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,461 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,461 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,461 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,461 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:14,462 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,462 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,462 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:14,462 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:14,462 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:14,463 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,463 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,463 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:14,474 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,475 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,481 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,482 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,489 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,492 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,499 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,500 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:14,500 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,501 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:14,501 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:14,502 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,502 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:14,506 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:14,506 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:14,507 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:14,507 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,507 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:14,508 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,508 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,508 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,509 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,510 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,510 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,511 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,511 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,511 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,511 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:14,511 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 20), dtype=torch.int64)', '19')
2023-10-31 12:39:14,511 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,511 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:14,512 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:14,514 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:14,515 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 20), dtype=torch.int64)', '19')
2023-10-31 12:39:14,515 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,516 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 20), dtype=torch.int64)', '19'), {})
2023-10-31 12:39:14,516 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,516 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,517 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,518 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,518 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,519 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,519 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,519 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,521 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:14,521 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,521 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,521 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:14,521 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:14,524 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:14,527 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,527 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,529 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,533 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,535 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,538 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,540 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,543 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,545 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,548 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,549 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,549 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,549 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:14,549 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,549 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,550 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:14,551 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:14,554 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:14,557 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,557 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,559 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,563 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,565 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,568 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,570 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,575 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,578 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,578 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,579 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,579 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:14,579 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,579 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,579 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:14,581 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:14,584 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:14,586 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,586 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,588 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,592 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,594 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,598 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,600 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,603 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,605 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,608 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,609 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,609 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,609 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:14,609 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,609 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,609 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:14,611 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:14,614 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:14,616 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,616 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,618 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,622 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,624 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,627 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,629 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,632 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,634 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,637 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,637 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,637 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,637 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:14,638 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,638 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,638 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:14,639 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:14,642 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:14,645 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,645 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,647 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,651 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,656 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,661 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,663 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,666 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,667 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,667 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,667 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:14,667 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,667 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,667 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:14,669 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:14,672 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:14,674 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,674 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,676 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,680 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,682 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,685 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,687 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,690 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,692 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,695 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,695 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,695 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,695 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:14,695 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,695 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,696 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:14,699 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:14,703 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:14,705 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,705 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,708 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,712 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,714 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,717 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,719 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,722 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,724 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,727 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,727 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,727 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,728 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:14,728 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,728 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,728 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:14,730 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:14,732 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:14,735 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,735 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,737 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,741 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,743 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,747 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,749 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,752 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,754 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,757 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,758 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,758 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,758 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:14,758 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,758 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,758 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:14,760 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:14,763 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:14,765 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,765 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,768 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,772 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,774 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,777 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,779 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,782 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,784 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,787 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,788 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,788 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,788 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:14,788 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,788 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,788 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:14,790 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:14,793 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:14,795 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,795 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,798 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,802 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,804 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,807 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,810 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,813 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,815 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,818 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,818 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,819 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,819 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:14,819 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,819 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,819 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:14,820 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:14,823 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:14,826 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,826 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,829 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,833 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,837 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,840 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,842 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,846 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,848 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,852 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,852 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,852 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,852 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:14,852 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,852 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,853 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:14,854 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:14,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:14,858 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,858 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,860 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 20), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 19, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,864 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,866 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,869 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,871 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,875 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,877 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,880 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,881 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,881 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,881 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:14,881 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,881 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,881 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:14,882 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:14,883 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:14,883 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,883 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,884 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:14,884 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,885 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,885 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,886 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,886 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,887 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,888 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,888 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,888 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:14,888 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,888 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,888 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:14,889 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:14,889 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:14,889 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,889 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,890 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:14,903 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,904 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,911 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,912 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,919 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:14,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,929 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:14,930 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,931 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:14,931 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:14,931 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,931 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:14,936 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:14,946 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:14,947 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:14,947 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,948 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:14,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,949 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,950 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,950 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,951 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,951 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,951 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,951 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:14,951 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 21), dtype=torch.int64)', '20')
2023-10-31 12:39:14,951 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:14,952 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:14,952 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:14,955 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:14,956 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 21), dtype=torch.int64)', '20')
2023-10-31 12:39:14,956 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:14,956 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 21), dtype=torch.int64)', '20'), {})
2023-10-31 12:39:14,957 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,957 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,958 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,958 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,959 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,959 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:14,960 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,960 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:14,960 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,961 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:14,962 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,962 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,962 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:14,962 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:14,965 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:14,968 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,968 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,971 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:14,975 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,977 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,981 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,986 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,988 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:14,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:14,992 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:14,992 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:14,992 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:14,992 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:14,992 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:14,992 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:14,994 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:14,997 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:15,000 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,000 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,002 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,007 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,009 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,012 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,014 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,018 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,020 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,023 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,023 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,023 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,023 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:15,024 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,024 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,024 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:15,026 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:15,028 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:15,031 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,031 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,033 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,037 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,039 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,042 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,044 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,047 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,049 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,052 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,052 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,052 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,052 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:15,052 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,052 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,053 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:15,054 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:15,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:15,059 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,060 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,061 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,065 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,066 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,069 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,074 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,076 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,079 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,079 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,079 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,079 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:15,079 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,079 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,080 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:15,081 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:15,084 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:15,087 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,087 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,088 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,093 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,094 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,097 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,099 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,102 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,104 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,107 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,107 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,107 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,108 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:15,108 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,108 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,108 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:15,109 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:15,112 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:15,115 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,115 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,116 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,120 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,122 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,125 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,127 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,131 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,135 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,135 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,136 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,136 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:15,136 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,136 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,136 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:15,137 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:15,140 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:15,143 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,143 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,145 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:15,148 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,150 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,153 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,155 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,158 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:15,162 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:15,162 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:15,162 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:15,163 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:15,163 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,163 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:15,163 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:15,164 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:15,167 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:15,170 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:15,170 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,430 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,434 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,436 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,439 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,441 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,444 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,446 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,449 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,449 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,449 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,449 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:16,449 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,449 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,450 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:16,451 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:16,454 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:16,456 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,456 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,458 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,462 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,464 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,467 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,469 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,472 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,473 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,476 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,476 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,477 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,477 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:16,477 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,477 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,477 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:16,479 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:16,481 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:16,484 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,484 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,485 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,489 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,494 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,495 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,498 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,500 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,502 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,503 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,503 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,503 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:16,503 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,503 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,503 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:16,504 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:16,507 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:16,510 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,510 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,511 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,515 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,516 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,519 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,520 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,523 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,524 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,526 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,527 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,527 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,527 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:16,527 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,527 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,527 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:16,529 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:16,529 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:16,532 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,532 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,533 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 21), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 20, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,537 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,538 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,541 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,542 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,545 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,546 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,549 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,549 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,549 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,549 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:16,549 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,549 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,549 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:16,550 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:16,551 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:16,551 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,551 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,552 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:16,552 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,553 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,553 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,554 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,554 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,555 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,555 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,555 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:16,556 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,556 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:16,556 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,556 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,556 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:16,556 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:16,557 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:16,557 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,557 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,558 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:16,567 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,568 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,574 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,580 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,580 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,586 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,588 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:16,588 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,589 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:16,589 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:16,589 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,590 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:16,593 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:16,594 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:16,594 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:16,594 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,595 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:16,595 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,596 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,596 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,596 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,597 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,597 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,598 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,598 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:16,598 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,598 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:16,598 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 22), dtype=torch.int64)', '21')
2023-10-31 12:39:16,598 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,598 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:16,599 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:16,601 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:16,602 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 22), dtype=torch.int64)', '21')
2023-10-31 12:39:16,602 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,602 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 22), dtype=torch.int64)', '21'), {})
2023-10-31 12:39:16,603 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,603 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,604 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,604 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,605 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,605 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,606 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,606 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:16,606 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,607 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:16,607 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,607 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,608 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:16,608 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:16,611 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:16,614 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,614 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,616 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,619 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,621 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,623 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,625 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,628 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,629 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,631 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,632 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,632 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,632 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:16,632 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,632 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,632 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:16,634 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:16,637 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:16,639 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,640 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,641 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,645 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,646 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,649 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,651 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,653 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,655 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,657 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,657 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,658 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,658 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:16,658 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,658 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,658 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:16,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:16,663 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:16,666 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,666 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,667 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,671 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,672 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,675 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,676 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,679 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,681 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,683 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,683 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,683 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,683 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:16,684 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,684 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,684 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:16,685 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:16,688 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:16,691 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,691 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,693 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,696 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,698 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,700 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,702 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,704 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,706 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,708 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,708 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,708 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,709 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:16,709 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,709 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,709 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:16,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:16,714 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:16,716 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,716 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,718 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,722 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,723 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,726 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,730 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,732 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,734 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,734 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,735 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,735 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:16,735 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,735 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,735 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:16,737 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:16,739 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:16,742 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,742 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,744 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,748 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,750 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,753 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,754 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,757 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,759 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,761 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,762 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,762 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,762 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:16,762 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,762 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,763 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:16,764 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:16,767 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:16,770 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,770 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,772 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,775 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,777 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,780 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,782 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,784 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,786 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,788 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,788 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,789 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,789 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:16,789 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,789 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,789 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:16,791 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:16,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:16,796 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,796 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,799 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,802 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,804 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,806 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,808 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,811 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,813 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,815 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,815 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,816 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,816 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:16,816 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,816 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,816 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:16,817 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:16,820 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:16,823 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,824 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,826 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,829 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,831 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,844 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,846 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,849 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,851 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,854 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,855 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,855 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,855 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:16,855 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,855 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,855 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:16,857 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:16,860 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:16,862 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,862 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,864 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,868 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,870 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,874 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,876 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,879 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,881 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,884 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,884 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,884 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,885 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:16,885 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,885 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,885 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:16,886 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:16,889 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:16,892 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,892 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,894 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,898 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,900 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,903 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,905 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,909 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,911 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,915 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,915 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,915 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,915 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:16,915 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,915 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,916 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:16,917 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:16,918 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:16,921 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,921 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:16,923 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 22), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 21, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:16,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,929 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,932 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,934 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,937 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,939 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'))
2023-10-31 12:39:16,942 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,942 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'))


2023-10-31 12:39:16,942 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,942 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:16,942 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,943 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,943 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:16,944 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:16,944 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:16,945 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,945 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,945 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:16,946 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,947 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,947 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,948 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,949 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,949 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:16,949 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,950 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:16,950 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,950 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,950 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:16,950 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:16,950 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:16,951 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:16,951 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,952 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:16,960 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,961 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,966 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,967 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,973 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,974 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:16,979 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,981 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:16,981 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,982 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:16,982 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:16,982 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,983 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:16,986 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:16,987 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:16,987 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:16,987 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,988 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:16,988 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,989 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,989 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,990 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,990 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,991 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,991 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:16,991 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:16,992 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:16,992 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 23), dtype=torch.int64)', '22')
2023-10-31 12:39:16,992 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:16,992 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:16,992 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:16,995 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:16,995 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 23), dtype=torch.int64)', '22')
2023-10-31 12:39:16,996 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:16,996 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 23), dtype=torch.int64)', '22'), {})
2023-10-31 12:39:16,996 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,997 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,997 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,998 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,998 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:16,999 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:16,999 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,000 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:17,000 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,001 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:17,001 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,001 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,002 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:17,002 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:17,005 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:17,007 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,007 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,009 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,013 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,015 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,017 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,019 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,021 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,023 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,026 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,026 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,026 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,026 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:17,026 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,026 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,026 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:17,028 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:17,031 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:17,033 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,033 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,036 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,039 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,041 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,043 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,047 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,049 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,052 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,052 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,052 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,052 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:17,052 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,052 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,052 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:17,054 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:17,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:17,059 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,060 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,062 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,065 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,067 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,069 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,073 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,075 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,077 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,078 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,078 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,078 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:17,078 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,078 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,078 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:17,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:17,083 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:17,085 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,085 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,087 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,094 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,097 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,099 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,103 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,105 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,108 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,108 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,108 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,108 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:17,108 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,108 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,109 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:17,110 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:17,113 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:17,116 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,116 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,118 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,122 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,124 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,126 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,128 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,130 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,135 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,135 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,135 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,135 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:17,135 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,135 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,136 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:17,137 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:17,140 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:17,142 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,143 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,145 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,148 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,152 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,157 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,161 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,161 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,161 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,162 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:17,162 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,162 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,162 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:17,163 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:17,166 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:17,169 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:17,169 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:17,171 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:17,175 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,177 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,179 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,181 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,184 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,186 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:17,188 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:17,188 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:17,189 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:17,189 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:18,716 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,716 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,716 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:18,718 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:18,720 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:18,723 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,723 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,725 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,728 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,730 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,734 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,736 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,739 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,741 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,744 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,745 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,745 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,745 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:18,745 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,745 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,746 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:18,747 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:18,752 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:18,756 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,756 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,759 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,762 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,764 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,767 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,769 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,771 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,773 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,776 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,776 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,776 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,776 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:18,776 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,777 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,777 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:18,778 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:18,781 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:18,784 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,784 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,786 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,790 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,792 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,794 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,796 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,799 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,800 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,803 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,803 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,803 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,803 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:18,803 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,804 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,804 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:18,805 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:18,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:18,811 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,811 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,813 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,817 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,819 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,821 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,823 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,826 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,828 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,830 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,831 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,831 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,831 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:18,831 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,831 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,831 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:18,833 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:18,833 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:18,836 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,836 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,838 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 23), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 22, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,842 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,844 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,846 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,848 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,851 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,852 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,855 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,856 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,856 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,856 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:18,856 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,856 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:18,856 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:18,858 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:18,858 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:18,859 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,859 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:18,859 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:18,860 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,860 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,861 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,862 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,862 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,863 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,863 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,863 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:18,864 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,864 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:18,864 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,864 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:18,864 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:18,865 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:18,865 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:18,865 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,866 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:18,866 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:18,878 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,880 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:18,887 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,888 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:18,894 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,896 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:18,902 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,905 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:18,905 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,906 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:18,907 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:18,907 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:18,907 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:18,911 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:18,911 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:18,912 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:18,912 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:18,912 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:18,913 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,913 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,914 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,914 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,914 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,915 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,915 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,916 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:18,916 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,916 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:18,916 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 24), dtype=torch.int64)', '23')
2023-10-31 12:39:18,916 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:18,916 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:18,917 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:18,920 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:18,920 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 24), dtype=torch.int64)', '23')
2023-10-31 12:39:18,920 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:18,923 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 24), dtype=torch.int64)', '23'), {})
2023-10-31 12:39:18,923 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,924 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,925 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,925 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,926 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:18,926 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,926 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:18,926 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,928 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:18,928 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,928 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,928 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:18,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:18,931 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:18,934 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,934 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,936 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,941 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,944 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,947 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,952 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,954 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,957 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,958 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,958 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,958 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:18,958 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,958 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,958 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:18,960 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:18,963 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:18,965 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,966 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,968 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:18,972 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,974 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,977 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,982 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,984 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:18,987 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:18,988 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:18,988 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:18,988 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:18,988 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,988 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,988 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:18,990 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:18,993 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:18,995 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:18,996 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:18,998 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,002 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,004 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,007 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,009 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,012 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,014 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,017 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,018 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,018 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,018 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:19,018 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,018 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,018 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:19,020 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:19,023 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:19,025 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,025 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,027 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,031 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,033 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,036 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,038 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,041 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,043 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,046 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,046 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,047 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,047 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:19,047 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,047 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,047 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:19,049 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:19,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:19,054 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,054 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,057 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,060 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,063 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,066 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,068 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,071 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,073 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,075 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,076 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,076 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,076 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:19,076 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,076 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,076 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:19,078 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:19,081 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:19,083 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,083 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,086 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,089 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,091 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,094 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,096 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,099 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,104 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,104 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,104 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,105 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:19,105 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,105 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,105 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:19,106 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:19,109 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:19,112 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,112 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,114 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,118 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,120 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,123 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,125 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,128 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,130 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,133 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,133 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,134 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,134 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:19,134 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,134 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,134 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:19,136 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:19,139 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:19,141 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,141 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,143 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,147 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,152 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,157 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,162 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,162 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,162 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,162 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:19,163 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,163 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,163 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:19,164 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:19,167 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:19,170 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,170 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,172 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,176 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,178 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,181 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,183 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,186 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,188 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,190 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,191 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,191 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,191 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:19,191 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,191 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,191 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:19,193 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:19,196 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:19,198 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,199 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,201 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,204 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,206 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,209 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,211 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,214 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,219 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,219 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,219 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,219 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:19,219 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,220 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,220 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:19,221 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:19,224 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:19,227 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,227 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,229 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,233 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,234 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,237 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,239 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,242 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,244 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,247 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,247 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,248 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,248 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:19,248 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,248 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,248 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:19,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:19,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:19,253 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,253 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,255 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 24), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 23, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,259 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,261 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,264 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,266 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,269 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,271 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,273 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,274 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,274 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,274 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:19,274 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,274 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:19,274 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:19,275 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:19,276 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:19,276 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,276 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:19,277 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:19,277 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,278 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,278 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,279 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,279 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,280 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,280 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,281 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:19,281 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,281 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:19,281 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,281 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:19,281 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:19,282 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:19,282 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:19,282 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,282 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:19,283 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:19,293 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,294 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:19,299 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:19,306 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,307 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:19,312 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,313 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:19,313 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,314 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:19,314 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:19,314 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:19,314 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:19,318 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:19,319 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:19,319 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:19,319 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:19,320 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:19,320 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,321 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,322 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,322 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,323 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,323 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:19,323 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,323 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:19,324 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 25), dtype=torch.int64)', '24')
2023-10-31 12:39:19,324 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:19,324 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:19,324 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:19,327 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:19,327 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 25), dtype=torch.int64)', '24')
2023-10-31 12:39:19,327 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:19,328 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 25), dtype=torch.int64)', '24'), {})
2023-10-31 12:39:19,328 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,329 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,329 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,330 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,331 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:19,331 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,331 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:19,331 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,333 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:19,333 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,333 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,333 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:19,333 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:19,336 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:19,339 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,339 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,340 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,344 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,345 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,348 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,349 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,351 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,355 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,356 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,356 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,356 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:19,356 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,356 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,356 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:19,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:19,361 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:19,363 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,363 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,365 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,368 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,370 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,372 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,373 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,376 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,377 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,380 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,380 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,380 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,380 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:19,380 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,380 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,380 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:19,382 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:19,385 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:19,387 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,387 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,389 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,392 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,396 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,398 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,400 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,404 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,404 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,404 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,404 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:19,405 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,405 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,405 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:19,406 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:19,409 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:19,411 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,412 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,413 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,417 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,418 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,421 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,422 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,425 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,426 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,428 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,429 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,429 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,429 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:19,429 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,429 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,430 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:19,431 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:19,434 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:19,436 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,437 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,438 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,442 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,443 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,446 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,447 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,450 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,451 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,455 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,456 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,456 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,456 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:19,456 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,456 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,456 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:19,458 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:19,461 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:19,463 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,464 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,465 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,469 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,470 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,473 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,474 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,477 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,479 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:19,481 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,481 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:19,481 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:19,481 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:19,482 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,482 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,482 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:19,483 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:19,486 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:19,488 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:19,489 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:19,490 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:19,494 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:19,496 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,460 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,462 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,465 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,467 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,470 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,470 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,470 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,471 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:21,471 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,471 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,471 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:21,473 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:21,476 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:21,478 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,478 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,480 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,484 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,485 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,488 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,489 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,492 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,493 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,496 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,497 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,497 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,497 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:21,497 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,497 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,497 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:21,498 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:21,502 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:21,504 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,504 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,506 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,509 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,511 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,513 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,515 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,521 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,522 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,522 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,522 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:21,522 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,522 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,522 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:21,524 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:21,527 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:21,529 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,529 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,531 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,535 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,536 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,539 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,540 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,545 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,546 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,549 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,549 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,549 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,550 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:21,550 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,550 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,550 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:21,551 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:21,555 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:21,557 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,557 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,559 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,562 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,564 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,566 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,568 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,571 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,572 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,574 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,575 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,575 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,575 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:21,575 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,575 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,576 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:21,577 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:21,578 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:21,580 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,580 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,582 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 25), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 24, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,585 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,587 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,591 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,593 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,595 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,597 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,597 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,597 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,598 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:21,598 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,598 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,598 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:21,599 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:21,599 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:21,600 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,600 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,600 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:21,601 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,601 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,602 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,602 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,603 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,603 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,604 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,604 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:21,604 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,605 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:21,605 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,605 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,605 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:21,605 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:21,606 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:21,606 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,606 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,607 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:21,615 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,616 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,621 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,622 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,628 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,628 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,634 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,637 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:21,637 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,638 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:21,638 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:21,638 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,638 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:21,642 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:21,642 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:21,643 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:21,643 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,644 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:21,644 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,644 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,645 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,645 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,645 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,646 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,646 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,647 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:21,647 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,647 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:21,647 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 26), dtype=torch.int64)', '25')
2023-10-31 12:39:21,647 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,647 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:21,648 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:21,650 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:21,651 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 26), dtype=torch.int64)', '25')
2023-10-31 12:39:21,651 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,652 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 26), dtype=torch.int64)', '25'), {})
2023-10-31 12:39:21,652 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,652 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,653 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,654 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,654 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,655 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,655 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:21,655 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,656 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:21,656 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,656 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,657 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:21,657 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:21,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:21,662 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,662 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,664 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,668 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,669 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,672 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,673 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,676 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,677 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,680 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,680 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,680 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,681 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:21,681 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,681 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,681 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:21,683 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:21,685 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:21,688 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,688 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,690 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,693 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,695 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,697 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,699 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,701 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,703 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,705 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,705 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,706 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,706 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:21,706 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,706 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,706 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:21,708 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:21,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:21,713 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,713 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,715 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,718 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,720 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,722 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,724 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,726 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,728 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,730 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,730 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,730 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,731 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:21,731 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,731 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,731 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:21,733 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:21,735 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:21,738 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,738 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,740 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,743 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,745 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,747 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,749 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,751 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,753 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,756 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,756 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,757 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,757 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:21,757 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,757 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,758 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:21,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:21,764 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:21,768 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,768 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,770 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,774 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,776 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,779 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,785 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,787 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,791 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,791 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,791 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,791 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:21,791 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,792 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,792 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:21,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:21,798 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:21,800 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,801 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,802 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,805 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,807 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,809 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,811 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,813 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,815 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,817 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,817 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,817 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,817 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:21,817 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,818 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,818 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:21,819 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:21,822 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:21,825 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,825 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,826 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,829 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,831 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,833 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,835 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,837 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,839 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,841 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,841 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,841 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,841 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:21,842 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,842 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,842 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:21,843 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:21,846 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:21,849 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,849 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,850 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,854 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,855 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,857 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,859 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,861 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,863 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,865 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,865 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,865 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,866 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:21,866 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,866 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,866 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:21,867 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:21,870 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:21,873 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,873 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,875 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,878 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,879 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,882 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,883 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,886 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,889 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,890 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,890 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,890 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:21,890 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,890 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,890 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:21,892 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:21,894 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:21,897 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,897 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,899 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,902 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,903 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,906 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,907 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,910 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,911 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,913 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,914 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,914 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,914 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:21,914 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,914 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,915 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:21,916 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:21,919 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:21,921 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,921 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,923 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,926 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,930 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,934 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,938 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,938 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,938 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,939 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:21,939 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,939 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,939 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:21,940 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:21,941 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:21,943 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,944 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:21,945 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 26), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 25, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:21,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,950 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,952 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,954 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,956 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,958 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'))
2023-10-31 12:39:21,960 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,960 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'))


2023-10-31 12:39:21,961 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,961 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:21,961 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,961 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,961 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:21,962 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:21,963 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:21,963 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,963 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,964 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:21,964 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,965 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,965 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,966 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,966 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,967 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:21,967 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,967 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:21,967 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:21,968 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:21,968 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,968 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:21,968 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:21,968 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:21,969 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:21,969 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:21,969 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:21,970 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:21,978 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,985 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,986 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,991 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:21,992 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:21,997 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,000 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:22,000 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,001 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:22,001 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:22,001 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:22,001 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:22,005 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:22,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:22,006 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:22,006 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:22,007 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:22,007 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,007 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,008 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,008 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,009 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,009 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,010 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,010 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:22,010 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,010 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:22,010 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 27), dtype=torch.int64)', '26')
2023-10-31 12:39:22,010 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:22,010 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:22,011 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:22,014 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:22,014 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 27), dtype=torch.int64)', '26')
2023-10-31 12:39:22,014 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:22,015 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 27), dtype=torch.int64)', '26'), {})
2023-10-31 12:39:22,015 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,015 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,016 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,016 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,017 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,017 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:22,018 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,018 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:22,018 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,019 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:22,019 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,020 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,020 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:22,020 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:22,023 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:22,025 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,025 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,027 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,030 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,032 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,034 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,036 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,038 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,040 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,042 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,042 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,043 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,043 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:22,043 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,043 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,043 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:22,045 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:22,048 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:22,050 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,050 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,052 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,055 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,057 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,060 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,061 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,064 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,065 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,068 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,068 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,068 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,068 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:22,068 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,068 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,069 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:22,070 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:22,073 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:22,075 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,076 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,077 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,081 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,082 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,085 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,086 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,089 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,090 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,093 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,093 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,093 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,093 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:22,093 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,094 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,094 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:22,095 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:22,098 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:22,100 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,100 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,102 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,105 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,107 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,110 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,112 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,114 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,118 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,118 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,118 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,118 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:22,118 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,119 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,119 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:22,120 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:22,123 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:22,126 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,126 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,128 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,131 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,135 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,136 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,139 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,140 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,143 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,143 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,143 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,143 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:22,143 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,144 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,144 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:22,145 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:22,148 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:22,150 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,150 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,152 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:22,155 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,157 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,160 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,164 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,165 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:22,167 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:22,168 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:22,168 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:22,168 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:22,168 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,168 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:22,168 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:22,170 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:22,173 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:22,175 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:22,175 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,702 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,705 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,707 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,711 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,713 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,716 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,717 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,720 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,720 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,720 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,721 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:32,721 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,721 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,721 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:32,723 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:32,725 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:32,728 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,728 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,729 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,733 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,735 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,738 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,740 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,743 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,744 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,747 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,747 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,747 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,748 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:32,748 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,748 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,748 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:32,749 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:32,752 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:32,754 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,755 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,756 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,760 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,762 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,765 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,766 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,769 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,771 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,774 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,774 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,774 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,774 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:32,774 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,775 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,775 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:32,776 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:32,779 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:32,781 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,782 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,783 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,787 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,789 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,792 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,794 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,797 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,798 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,801 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,801 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,802 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,802 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:32,802 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,802 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,802 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:32,803 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:32,807 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:32,809 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,809 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,811 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,814 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,816 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,819 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,820 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,823 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,824 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,827 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,827 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,827 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,828 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:32,828 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,828 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,828 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:32,829 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:32,830 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:32,832 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,833 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,835 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 27), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 26, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,838 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,839 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,842 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,844 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,848 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,850 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,852 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,852 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,852 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,853 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:32,853 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,853 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:32,853 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:32,854 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:32,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:32,855 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,855 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:32,856 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:32,856 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,857 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,857 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,858 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,858 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,859 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,859 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,860 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:32,860 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,860 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:32,860 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,860 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:32,860 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:32,861 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:32,861 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:32,862 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,862 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:32,862 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:32,871 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,872 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:32,877 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,878 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:32,883 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,884 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:32,890 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,892 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:32,892 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,893 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:32,893 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:32,893 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:32,893 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:32,897 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:32,897 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:32,898 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:32,898 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:32,899 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:32,899 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,899 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,900 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,900 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,900 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,901 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,901 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,902 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:32,902 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,902 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:32,902 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 28), dtype=torch.int64)', '27')
2023-10-31 12:39:32,902 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:32,902 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:32,903 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:32,906 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:32,906 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 28), dtype=torch.int64)', '27')
2023-10-31 12:39:32,906 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:32,907 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 28), dtype=torch.int64)', '27'), {})
2023-10-31 12:39:32,907 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,907 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,908 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,908 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,909 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,909 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:32,910 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,910 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:32,910 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,911 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:32,911 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,912 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,912 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:32,912 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:32,915 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:32,918 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,918 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,919 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,923 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,935 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,935 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,935 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,935 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:32,935 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,935 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,936 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:32,937 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:32,940 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:32,942 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,943 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,944 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,948 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,952 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,953 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,956 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,957 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,960 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,960 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,960 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,960 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:32,960 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,961 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,961 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:32,963 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:32,965 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:32,968 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,968 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,970 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,973 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,978 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,982 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:32,986 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:32,986 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:32,986 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:32,986 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:32,986 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,987 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,987 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:32,988 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:32,991 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:32,994 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:32,994 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:32,995 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:32,999 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,000 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,003 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,004 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,007 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,008 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,010 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,011 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,011 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,011 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:33,011 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,011 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,011 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:33,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:33,016 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:33,019 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,019 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,020 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,024 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,025 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,028 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,029 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,032 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,033 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,036 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,036 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,036 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,036 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:33,036 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,037 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,037 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:33,038 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:33,041 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:33,044 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,044 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,046 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,049 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,051 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,054 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,055 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,058 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,059 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,061 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,062 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,062 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,062 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:33,062 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,062 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,062 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:33,064 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:33,067 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:33,069 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,070 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,071 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,075 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,076 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,079 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,080 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,083 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,084 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,087 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,087 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,087 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,087 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:33,088 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,088 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,088 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:33,089 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:33,092 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:33,095 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,095 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,097 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,100 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,105 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,109 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,111 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,113 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,114 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,114 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,114 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:33,114 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,114 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,114 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:33,116 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:33,119 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:33,121 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,121 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,123 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,127 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,128 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,131 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,133 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,135 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,137 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,140 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,140 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,140 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,140 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:33,141 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,141 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,141 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:33,142 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:33,145 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:33,147 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,147 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,149 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,153 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,157 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,162 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,163 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,166 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,166 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,167 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,167 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:33,167 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,167 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,167 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:33,168 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:33,171 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:33,174 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,174 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,175 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,179 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,181 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,184 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,186 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,189 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,190 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,193 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,193 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,193 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,194 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:33,194 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,194 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,194 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:33,195 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:33,196 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:33,198 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,198 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,200 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 28), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 27, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,204 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,205 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,208 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,213 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,214 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,217 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,217 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,218 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,218 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:33,218 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,218 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:33,218 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:33,219 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:33,219 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:33,220 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,220 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:33,221 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:33,221 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,221 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,222 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,222 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,223 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,223 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,224 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,224 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:33,224 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,224 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:33,224 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,224 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:33,224 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:33,225 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:33,225 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:33,225 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,225 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:33,226 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:33,237 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,238 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:33,244 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,245 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:33,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,253 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:33,259 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,260 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:33,260 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,261 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:33,261 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:33,261 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:33,261 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:33,265 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:33,266 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:33,266 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:33,266 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:33,267 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:33,267 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,267 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,267 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,268 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,268 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,269 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,269 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,269 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:33,269 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,270 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:33,270 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 29), dtype=torch.int64)', '28')
2023-10-31 12:39:33,270 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:33,270 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:33,270 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:33,273 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:33,273 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 29), dtype=torch.int64)', '28')
2023-10-31 12:39:33,273 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:33,274 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 29), dtype=torch.int64)', '28'), {})
2023-10-31 12:39:33,274 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,275 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,275 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,275 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,276 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,276 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:33,277 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,277 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:33,277 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,278 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:33,278 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,278 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,278 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:33,279 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:33,281 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:33,284 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,284 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,286 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,290 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,291 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,294 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,296 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,299 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,303 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,304 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,304 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,304 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:33,304 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,304 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,304 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:33,306 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:33,309 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:33,311 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,311 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,313 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,317 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,318 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,321 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,323 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,327 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,329 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,332 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,333 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,333 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,333 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:33,333 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,333 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,334 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:33,336 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:33,340 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:33,344 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,344 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,346 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,351 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,356 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,358 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,362 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,364 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,371 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,371 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,371 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,371 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:33,371 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,371 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,371 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:33,373 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:33,376 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:33,378 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,378 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,380 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,383 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,385 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,387 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,389 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,391 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,393 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,395 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,396 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,396 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,396 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:33,396 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,396 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,396 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:33,398 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:33,401 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:33,403 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,403 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,405 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,408 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,410 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,413 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,414 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,417 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,418 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,421 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,421 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,421 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,421 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:33,421 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,421 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,422 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:33,423 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:33,426 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:33,428 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:33,428 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:33,430 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:33,433 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,435 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,438 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,439 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,442 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,443 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:33,446 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:33,446 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:33,446 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:33,446 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:34,174 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,174 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,174 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:34,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:34,179 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:34,181 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,182 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,183 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,187 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,189 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,192 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,194 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,198 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,199 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,202 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,203 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,203 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,203 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:34,203 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,203 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,203 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:34,205 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:34,208 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:34,210 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,210 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,212 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,216 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,217 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,221 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,222 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,225 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,229 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,230 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,230 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,230 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:34,230 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,230 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,230 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:34,231 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:34,235 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:34,237 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,237 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,239 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,243 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,244 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,247 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,249 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,252 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,253 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,256 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,256 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,256 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,257 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:34,257 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,257 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,257 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:34,258 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:34,261 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:34,264 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,264 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,266 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,270 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,271 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,274 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,275 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,278 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,280 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,283 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,283 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,283 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,283 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:34,283 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,283 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,284 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:34,285 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:34,288 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:34,290 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,290 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,292 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,296 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,297 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,300 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,302 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,305 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,306 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,309 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,310 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,310 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,310 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:34,310 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,310 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,310 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:34,312 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:34,312 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:34,315 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,315 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,317 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 29), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 28, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,320 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,322 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,325 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,326 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,329 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,331 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,334 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,334 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,334 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,334 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:34,334 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,335 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,335 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:34,336 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:34,336 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:34,337 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,337 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,337 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:34,338 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,338 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,339 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,340 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,341 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,341 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,341 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,341 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:34,341 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,341 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,342 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:34,342 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:34,342 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:34,343 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,343 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,343 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:34,355 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,356 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,364 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,365 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,374 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,375 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,383 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,386 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:34,386 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,387 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:34,387 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:34,387 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,387 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:34,391 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:34,391 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:34,392 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:34,392 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,392 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:34,393 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,393 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,394 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,394 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,395 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,395 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,395 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,396 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,396 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:34,396 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 30), dtype=torch.int64)', '29')
2023-10-31 12:39:34,396 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,396 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:34,397 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:34,399 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:34,400 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 30), dtype=torch.int64)', '29')
2023-10-31 12:39:34,400 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,400 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 30), dtype=torch.int64)', '29'), {})
2023-10-31 12:39:34,401 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,401 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,402 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,403 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,403 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,403 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,404 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,404 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,405 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:34,405 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,405 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,405 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:34,406 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:34,409 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:34,411 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,411 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,413 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,417 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,418 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,421 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,423 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,426 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,427 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,430 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,430 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,430 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,430 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:34,431 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,431 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,431 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:34,432 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:34,435 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:34,438 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,438 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,440 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,443 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,445 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,448 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,449 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,452 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,454 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,456 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,457 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,457 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,457 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:34,457 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,457 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,457 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:34,459 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:34,462 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:34,464 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,464 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,466 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,471 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,472 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,476 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,478 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,482 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,484 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,487 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,488 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,488 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,488 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:34,488 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,488 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,489 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:34,491 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:34,495 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:34,499 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,499 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,501 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,504 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,506 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,509 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,510 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,513 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,515 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,518 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,518 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,518 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:34,518 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,518 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,518 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:34,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:34,523 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:34,525 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,525 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,527 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,531 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,532 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,535 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,536 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,539 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,541 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,544 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,544 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,544 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,544 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:34,544 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,544 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,544 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:34,546 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:34,548 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:34,551 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,551 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,552 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,556 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,558 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,561 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,562 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,565 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,567 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,569 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,570 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,570 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,570 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:34,570 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,570 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,570 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:34,571 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:34,574 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:34,577 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,577 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,578 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,582 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,584 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,587 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,588 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,591 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,593 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,596 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,596 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,596 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,596 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:34,596 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,596 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,597 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:34,598 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:34,601 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:34,603 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,604 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,605 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,609 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,610 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,614 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,615 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,618 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,619 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,622 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,623 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,623 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,623 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:34,623 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,623 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,623 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:34,624 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:34,628 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:34,630 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,630 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,632 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,636 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,637 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,640 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,643 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,647 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,649 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,652 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,653 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,653 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,653 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:34,653 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,653 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,654 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:34,655 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:34,658 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:34,660 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,661 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,662 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,667 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,668 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,671 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,673 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,676 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,681 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,681 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,681 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,682 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:34,682 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,682 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,682 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:34,683 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:34,686 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:34,689 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,689 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,691 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,695 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,697 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,700 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,701 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,704 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,705 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,708 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,708 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,708 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,708 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:34,708 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,709 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,709 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:34,710 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:34,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:34,713 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,713 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,715 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 30), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 29, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,719 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,720 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,723 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,725 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,727 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,729 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,731 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,732 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,732 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,732 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:34,732 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,732 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,732 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:34,734 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:34,734 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:34,734 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,735 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,735 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:34,735 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,736 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,736 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,737 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,737 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,738 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,738 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,739 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,739 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,739 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:34,739 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,739 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,739 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:34,740 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:34,740 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:34,740 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,741 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,741 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:34,753 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,754 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,760 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,761 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,768 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,768 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:34,775 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,778 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:34,778 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,779 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:34,780 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:34,780 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,780 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:34,784 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:34,784 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:34,785 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:34,785 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,785 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:34,786 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,786 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,786 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,787 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,787 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,788 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,788 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,788 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,788 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,789 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:34,789 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 31), dtype=torch.int64)', '30')
2023-10-31 12:39:34,789 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:34,789 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:34,789 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:34,792 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:34,793 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 31), dtype=torch.int64)', '30')
2023-10-31 12:39:34,793 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:34,793 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 31), dtype=torch.int64)', '30'), {})
2023-10-31 12:39:34,794 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,794 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,794 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,795 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,795 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,796 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:34,796 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,797 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:34,797 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,798 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:34,798 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,798 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,798 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:34,799 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:34,801 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:34,804 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,804 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,806 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,810 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,811 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,814 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,816 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,819 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,820 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,823 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,823 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,823 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,824 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:34,824 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,824 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,824 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:34,825 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:34,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:34,831 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,831 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,832 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,836 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,838 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,841 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,842 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,846 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,847 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,850 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,850 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,850 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,851 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:34,851 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,851 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,851 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:34,852 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:34,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:34,858 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,858 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,859 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,863 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,865 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,868 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,869 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,872 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,875 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,879 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,879 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,879 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,879 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:34,879 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,880 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,880 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:34,881 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:34,884 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:34,886 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,887 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,888 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,892 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,894 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,896 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,898 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,901 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,902 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,904 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,905 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,905 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,905 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:34,905 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,905 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,905 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:34,907 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:34,910 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:34,912 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,912 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,914 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,918 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,922 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:34,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,931 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:34,932 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:34,932 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:34,932 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,932 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,932 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:34,934 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:34,936 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:34,939 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:34,939 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:34,940 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:34,957 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:34,959 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,471 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,473 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,477 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,478 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,481 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,482 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,482 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,482 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:48,482 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,482 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,482 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:48,484 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:48,487 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:48,490 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,490 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,492 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,496 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,497 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,501 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,502 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,505 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,507 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,510 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,510 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,510 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,510 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:48,510 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,511 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,511 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:48,512 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:48,515 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:48,517 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,518 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,519 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,523 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,525 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,528 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,530 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,533 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,534 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,539 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,539 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,539 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,539 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:48,540 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,540 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,540 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:48,541 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:48,544 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:48,546 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,547 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,548 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,552 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,554 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,557 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,559 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,562 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,564 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,567 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,567 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,567 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,567 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:48,567 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,567 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,568 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:48,569 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:48,572 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:48,574 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,574 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,576 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,580 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,581 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,584 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,586 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,589 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,590 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,593 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,593 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,594 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,594 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:48,594 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,594 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,594 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:48,595 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:48,598 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:48,600 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,601 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,602 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,606 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,607 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,610 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,612 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,615 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,616 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,619 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,619 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,619 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,620 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:48,620 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,620 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,620 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:48,621 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:48,622 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:48,625 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,625 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,627 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 31), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 30, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,635 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,636 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,642 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,643 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,647 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,648 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,651 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,652 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,652 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,652 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:48,652 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,652 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:48,652 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:48,653 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:48,654 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:48,654 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,654 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:48,655 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:48,655 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,656 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,656 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,657 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,657 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,658 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,658 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:48,658 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,659 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:48,659 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,659 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:48,659 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:48,659 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:48,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:48,660 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,660 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:48,661 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:48,670 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,671 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:48,677 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:48,685 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,685 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:48,692 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,695 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:48,695 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,696 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:48,696 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:48,696 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:48,696 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:48,700 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:48,701 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:48,701 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:48,701 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:48,702 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:48,702 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,702 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,703 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,703 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,703 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,704 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,704 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,705 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:48,705 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,705 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:48,705 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 32), dtype=torch.int64)', '31')
2023-10-31 12:39:48,705 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:48,705 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:48,706 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:48,708 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:48,709 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 32), dtype=torch.int64)', '31')
2023-10-31 12:39:48,709 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:48,709 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 32), dtype=torch.int64)', '31'), {})
2023-10-31 12:39:48,710 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,710 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,710 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,711 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,711 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,712 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:48,712 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,712 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:48,713 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,714 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:48,714 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,714 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,714 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:48,715 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:48,717 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:48,720 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,720 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,721 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,725 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,730 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,732 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,735 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,737 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,739 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,740 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,740 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,740 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:48,740 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,740 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,740 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:48,742 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:48,745 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:48,747 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,747 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,749 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,753 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,755 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,758 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,759 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,763 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,764 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,767 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,768 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,768 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,768 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:48,768 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,768 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,768 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:48,770 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:48,773 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:48,775 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,775 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,777 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,781 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,782 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,786 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,787 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,790 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,792 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,795 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,796 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,796 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,796 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:48,796 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,796 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,796 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:48,798 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:48,800 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:48,803 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,803 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,805 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,812 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,814 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,817 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,819 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,823 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,825 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,828 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,828 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,828 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,829 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:48,829 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,829 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,829 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:48,831 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:48,836 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:48,840 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,840 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,842 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,846 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,848 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,852 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,854 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,856 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,858 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,861 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,861 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,861 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,861 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:48,861 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,862 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,862 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:48,863 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:48,866 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:48,868 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,868 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,870 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,873 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,875 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,877 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,879 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,881 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,883 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,885 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,886 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,886 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,886 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:48,886 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,886 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,886 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:48,887 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:48,890 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:48,893 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,893 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,894 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,898 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,899 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,902 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,903 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,906 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,907 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,910 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,910 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,911 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,911 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:48,911 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,911 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,911 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:48,912 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:48,915 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:48,917 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,918 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,919 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,923 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,933 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,935 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,935 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,936 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,936 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:48,936 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,936 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,936 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:48,937 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:48,940 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:48,942 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,942 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,944 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,947 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,949 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,951 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,953 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,956 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,957 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,960 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,960 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,960 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,960 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:48,960 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,961 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,961 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:48,962 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:48,965 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:48,967 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,967 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,969 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,972 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,974 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,976 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,978 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,981 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,982 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:48,985 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,985 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:48,985 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:48,985 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:48,985 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,985 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,985 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:48,986 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:48,989 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:48,992 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:48,992 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:48,994 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:48,997 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:48,999 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,001 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,003 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,005 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,007 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,009 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,010 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,010 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,010 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:49,010 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,010 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,010 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:49,012 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:49,012 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:49,015 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,015 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,016 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 32), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 31, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,020 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,021 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,024 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,026 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,028 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,030 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,032 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,032 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,033 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,033 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:49,033 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,033 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:49,033 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:49,034 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:49,035 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:49,035 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,035 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:49,036 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:49,036 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,037 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,037 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,038 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,038 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,039 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,039 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,039 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:49,039 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,040 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:49,040 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,040 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:49,040 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:49,040 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:49,041 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:49,041 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,041 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:49,042 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:49,051 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,052 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:49,059 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,059 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:49,066 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,066 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:49,073 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,075 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:49,076 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,076 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:49,077 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:49,077 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:49,077 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:49,081 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:49,081 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:49,082 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:49,082 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:49,082 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:49,082 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,083 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,083 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,084 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,084 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,085 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,085 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,085 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:49,085 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,086 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:49,086 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 33), dtype=torch.int64)', '32')
2023-10-31 12:39:49,086 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:49,086 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:49,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:49,089 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:49,090 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 33), dtype=torch.int64)', '32')
2023-10-31 12:39:49,090 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:49,090 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 33), dtype=torch.int64)', '32'), {})
2023-10-31 12:39:49,090 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,091 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,091 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,092 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,092 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:49,093 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,093 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:49,094 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,095 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:49,095 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,095 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,095 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:49,095 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:49,098 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:49,101 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,101 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,103 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,106 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,108 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,111 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,112 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,115 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,119 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,119 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,119 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,119 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:49,120 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,120 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,120 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:49,121 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:49,124 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:49,126 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,127 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,128 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,132 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,133 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,136 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,138 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,141 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,142 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,145 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,145 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,146 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,146 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:49,146 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,146 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,146 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:49,148 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:49,150 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:49,153 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,153 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,155 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,158 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,160 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,163 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,164 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,167 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,169 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,171 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,172 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,172 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,172 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:49,172 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,172 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,172 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:49,174 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:49,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:49,179 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,179 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,180 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,184 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,185 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,188 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,189 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,192 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,194 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,196 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,196 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,196 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,197 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:49,197 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,197 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,197 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:49,198 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:49,201 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:49,204 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,204 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,206 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:49,209 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,214 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,219 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,221 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:49,224 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:49,224 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:49,224 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:49,224 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:49,224 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,224 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:49,224 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:49,226 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:49,229 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:49,231 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:49,232 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,602 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,606 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,608 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,613 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,614 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,617 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,619 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,622 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,622 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,622 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,623 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:52,623 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,623 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,623 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:52,624 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:52,628 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:52,630 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,631 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,632 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,636 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,637 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,640 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,642 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,644 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,646 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,648 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,649 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,649 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,649 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:52,649 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,649 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,650 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:52,651 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:52,654 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:52,656 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,656 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,658 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,662 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,667 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,668 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,671 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,673 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,675 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,676 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,676 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,676 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:52,676 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,676 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,676 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:52,678 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:52,681 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:52,683 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,683 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,685 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,689 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,690 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,693 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,695 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,698 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,699 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,702 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,702 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,702 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,703 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:52,703 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,703 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,703 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:52,704 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:52,707 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:52,710 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,710 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,712 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,715 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,717 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,720 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,721 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,724 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,726 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,728 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,729 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,729 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,729 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:52,729 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,729 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,730 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:52,731 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:52,734 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:52,736 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,736 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,738 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,742 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,744 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,747 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,748 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,751 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,753 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,756 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,756 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,756 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,756 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:52,756 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,756 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,757 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:52,758 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:52,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:52,761 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,761 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,763 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 33), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 32, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,767 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,768 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,771 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,773 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,775 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,777 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,780 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,780 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,780 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,780 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:52,780 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,780 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:52,781 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:52,782 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:52,782 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:52,783 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,783 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:52,783 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:52,784 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,784 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,785 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,785 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,786 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,786 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,787 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,787 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:52,787 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,787 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:52,787 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,787 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:52,787 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:52,788 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:52,788 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:52,789 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,789 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:52,789 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:52,799 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,800 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:52,806 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,807 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:52,813 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,814 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:52,821 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,822 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:52,822 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,823 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:52,823 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:52,824 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:52,824 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:52,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:52,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:52,828 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:52,829 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:52,829 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:52,829 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,830 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,830 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,831 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,831 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,832 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,832 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:52,832 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,833 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:52,833 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 34), dtype=torch.int64)', '33')
2023-10-31 12:39:52,833 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:52,833 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:52,833 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:52,836 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:52,837 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 34), dtype=torch.int64)', '33')
2023-10-31 12:39:52,837 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:52,837 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 34), dtype=torch.int64)', '33'), {})
2023-10-31 12:39:52,838 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,838 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,838 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,839 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,839 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,840 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:52,840 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,841 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:52,841 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,842 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:52,842 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,842 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,842 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:52,843 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:52,846 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:52,848 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,849 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,850 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,854 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,856 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,859 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,860 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,863 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,865 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,867 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,868 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,868 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,868 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:52,868 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,868 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,868 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:52,870 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:52,873 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:52,875 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,876 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,877 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,881 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,883 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,886 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,891 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,892 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,895 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,895 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,895 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,896 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:52,896 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,896 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,896 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:52,898 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:52,901 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:52,903 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,903 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,905 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,909 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,911 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,914 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,915 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,918 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,923 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,923 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,923 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,923 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:52,923 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,923 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,924 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:52,925 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:52,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:52,930 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,931 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,932 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,936 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,937 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,940 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,942 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,945 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,946 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,949 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,949 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,949 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,949 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:52,949 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,949 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,950 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:52,951 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:52,954 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:52,957 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,957 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,958 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,962 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,964 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,966 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,968 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,971 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,972 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,975 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,975 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:52,975 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:52,975 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:52,976 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,976 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,976 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:52,977 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:52,980 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:52,983 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:52,983 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:52,985 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:52,988 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,990 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,992 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,994 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:52,997 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:52,998 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,001 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,001 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,001 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,002 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:53,002 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,002 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,002 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:53,003 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:53,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:53,009 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,009 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,011 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,014 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,016 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,018 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,020 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,023 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,024 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,027 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,027 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,027 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,027 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:53,027 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,027 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,028 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:53,029 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:53,032 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:53,034 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,035 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,036 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,040 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,041 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,044 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,048 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,050 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,052 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,052 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,053 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,053 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:53,053 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,053 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,053 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:53,054 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:53,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:53,060 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,060 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,062 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,065 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,067 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,070 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,074 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,075 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,078 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,078 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,078 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,078 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:53,079 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,079 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,079 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:53,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:53,083 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:53,086 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,086 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,087 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,091 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,095 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,097 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,100 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,104 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,104 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,104 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,104 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:53,104 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,105 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,105 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:53,106 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:53,109 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:53,111 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,111 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,113 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,116 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,118 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,121 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,122 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,125 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,126 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,129 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,129 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,129 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,129 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:53,129 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,130 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,130 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:53,131 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:53,132 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:53,134 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,134 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,136 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 34), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 33, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,139 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,141 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,143 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,145 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,148 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,152 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,152 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,152 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,152 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:53,152 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,152 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:53,153 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:53,154 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:53,154 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:53,155 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,155 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:53,155 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:53,156 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,156 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,157 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,157 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,158 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,158 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,159 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,159 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:53,159 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,159 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:53,159 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,159 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:53,160 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:53,160 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:53,160 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:53,161 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,161 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:53,161 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:53,171 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,172 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:53,178 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,179 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:53,185 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,186 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:53,193 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,194 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:53,194 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,195 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:53,196 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:53,196 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:53,196 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:53,200 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:53,200 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:53,201 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:53,201 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:53,202 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:53,202 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,202 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,203 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,203 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,204 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,204 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,204 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,205 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:53,205 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,205 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:53,205 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 35), dtype=torch.int64)', '34')
2023-10-31 12:39:53,205 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:53,205 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:53,206 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:53,209 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:53,209 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 35), dtype=torch.int64)', '34')
2023-10-31 12:39:53,209 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:53,210 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 35), dtype=torch.int64)', '34'), {})
2023-10-31 12:39:53,210 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,211 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,211 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,212 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,212 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:53,213 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,213 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:53,213 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,214 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:53,214 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,215 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,215 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:53,215 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:53,218 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:53,220 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,220 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,222 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,226 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,231 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,232 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,235 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,237 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,240 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,240 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,240 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,241 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:53,241 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,241 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,241 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:53,242 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:53,245 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:53,247 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,248 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,249 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,253 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,255 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,258 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,260 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,263 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,265 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,268 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,268 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,268 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,268 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:53,268 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,269 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,269 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:53,270 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:53,273 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:53,275 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,276 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,277 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,281 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,283 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,286 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,288 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,291 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,292 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,295 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,296 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,296 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,296 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:53,296 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,296 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,296 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:53,298 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:53,301 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:53,303 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,303 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,305 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,308 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,313 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,315 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,318 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,319 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,322 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,322 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,323 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,323 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:53,323 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,323 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,323 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:53,325 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:53,328 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:53,330 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,330 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,332 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,335 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,337 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,342 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,344 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,346 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,349 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,349 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,349 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,350 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:53,350 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,350 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,926 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:53,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:53,931 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:53,934 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,935 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,936 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,940 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,942 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,945 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,947 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,950 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,952 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,955 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,955 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,955 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,956 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:53,956 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,956 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,956 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:53,957 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:53,960 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:53,963 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,963 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,965 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,968 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,970 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,973 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,978 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:53,982 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,983 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:53,983 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:53,983 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:53,983 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,983 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,984 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:53,985 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:53,988 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:53,991 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:53,991 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:53,992 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:53,996 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:53,998 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,001 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,003 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,006 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,007 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,010 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,011 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,011 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,011 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:54,011 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,011 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,011 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:54,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:54,016 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:54,018 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,018 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,020 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,024 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,026 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,029 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,030 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,033 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,035 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,038 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,038 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,038 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,039 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:54,039 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,039 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,039 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:54,041 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:54,043 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:54,046 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,046 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,048 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,051 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,053 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,056 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,058 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,061 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,063 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,066 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,066 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,066 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,066 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:54,067 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,067 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,067 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:54,068 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:54,071 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:54,074 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,074 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,075 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,079 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,081 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,084 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,086 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,089 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,090 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,093 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,094 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,094 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,094 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:54,094 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,094 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,095 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:54,096 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:54,097 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:54,099 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,099 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,101 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 35), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 34, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,105 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,109 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,111 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,114 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,119 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,119 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,119 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,120 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:54,120 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,120 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,120 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:54,121 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:54,122 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:54,122 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,122 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,123 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:54,123 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,124 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,124 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,125 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,125 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,126 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,126 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,126 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,126 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,127 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:54,127 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,127 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,127 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:54,127 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:54,128 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:54,128 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,128 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,129 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:54,138 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,139 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,146 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,147 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,153 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,160 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,162 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:54,162 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,163 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:54,163 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:54,163 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,163 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:54,167 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:54,168 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:54,168 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:54,168 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,169 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:54,169 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,170 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,170 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,170 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,171 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,171 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,172 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,172 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,172 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,172 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:54,172 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 36), dtype=torch.int64)', '35')
2023-10-31 12:39:54,173 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,173 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:54,173 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:54,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:54,176 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 36), dtype=torch.int64)', '35')
2023-10-31 12:39:54,177 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,177 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 36), dtype=torch.int64)', '35'), {})
2023-10-31 12:39:54,177 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,178 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,178 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,179 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,179 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,180 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,180 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,180 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,180 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,182 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:54,182 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,182 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,182 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:54,182 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:54,185 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:54,188 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,188 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,190 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,193 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,197 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,200 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,202 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,205 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,207 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,210 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,211 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,211 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,211 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:54,211 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,211 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,211 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:54,213 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:54,216 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:54,218 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,218 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,220 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,224 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,226 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,229 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,231 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,235 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,236 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,239 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,240 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,240 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,240 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:54,240 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,240 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,240 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:54,242 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:54,245 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:54,247 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,248 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,249 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,254 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,255 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,259 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,261 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,264 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,266 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,269 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,269 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,269 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,270 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:54,270 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,270 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,270 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:54,272 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:54,274 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:54,277 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,277 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,279 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,283 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,285 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,288 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,290 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,293 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,294 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,297 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,298 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,298 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,298 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:54,298 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,298 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,299 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:54,300 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:54,303 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:54,306 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,306 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,308 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,312 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,313 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,316 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,318 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,321 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,323 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,326 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,326 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,327 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,327 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:39:54,327 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,327 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,327 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:39:54,329 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:54,331 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:54,334 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,334 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,336 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,340 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,342 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,345 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,346 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,350 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,351 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,355 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,355 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,355 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,355 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:39:54,355 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,355 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,356 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:39:54,357 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:54,360 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:39:54,362 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,363 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,364 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,368 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,370 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,373 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,375 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,378 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,380 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,383 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,383 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,383 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,384 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:39:54,384 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,384 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,384 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:39:54,386 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:54,388 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:39:54,391 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,391 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,393 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,397 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,399 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,402 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,404 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,407 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,408 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,411 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,412 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,412 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,412 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:39:54,412 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,412 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,413 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:39:54,414 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:54,417 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:39:54,419 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,419 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,421 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,425 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,427 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,430 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,432 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,435 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,437 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,440 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,440 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,440 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,440 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:39:54,440 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,441 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,441 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:39:54,442 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:54,445 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:39:54,448 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,448 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,450 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,454 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,455 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,458 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,460 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,463 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,465 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,468 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,469 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,469 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,469 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:39:54,469 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,469 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,469 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:39:54,470 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:54,474 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:39:54,477 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,477 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,479 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,482 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,484 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,487 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,489 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,492 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,493 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,496 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,497 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,497 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,497 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:39:54,497 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,497 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,497 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:39:54,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:54,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:39:54,502 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,502 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,504 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 36), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 35, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,508 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,510 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,512 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,514 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,517 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,522 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,522 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,523 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,523 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:39:54,523 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,523 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,523 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:39:54,525 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:54,525 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:39:54,526 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,526 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,526 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:54,527 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,527 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,528 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,528 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,529 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,529 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,530 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,530 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,530 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,530 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:39:54,530 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,530 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,530 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:39:54,531 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:54,531 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:39:54,531 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,532 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,532 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:39:54,541 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,542 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,549 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,550 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,556 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,557 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:39:54,563 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,565 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:39:54,565 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,566 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:39:54,566 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:39:54,566 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,566 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:39:54,570 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:54,570 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:39:54,571 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:39:54,571 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,571 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:39:54,572 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,572 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,573 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,574 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,574 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,575 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,575 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,575 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:39:54,575 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 37), dtype=torch.int64)', '36')
2023-10-31 12:39:54,575 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:39:54,575 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:39:54,576 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:54,578 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:39:54,579 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 37), dtype=torch.int64)', '36')
2023-10-31 12:39:54,579 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:39:54,579 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 37), dtype=torch.int64)', '36'), {})
2023-10-31 12:39:54,580 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,580 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,581 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,581 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,582 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,582 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:39:54,582 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,583 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:39:54,583 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,584 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:39:54,584 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,584 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,584 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:39:54,585 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:54,587 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:39:54,590 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,590 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,592 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,595 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,598 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,601 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,602 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,605 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,607 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,610 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,611 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,611 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,611 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:39:54,611 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,611 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,611 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:39:54,613 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:54,615 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:39:54,618 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,618 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,620 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,624 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,625 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,628 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,630 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,633 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,635 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,638 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,638 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,638 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,639 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:39:54,639 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,639 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,639 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:39:54,641 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:54,643 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:39:54,646 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,646 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,647 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,651 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,656 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,661 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,663 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,666 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,666 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,666 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,666 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:39:54,667 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,667 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,667 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:39:54,668 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:54,671 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:39:54,673 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,674 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,675 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,679 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,680 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,683 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,685 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,688 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,690 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:39:54,692 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,693 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:39:54,693 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:39:54,693 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:39:54,693 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,693 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,693 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:39:54,695 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:39:54,698 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:39:54,700 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:39:54,700 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:39:54,702 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:39:54,706 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:39:54,707 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,348 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,350 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,354 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,355 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,358 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,359 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,359 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,359 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:40:00,359 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,359 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,359 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:40:00,361 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:40:00,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:40:00,367 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,367 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,369 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,372 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,374 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,377 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,379 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,382 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,383 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,387 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,387 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,387 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,387 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:40:00,387 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,387 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,388 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:40:00,389 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:40:00,392 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:40:00,394 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,394 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,396 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,399 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,401 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,404 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,406 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,409 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,411 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,413 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,414 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,414 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,414 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:40:00,414 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,414 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,414 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:40:00,416 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:40:00,418 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:40:00,421 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,421 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,423 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,426 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,428 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,431 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,433 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,435 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,437 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,440 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,440 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,440 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,440 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:40:00,441 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,441 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,441 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:40:00,442 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:40:00,446 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:40:00,448 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,448 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,450 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,454 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,455 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,458 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,460 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,463 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,465 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,467 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,468 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,468 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,468 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:40:00,468 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,468 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,468 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:40:00,470 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:40:00,473 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:40:00,475 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,475 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,477 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,480 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,482 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,485 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,487 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,490 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,494 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,494 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,495 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,495 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:40:00,495 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,495 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,495 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:40:00,496 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:40:00,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:40:00,502 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,502 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,503 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,507 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,509 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,512 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,516 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,518 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,521 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,521 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,521 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,522 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:40:00,522 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,522 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,522 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:40:00,523 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:40:00,524 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:40:00,526 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,527 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,528 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 37), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 36, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,532 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,533 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,536 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,538 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,541 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,543 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,546 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,546 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,546 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,546 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:40:00,547 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,547 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,547 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:40:00,548 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:40:00,548 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:40:00,549 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,549 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,549 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:40:00,550 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,551 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,551 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,551 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,552 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,552 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,553 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,553 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:40:00,553 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,554 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:40:00,554 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,554 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,554 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:40:00,554 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:40:00,555 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:40:00,555 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,555 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,556 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:40:00,565 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,566 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,573 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,573 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,580 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,581 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,587 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,589 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:40:00,589 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,590 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 12:40:00,590 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 1), dtype=torch.int64)',)
2023-10-31 12:40:00,590 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,590 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 12:40:00,594 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:40:00,595 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:40:00,595 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 1), dtype=torch.int64)',)
2023-10-31 12:40:00,595 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,596 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1), dtype=torch.int64)',), {})
2023-10-31 12:40:00,596 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,597 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,597 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,597 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,598 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,598 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,599 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,599 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:40:00,599 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,599 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 12:40:00,599 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('Tensor(shape=(4, 38), dtype=torch.int64)', '37')
2023-10-31 12:40:00,599 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,600 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 12:40:00,600 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:40:00,603 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 12:40:00,603 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(1, 38), dtype=torch.int64)', '37')
2023-10-31 12:40:00,603 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,604 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 38), dtype=torch.int64)', '37'), {})
2023-10-31 12:40:00,604 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,605 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,605 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,606 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,606 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,607 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,607 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,607 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:40:00,607 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,609 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 12:40:00,609 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,609 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,609 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 12:40:00,609 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:40:00,612 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 12:40:00,615 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,615 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,617 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,621 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,622 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,625 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,627 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,630 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,632 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,635 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,636 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,636 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,636 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 12:40:00,636 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,636 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,636 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 12:40:00,638 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:40:00,641 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 12:40:00,643 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,643 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,645 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,649 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,651 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,655 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,662 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,667 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,668 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,668 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,668 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 12:40:00,668 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,668 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,668 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 12:40:00,671 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:40:00,675 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 12:40:00,679 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,679 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,681 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,686 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,688 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,692 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,694 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,697 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,699 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,702 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,702 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,702 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,703 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 12:40:00,703 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,703 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,703 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 12:40:00,704 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:40:00,707 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 12:40:00,710 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,710 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,711 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,715 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,716 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,719 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,721 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,723 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,725 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,727 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,728 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,728 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,728 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 12:40:00,728 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,728 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,728 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 12:40:00,730 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:40:00,733 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 12:40:00,735 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,736 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,737 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,741 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,742 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,745 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,746 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,749 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,751 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,753 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,754 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,754 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,754 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 12:40:00,754 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,754 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,754 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 12:40:00,756 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:40:00,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 12:40:00,761 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,761 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,763 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,766 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,768 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,771 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,772 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,775 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,776 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,779 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,780 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,780 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,780 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 12:40:00,780 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,780 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,780 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 12:40:00,781 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:40:00,785 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 12:40:00,787 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,787 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,789 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,793 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,794 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,797 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,798 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,801 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,803 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,805 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,805 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,806 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,806 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 12:40:00,806 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,806 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,806 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 12:40:00,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:40:00,810 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 12:40:00,813 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,813 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,815 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,818 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,820 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,823 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,824 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,827 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,828 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,831 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,831 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,831 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,832 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 12:40:00,832 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,832 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,832 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 12:40:00,833 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:40:00,836 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 12:40:00,839 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,839 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,841 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,844 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,846 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,848 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,850 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,852 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,854 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,857 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,857 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,857 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,857 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 12:40:00,857 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,857 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,858 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 12:40:00,859 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:40:00,862 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 12:40:00,864 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,864 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,866 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,870 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,871 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,874 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,875 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,878 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,880 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,882 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,882 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,883 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,883 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 12:40:00,883 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,883 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,883 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 12:40:00,884 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:40:00,887 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 12:40:00,890 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,890 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,892 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,895 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,897 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,899 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,901 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,904 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,905 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,908 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,908 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,908 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,908 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 12:40:00,908 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,908 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(4, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,908 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 12:40:00,910 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:40:00,910 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 12:40:00,913 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,913 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 12:40:00,915 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(1, 1, 1, 38), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 37, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 12:40:00,918 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,923 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,924 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,927 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)', 'MixTensor(shape=(1, 12, 38, 64), dtype=torch.float32)'))
2023-10-31 12:40:00,931 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,931 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)', 'BatchListTensor(shape=(4, 12, 38, 64), dtype=torch.float32)'))


2023-10-31 12:40:00,931 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,932 [flexgen.py:200 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 12:40:00,932 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,932 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,932 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 12:40:00,933 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:40:00,933 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 12:40:00,934 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,934 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,935 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:40:00,935 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,936 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,937 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,937 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 768), dtype=torch.float32)
2023-10-31 12:40:00,938 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,938 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)


2023-10-31 12:40:00,938 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,938 [flexgen.py:200 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 12:40:00,938 [flexgen.py:201 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(4, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,939 [flexgen.py:202 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 12:40:00,939 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 12:40:00,939 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 12:40:00,939 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 12:40:00,940 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',)
2023-10-31 12:40:00,940 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 12:40:00,940 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('MixTensor(shape=(1, 1, 768), dtype=torch.float32)',), {})
2023-10-31 12:40:00,950 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,951 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,957 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,958 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,964 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,965 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, output: MixTensor(shape=(1, 1, 50272), dtype=torch.float32)
2023-10-31 12:40:00,972 [flexgen.py:157 in batch_sync] DEBUG - sync prev_batch, next_batch, current
2023-10-31 12:40:00,974 [flexgen.py:223 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(4, 1, 50272), dtype=torch.float32)


2023-10-31 12:40:00,974 [flexgen.py:164 in layer_sync] DEBUG - sync prev_layer, next_layer, current
2023-10-31 12:40:00,976 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 12:40:00,976 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 12:40:00,976 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 12:40:00,976 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 12:40:00,976 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 12:40:00,976 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 12:40:00,976 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 12:40:00,976 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 12:40:00,988 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 12:40:00,988 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 12:40:00,988 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 12:40:00,988 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 12:40:00,988 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 12:40:00,989 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 12:40:00,990 [flexgen.py:86 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 12:40:00,990 [flexgen.py:86 in layer_reset] DEBUG - lm_head from flexgen to old.
