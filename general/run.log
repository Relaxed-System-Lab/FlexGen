2023-10-30 05:13:19,619 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpxldnaw79
2023-10-30 05:13:19,620 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpxldnaw79/_remote_module_non_scriptable.py
2023-10-30 05:13:20,048 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-30 05:13:20,138 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:13:21,716 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-30 05:13:22,005 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 05:13:22,005 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 05:13:22,005 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 05:13:22,006 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 05:13:22,813 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:13:22,890 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-30 05:13:22,890 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-30 05:13:22,931 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 05:13:23,007 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-30 05:13:23,011 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-30 05:13:23,011 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-30 05:13:23,012 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-30 05:13:23,013 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-30 05:13:23,014 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-30 05:13:23,015 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-30 05:13:23,016 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-30 05:13:23,017 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-30 05:13:23,018 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-30 05:13:23,018 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-30 05:13:23,020 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-30 05:13:23,021 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-30 05:13:23,021 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-30 05:13:23,022 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-30 05:13:23,023 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 05:13:23,024 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 05:13:23,024 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-30 05:13:23,026 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-30 05:13:23,068 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 05:13:23,216 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-30 05:13:23,216 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-30 05:13:23,216 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-30 05:13:23,216 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-30 05:13:23,216 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-30 05:13:23,217 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-30 05:13:23,218 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-30 05:13:23,218 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-30 05:13:23,221 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:23,222 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-30 05:13:23,223 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:23,224 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-30 05:13:23,224 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:23,235 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-30 05:13:23,238 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:23,245 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-30 05:13:23,249 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:23,255 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-30 05:13:23,257 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:23,263 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-30 05:13:23,265 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:23,271 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-30 05:13:23,273 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:23,279 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-30 05:13:23,281 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:23,286 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-30 05:13:23,288 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:23,294 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-30 05:13:23,296 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:23,302 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-30 05:13:23,304 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:23,309 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-30 05:13:23,311 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:23,317 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-30 05:13:23,320 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:23,326 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-30 05:13:23,328 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:23,329 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-30 05:13:23,330 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:23,338 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-30 05:13:23,342 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-30 05:13:23,343 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-30 05:13:23,344 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-30 05:13:23,352 [model.py:408 in init_all_weights] DEBUG - init all weights...
2023-10-30 05:13:23,379 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-30 05:13:23,379 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-30 05:13:23,379 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-30 05:13:23,379 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-30 05:13:23,380 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-30 05:13:23,381 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-30 05:13:23,381 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-30 05:13:23,381 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-30 05:13:23,381 [flexgen.py:148 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-30 05:13:23,381 [flexgen.py:148 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-30 05:13:23,417 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 05:13:23,546 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:23,546 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:23,547 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-30 05:13:23,547 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,548 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,549 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,549 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,550 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,550 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:13:23,550 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:23,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:23,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:23,555 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-30 05:13:23,555 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,555 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,556 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,557 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,558 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,558 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:13:23,558 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:23,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:23,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:23,568 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,568 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,589 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,593 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,597 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,601 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,601 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,601 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:23,602 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:23,606 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:23,609 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,610 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,614 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,620 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,623 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,627 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,628 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,628 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:23,629 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:23,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:23,636 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,637 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,642 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,645 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,649 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,653 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,653 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,653 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:23,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:23,658 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:23,662 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,662 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,667 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,671 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,674 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,678 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,678 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,678 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:23,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:23,684 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:23,687 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,687 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,692 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,696 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,700 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,704 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,704 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,704 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:23,705 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:23,709 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:23,712 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,712 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,717 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,721 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,725 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,729 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,729 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,729 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:23,730 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:23,733 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:23,737 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,737 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,742 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,746 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,750 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,753 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,754 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,754 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:23,755 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:23,759 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:23,762 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,763 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,768 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,771 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,775 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,779 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,779 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,779 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:23,780 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:23,784 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:23,787 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,787 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,792 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,796 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,800 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,804 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,804 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,804 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:23,805 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:23,809 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:23,813 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,813 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,818 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,822 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,825 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,829 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,829 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,829 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:23,830 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:23,833 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:23,837 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,837 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,842 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,845 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,850 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,854 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,854 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,854 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:23,855 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:23,859 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:23,859 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,860 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,865 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,869 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,873 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,876 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 05:13:23,877 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 05:13:23,877 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:23,878 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:23,879 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:23,879 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,879 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,880 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,881 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,883 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 05:13:23,883 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 05:13:23,883 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:23,884 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:23,884 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:23,885 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 05:13:23,885 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,895 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:13:23,904 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:13:23,912 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:13:23,923 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 9, 50272])
2023-10-30 05:13:23,925 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-30 05:13:23,925 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:23,930 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:23,931 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:23,931 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:23,932 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,933 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,933 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,934 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,935 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,935 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:23,935 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:23,936 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:23,936 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:23,941 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-30 05:13:23,941 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:23,942 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,943 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,943 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,944 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:23,944 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:23,945 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:23,948 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:23,951 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:23,955 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:23,956 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,961 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,965 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,968 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,972 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,972 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:23,973 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:23,974 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:23,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:23,981 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:23,981 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:23,986 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,990 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,994 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,997 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:23,997 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:23,998 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:23,999 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:24,003 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,007 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,007 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,012 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,016 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,020 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,023 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,024 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,024 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:24,025 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,029 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,033 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,033 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,038 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,042 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,054 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,057 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,058 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,058 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:24,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,063 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,068 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,068 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,073 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,077 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,081 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,085 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,085 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,085 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:24,087 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,090 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:24,094 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,094 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,099 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,103 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,106 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,110 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,110 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,110 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:24,112 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:24,115 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:24,119 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,119 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,124 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,128 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,131 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,135 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,135 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,135 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:24,137 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:24,140 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:24,144 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,144 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,149 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,153 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,157 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,161 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,161 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,161 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:24,162 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:24,166 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:24,170 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,170 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,175 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,179 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,183 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,186 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,187 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,187 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:24,188 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:24,192 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:24,195 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,196 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,201 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,205 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,208 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,212 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,212 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,212 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:24,213 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:24,217 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:24,221 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,221 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,226 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,230 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,234 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,237 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,237 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,238 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:24,239 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:24,243 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:24,243 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,243 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,248 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,252 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,255 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,259 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 05:13:24,259 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 05:13:24,259 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:24,260 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:24,261 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:24,261 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,262 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,262 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,263 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,264 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,265 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,265 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,265 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:24,266 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:24,266 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:24,267 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,267 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,274 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,280 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,287 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,293 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,294 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:24,294 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:24,298 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:24,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:24,300 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:24,300 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,300 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,301 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,302 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,302 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,302 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,303 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:24,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:24,304 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:24,308 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-30 05:13:24,308 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,309 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,309 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,310 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,311 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,311 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,311 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:24,314 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:24,318 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:24,322 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,322 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,327 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,331 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,358 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,388 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,388 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,388 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:24,390 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:24,394 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:24,398 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,398 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,428 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,435 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,441 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,466 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,466 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,467 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:24,468 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:24,472 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,477 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,478 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,484 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,488 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,493 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,497 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,497 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,498 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:24,500 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,504 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,510 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,510 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,516 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,521 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,526 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,531 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,531 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,531 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:24,534 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,539 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,545 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,546 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,552 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,557 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,562 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,567 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,567 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,567 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:24,570 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,575 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:24,581 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,581 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,588 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,599 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,602 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,606 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,606 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,606 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:24,608 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:24,611 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:24,615 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,616 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,620 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,625 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,628 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,632 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,632 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:24,634 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:24,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:24,641 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,641 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,646 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,650 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,654 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,681 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,682 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,682 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:24,683 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:24,687 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:24,691 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,691 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,696 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,700 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,703 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,708 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,708 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,708 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:24,710 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:24,713 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:24,717 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,717 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,722 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,726 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,730 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,733 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,734 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,734 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:24,735 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:24,739 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:24,743 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,743 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,748 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,754 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,758 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,761 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,762 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,762 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:24,763 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:24,767 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:24,768 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,768 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,773 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,777 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,781 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,784 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 05:13:24,784 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 05:13:24,784 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:24,786 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:24,786 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:24,787 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,787 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,788 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,789 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,789 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,790 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,790 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,790 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:24,791 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:24,791 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:24,792 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,792 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,799 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,806 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,812 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,819 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:24,820 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:24,820 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:24,825 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:24,825 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:24,826 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:24,826 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,827 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,827 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,828 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,829 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,829 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,829 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:24,829 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:24,830 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:24,834 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-30 05:13:24,834 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:24,835 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,835 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,836 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,837 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:24,837 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:24,837 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:24,841 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:24,844 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:24,848 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,848 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,866 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,870 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,874 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,877 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,877 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:24,877 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:24,879 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:24,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:24,886 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,887 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,891 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,895 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,899 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,902 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,903 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:24,903 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:24,904 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:24,908 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,912 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,912 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,917 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,920 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,924 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,927 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,928 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:24,928 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:24,929 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:24,933 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,936 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,937 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,941 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,945 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,948 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,951 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:24,951 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:24,953 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:24,957 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,961 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,962 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,966 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,970 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,973 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,977 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,977 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:24,977 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:24,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:24,982 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:24,986 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:24,986 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:24,990 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,994 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:24,997 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,001 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,001 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,001 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:25,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:25,006 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,010 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,010 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,015 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,018 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,022 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,025 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,025 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,025 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:25,027 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,034 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,035 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,040 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,051 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,055 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,055 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,055 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:25,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,060 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,064 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,064 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,069 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,072 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,075 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,079 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,079 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,080 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:25,081 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,085 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:25,089 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,089 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,093 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,097 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,101 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,104 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,104 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,104 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:25,106 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:25,109 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:25,113 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,113 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,135 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,138 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,142 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,145 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,145 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,145 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:25,147 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:25,150 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:25,151 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,151 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,156 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,159 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,163 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,166 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 05:13:25,166 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 05:13:25,166 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:25,167 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:25,168 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:25,168 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,168 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,169 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,170 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,171 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,171 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,172 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,172 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:25,172 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:25,172 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:25,173 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,173 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,180 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,186 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,192 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,200 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,200 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:25,200 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:25,205 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:25,206 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:25,206 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:25,206 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,207 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,208 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,208 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,209 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,209 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,210 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:25,210 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:25,210 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:25,214 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-30 05:13:25,215 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,215 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,216 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,217 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,217 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:25,220 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:25,224 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:25,228 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,228 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,232 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,235 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,239 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,242 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,242 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,242 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:25,244 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:25,247 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:25,251 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,251 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,255 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,259 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,262 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,266 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,266 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,266 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:25,268 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:25,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:25,275 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,275 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,280 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,284 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,287 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,290 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,290 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,291 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:25,292 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:25,295 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:25,299 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,299 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,304 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,307 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,310 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,314 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,314 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,314 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:25,316 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:25,319 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:25,323 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,323 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,347 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,351 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,354 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,357 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,358 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,358 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:25,359 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:25,362 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:25,366 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,366 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,370 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,374 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,378 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,382 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,382 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,383 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:25,384 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:25,387 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,390 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,390 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,395 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,398 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,402 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,442 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,443 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,443 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:25,444 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,447 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,451 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,451 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,492 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,521 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,551 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,566 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,567 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,567 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:25,569 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,573 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,577 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,577 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,582 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,586 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,591 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,619 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,619 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,619 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:25,621 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,625 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:25,628 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,629 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,633 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,637 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,642 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,645 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,645 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,645 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:25,646 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:25,650 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:25,654 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,654 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,658 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,662 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,665 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,668 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,669 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,669 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:25,670 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:25,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:25,674 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,674 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,679 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,683 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,686 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,689 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 05:13:25,689 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 05:13:25,689 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:25,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:25,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:25,692 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,692 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,692 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,693 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,694 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,695 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,695 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,695 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:25,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:25,696 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:25,696 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,696 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,704 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,710 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,716 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,722 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:25,723 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:25,723 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:25,731 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:25,732 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:25,733 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:25,733 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,734 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,735 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,736 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,736 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,737 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,737 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:25,737 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:25,737 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:25,741 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-30 05:13:25,741 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:25,742 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,743 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,744 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,744 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:25,745 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:25,745 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:25,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:25,751 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:25,755 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,755 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,759 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,763 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,767 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,770 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,770 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,770 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:25,772 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:25,775 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:25,779 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,779 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,784 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,787 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,790 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,794 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,794 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,794 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:25,796 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:25,799 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:25,803 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,803 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,808 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,811 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,815 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,818 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,818 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,819 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:25,820 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:25,824 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:25,827 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,827 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,832 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,847 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,857 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,861 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,861 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,861 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:25,862 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:25,866 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:25,870 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,870 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,874 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,878 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,886 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,886 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,886 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:25,888 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:25,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:25,896 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,896 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,900 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,904 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,908 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,911 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,911 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,912 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:25,913 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:25,916 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,920 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,920 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,925 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,928 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,932 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,935 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,935 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,935 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:25,937 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:25,940 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,944 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,944 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,950 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,954 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,957 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,960 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,961 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,961 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:25,962 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:25,965 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,969 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,969 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,974 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,978 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,981 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,985 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:25,985 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:25,985 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:25,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:25,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:25,994 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:25,994 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:25,999 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,002 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,006 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,009 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,009 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:26,009 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:26,010 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:26,014 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,018 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,018 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,023 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,026 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,030 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,033 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,033 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:26,033 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:26,035 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,038 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,039 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,039 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,047 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,051 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,054 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 05:13:26,054 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 05:13:26,054 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:26,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,057 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,057 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,057 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,058 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,059 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,060 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,060 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,060 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:26,060 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,061 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,061 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,061 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,069 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,075 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,082 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,087 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,088 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:26,088 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:26,101 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,102 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,102 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:26,103 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,103 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,104 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,105 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,106 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,106 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,106 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:26,107 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,108 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,112 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-30 05:13:26,112 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,113 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,114 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,114 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,115 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,116 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,116 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:26,119 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,123 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,128 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,129 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,163 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,196 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,222 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,226 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,227 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,227 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:26,229 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,233 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:26,236 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,236 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,241 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,245 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,248 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,252 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,252 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,252 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:26,254 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:26,257 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:26,260 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,260 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,265 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,268 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,271 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,275 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,275 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,276 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:26,277 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:26,281 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:26,284 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,284 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,288 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,292 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,296 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,299 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,299 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,299 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:26,300 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:26,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:26,307 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,307 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,312 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,315 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,319 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,322 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,322 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,322 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:26,324 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:26,327 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:26,330 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,330 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,335 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,338 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,349 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,363 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,364 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,364 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:26,365 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:26,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:26,373 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,373 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,378 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,381 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,385 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,388 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,388 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,389 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:26,390 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:26,393 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:26,397 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,397 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,402 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,406 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,410 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,413 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,413 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,413 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:26,415 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:26,418 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:26,422 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,423 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,427 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,431 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,435 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,441 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,441 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,441 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:26,443 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:26,449 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:26,453 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,454 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,459 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,462 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,466 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,470 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,470 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,471 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:26,473 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:26,477 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,482 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,482 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,487 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,492 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,495 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,499 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,499 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,499 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:26,501 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,504 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,505 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,505 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,510 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,514 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,517 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,521 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 05:13:26,521 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 05:13:26,521 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:26,522 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,523 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,523 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,523 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,524 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,525 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,526 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,527 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,527 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,527 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:26,527 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,528 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,528 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,528 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,536 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,542 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,548 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,553 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,554 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:26,554 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:26,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,561 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:26,561 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,562 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,563 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,563 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,564 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,565 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:26,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,570 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-30 05:13:26,570 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,571 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,571 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,572 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,573 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,573 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,573 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:26,576 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,580 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,584 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,584 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,589 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,593 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,597 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,601 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,601 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,601 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:26,603 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,607 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:26,612 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,612 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,634 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,638 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,642 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,645 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,645 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,646 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:26,647 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:26,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:26,654 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,654 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,659 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,663 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,666 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,669 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,670 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,670 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:26,671 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:26,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:26,678 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,678 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,683 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,686 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,690 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,693 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,693 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,693 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:26,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:26,698 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:26,702 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,702 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,707 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,710 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,714 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,717 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,717 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,717 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:26,719 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:26,722 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:26,725 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,725 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,730 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,734 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,738 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,741 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,741 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,741 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:26,743 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:26,746 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:26,750 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,750 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,754 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,758 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,761 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,765 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,765 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,765 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:26,766 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:26,770 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:26,773 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,773 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,778 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,782 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,785 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,788 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,789 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,789 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:26,790 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:26,793 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:26,797 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,797 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,802 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,805 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,809 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,813 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,813 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,813 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:26,814 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:26,818 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:26,821 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,822 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,826 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,830 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,833 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,837 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,837 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,837 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:26,838 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:26,841 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,845 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,845 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,875 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,880 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,883 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,887 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,887 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,887 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:26,888 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:26,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,892 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,892 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,897 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,900 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,903 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,906 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 05:13:26,907 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 05:13:26,907 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:26,908 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:26,908 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,909 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,909 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,910 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,910 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,911 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,912 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,912 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,912 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:26,912 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:26,913 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,913 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,913 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,920 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,926 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,932 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,937 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:26,938 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:26,938 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:26,943 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:26,943 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,944 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:26,944 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,945 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,945 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,946 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,946 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,946 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,946 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:26,947 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:26,947 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,951 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-30 05:13:26,951 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:26,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,952 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,953 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,954 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:26,954 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:26,954 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:26,957 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:26,960 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,963 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,963 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,968 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:26,971 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:26,974 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:26,985 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:26,985 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:26,985 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:26,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:26,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:26,994 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:26,994 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:26,999 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,002 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,006 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,010 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,010 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,010 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:27,013 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:27,016 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,020 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,020 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,025 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,028 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,032 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,036 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,036 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,036 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:27,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,043 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,047 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,047 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,051 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,055 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,059 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,062 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,062 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,062 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:27,064 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,067 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,071 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,072 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,077 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,080 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,085 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,088 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,088 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,088 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:27,090 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,094 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,097 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,098 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,102 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,106 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,109 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,114 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,115 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,115 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:27,116 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,120 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,124 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,124 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,130 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,144 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,147 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,151 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,151 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,151 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:27,153 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,156 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,160 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,160 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,165 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,168 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,172 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,175 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,175 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,175 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:27,177 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,184 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,184 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,189 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,192 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,196 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,200 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,200 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,200 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:27,201 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,205 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,208 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,209 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,214 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,221 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,224 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,224 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,224 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:27,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,229 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,233 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,233 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,238 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,242 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,246 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,249 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,249 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,249 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:27,251 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,254 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,255 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,255 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,260 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,263 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,267 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,270 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 05:13:27,270 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 05:13:27,270 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:27,272 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,272 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,273 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,273 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,273 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,274 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,275 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,276 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,276 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,276 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:27,277 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,277 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:27,277 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,277 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,284 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,290 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,296 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,301 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,302 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:27,302 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:27,307 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:27,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:27,308 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:27,308 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,309 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,310 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,311 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,311 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,312 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,312 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:27,312 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:27,313 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:27,317 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-30 05:13:27,317 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,318 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,318 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,319 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,320 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,320 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,320 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:27,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:27,326 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:27,330 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,330 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,338 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,342 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,345 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,349 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,349 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,349 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:27,350 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:27,354 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:27,358 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,358 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,363 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,366 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,369 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,373 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,373 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,374 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:27,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:27,378 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,382 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,383 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,387 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,391 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,394 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,398 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,398 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,398 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:27,400 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,403 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,407 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,407 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,411 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,415 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,418 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,422 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,422 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,422 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:27,424 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,427 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,431 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,431 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,435 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,439 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,443 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,446 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,446 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,446 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:27,448 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,451 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,455 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,455 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,459 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,463 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,466 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,470 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,470 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,470 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:27,471 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,478 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,478 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,483 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,487 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,490 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,494 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,494 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,494 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:27,497 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,500 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,503 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,504 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,508 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,512 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,515 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,519 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,519 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,519 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:27,520 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,523 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,527 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,527 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,532 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,536 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,539 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,543 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,544 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,544 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:27,545 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,549 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,552 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,552 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,557 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,561 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,567 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,570 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,570 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,570 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:27,571 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,575 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,578 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,579 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,583 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,595 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,606 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,610 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,610 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,610 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:27,612 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,615 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,616 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,616 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,621 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,624 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,628 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,631 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 05:13:27,631 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 05:13:27,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:27,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,634 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,634 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,635 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,635 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,636 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,637 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,637 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,637 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:27,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:27,638 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,638 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,645 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,651 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,656 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,662 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:27,662 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:27,663 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:27,667 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:27,668 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:27,668 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:27,669 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,669 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,670 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,671 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,671 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,671 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,672 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:27,672 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:27,673 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:27,676 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-30 05:13:27,677 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,677 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,678 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,679 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,679 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,679 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,680 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:27,683 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:27,686 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:27,690 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,690 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,695 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,699 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,702 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,706 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,706 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,706 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:27,707 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:27,711 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:27,714 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,714 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,719 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,723 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,726 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,730 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,730 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,730 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:27,732 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:27,735 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,739 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,739 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,743 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,747 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,750 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,754 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,754 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,754 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:27,756 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:27,759 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,762 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,763 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,767 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,771 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,774 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,777 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,778 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,778 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:27,779 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:27,782 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,786 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,786 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,791 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,795 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,799 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,803 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,803 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,803 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:27,805 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:27,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,812 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,812 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,817 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,820 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,824 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,827 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,827 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,827 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:27,829 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:27,832 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,836 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,836 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,841 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,844 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,848 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,851 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,851 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,851 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:27,853 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:27,856 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,860 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,860 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,875 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,879 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,885 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,886 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,886 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:27,887 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:27,890 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,894 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,894 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,900 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,903 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,907 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,912 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,912 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,912 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:27,914 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:27,917 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,921 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,921 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,926 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,930 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,933 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,937 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,937 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,937 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:27,939 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:27,942 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,946 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,946 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,955 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,959 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,962 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,962 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,963 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:27,964 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:27,967 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,968 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,968 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:27,973 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,977 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,980 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,984 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 05:13:27,984 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 05:13:27,984 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:27,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:27,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,987 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,987 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:27,988 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,988 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,989 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,990 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:27,990 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:27,990 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:27,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:27,991 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:27,991 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:27,991 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,000 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,008 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,014 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,021 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,022 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:28,022 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:28,027 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:28,028 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,028 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:28,028 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,029 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,030 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,030 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,031 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,031 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,031 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:28,032 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,032 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,036 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-30 05:13:28,036 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,037 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,038 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,039 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,039 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,039 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,040 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:28,043 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,046 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,050 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,051 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,056 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,059 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,063 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,067 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,067 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,068 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:28,069 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,076 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,076 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,083 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,087 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,091 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,094 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,095 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,095 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:28,097 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,100 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,104 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,104 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,109 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,113 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,117 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,120 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,120 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,120 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:28,122 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,125 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,129 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,129 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,143 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,147 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,151 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,154 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,155 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,155 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:28,156 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,160 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,163 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,164 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,169 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,173 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,176 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,179 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,180 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,180 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:28,181 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,185 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,189 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,189 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,194 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,198 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,201 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,204 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,205 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,205 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:28,206 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,209 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:28,213 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,213 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,218 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,222 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,226 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,229 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,229 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,229 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:28,231 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:28,234 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:28,238 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,238 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,243 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,247 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,251 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,254 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,254 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,254 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:28,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:28,259 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:28,263 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,263 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,268 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,271 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,274 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,278 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,278 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,278 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:28,280 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:28,283 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:28,287 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,287 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,292 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,298 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,301 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,305 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,305 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,305 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:28,306 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:28,309 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:28,313 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,313 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,318 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,322 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,325 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,329 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,329 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,329 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:28,330 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:28,334 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:28,334 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,334 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,339 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,343 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,346 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,349 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 05:13:28,350 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 05:13:28,350 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:28,351 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:28,351 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:28,352 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,352 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,353 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,353 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,354 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,355 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,355 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,355 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:28,356 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:28,356 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:28,357 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,357 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,364 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,371 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,376 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,382 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,382 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:28,383 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:28,387 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:28,388 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,388 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:28,389 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,389 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,390 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,391 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,391 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,392 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,392 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:28,392 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,392 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,396 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-30 05:13:28,396 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,397 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,398 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,399 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,399 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,399 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,399 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:28,402 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,406 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,409 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,410 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,419 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,426 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,429 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,433 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,433 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,433 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:28,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,438 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,442 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,442 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,446 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,450 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,454 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,457 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,457 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,457 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:28,458 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,462 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,465 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,465 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,470 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,474 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,477 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,481 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,481 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,481 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:28,483 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,486 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,490 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,491 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,495 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,499 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,502 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,505 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,506 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,506 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:28,507 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,511 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,515 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,515 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,520 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,523 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,527 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,530 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,531 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,531 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:28,532 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,536 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,540 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,540 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,545 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,548 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,551 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,574 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,574 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,574 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:28,575 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,579 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:28,583 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,583 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,592 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,595 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,599 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,603 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,603 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,603 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:28,605 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:28,609 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:28,612 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,613 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,617 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,621 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,625 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,628 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,628 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,628 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:28,630 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:28,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:28,637 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,637 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,653 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,658 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,662 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,666 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,666 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,666 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:28,668 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:28,672 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:28,676 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,676 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,695 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,700 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,705 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,711 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,711 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,711 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:28,713 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:28,719 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:28,726 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,726 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,732 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,736 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,741 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,744 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,744 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,744 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:28,746 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:28,750 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:28,751 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,751 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,756 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,759 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,763 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,766 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 05:13:28,767 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 05:13:28,767 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:28,768 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:28,769 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:28,769 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,769 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,770 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,771 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,772 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,772 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,773 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,773 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:28,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:28,774 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:28,774 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,774 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,781 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,787 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,793 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,799 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:28,800 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:28,800 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:28,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:28,809 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,809 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:28,810 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,811 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,811 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,812 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,813 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,814 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,814 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:28,814 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:28,815 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,821 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-30 05:13:28,821 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:28,822 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,823 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,823 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,824 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:28,824 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:28,824 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:28,827 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:28,833 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,837 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,837 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,842 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,846 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,850 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,853 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,853 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,854 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:28,855 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:28,859 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,863 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,863 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,868 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,871 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,875 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,878 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,879 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,879 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:28,880 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:28,884 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,887 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,887 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,892 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,897 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,902 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,907 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,907 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,907 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:28,910 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:28,916 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,922 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,923 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,928 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,932 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,936 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,939 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,939 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,940 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:28,941 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:28,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,949 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,949 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,954 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,958 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,961 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,965 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,965 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,965 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:28,967 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:28,971 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,975 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:28,975 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:28,980 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,983 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,987 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,990 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:28,990 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:28,991 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:28,992 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:28,995 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,001 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,001 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,014 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,018 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,022 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,025 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,025 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,026 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:29,027 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,031 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,035 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,035 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,040 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,047 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,050 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,051 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,051 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:29,052 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:29,060 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,060 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,065 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,069 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,074 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,078 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,078 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,078 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:29,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:29,084 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:29,087 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,087 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,107 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,111 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,116 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,119 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,119 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,119 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:29,121 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:29,124 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:29,128 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,128 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,133 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,137 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,141 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,144 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,144 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,144 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:29,146 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:29,149 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:29,150 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,150 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,155 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,159 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,162 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,166 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 05:13:29,166 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 05:13:29,167 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:29,168 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:29,169 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:29,169 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,169 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,170 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,171 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,172 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,172 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,172 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,173 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:29,173 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:29,173 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:29,174 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,174 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,182 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,188 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,194 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,200 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,201 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:29,201 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:29,206 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:29,207 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:29,207 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:29,208 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,208 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,209 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,210 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,210 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,210 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,210 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:29,211 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:29,211 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:29,216 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-30 05:13:29,216 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,216 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,218 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,218 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,219 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,219 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:29,222 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:29,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:29,231 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,231 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,236 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,240 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,244 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,248 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,248 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,248 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:29,253 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:29,258 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:29,264 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,265 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,272 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,282 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,286 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,290 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,290 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,290 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:29,292 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:29,295 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:29,300 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,300 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,305 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,309 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,314 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,318 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,318 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,318 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:29,320 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:29,324 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:29,328 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,328 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,347 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,351 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,355 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,359 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,359 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,359 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:29,360 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:29,364 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:29,368 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,369 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,373 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,377 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,380 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,384 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,384 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,384 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:29,386 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:29,389 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:29,393 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,394 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,398 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,402 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,405 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,408 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,409 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,409 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:29,410 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:29,414 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,418 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,418 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,423 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,426 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,430 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,433 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,433 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,433 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:29,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,439 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,443 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,443 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,448 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,452 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,456 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,459 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,459 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,459 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:29,461 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,464 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:29,468 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,468 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,473 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,476 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,480 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,484 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,484 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,484 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:29,486 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:29,489 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:29,493 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,493 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,498 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,502 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,506 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,509 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,510 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,510 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:29,511 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:29,515 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:29,519 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,519 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,524 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,528 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,532 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,535 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,536 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,536 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:29,537 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:29,541 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:29,542 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,542 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,547 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,551 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,555 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,559 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 05:13:29,559 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 05:13:29,559 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:29,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:29,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:29,561 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,561 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,562 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,563 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,565 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,565 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:29,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:29,565 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:29,566 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,566 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,575 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,584 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,593 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,603 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:29,612 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:29,612 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:29,617 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:29,618 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:29,618 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:29,618 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,619 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,620 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,620 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,621 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,621 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,621 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:29,622 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:29,622 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:29,626 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-30 05:13:29,626 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:29,627 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,628 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,629 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,629 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:29,629 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:29,629 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:29,632 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:29,636 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:29,639 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,639 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,644 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,647 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,651 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,654 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,655 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,655 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:29,656 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:29,660 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:29,663 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,663 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,668 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,672 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,676 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,679 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,680 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,680 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:29,681 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:29,685 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:29,688 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,688 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,693 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,697 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,700 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,703 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,704 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,704 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:29,705 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:29,709 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:29,713 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,713 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,717 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,721 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,724 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,728 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,728 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,728 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:29,730 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:29,733 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:29,737 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,737 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,742 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,746 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,750 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,753 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,753 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,754 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:29,755 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:29,758 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:29,762 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,762 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,767 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,771 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,775 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,778 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,778 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,779 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:29,780 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:29,783 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,787 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,787 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,791 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,795 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,798 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,802 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,802 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,802 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:29,804 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:29,807 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,811 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,811 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,816 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,820 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,824 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,838 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,839 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:29,839 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:29,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:29,844 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:29,848 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:29,848 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:29,853 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,857 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:29,861 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,025 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,025 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:30,025 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:30,027 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:30,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,034 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,034 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,038 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,042 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,046 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,049 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,049 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:30,050 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:30,051 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,054 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,058 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,059 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,064 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,067 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,071 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,074 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,075 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:30,075 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:30,076 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,080 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,081 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,087 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,090 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,112 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,116 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 05:13:30,116 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 05:13:30,116 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:30,118 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,118 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,119 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,119 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,119 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,120 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,121 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,122 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,122 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,122 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:30,122 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,123 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:30,123 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,123 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,131 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,138 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,144 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,150 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,151 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:30,151 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:30,156 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:30,157 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:30,157 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:30,157 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,158 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,159 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,159 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,160 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,160 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,160 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:30,161 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:30,161 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:30,165 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-30 05:13:30,165 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,166 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,167 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,167 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,168 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,168 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,168 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:30,171 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:30,174 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:30,178 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,178 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,183 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,187 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,190 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,194 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,194 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,194 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:30,196 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:30,199 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:30,203 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,203 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,208 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,212 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,219 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,222 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,223 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,223 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:30,224 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:30,228 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:30,232 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,232 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,252 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,256 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,260 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,263 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,263 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,263 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:30,265 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:30,268 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:30,271 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,272 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,276 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,280 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,283 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,287 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,287 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,288 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:30,289 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:30,292 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:30,296 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,297 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,304 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,308 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,311 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,316 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,317 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,317 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:30,318 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:30,322 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:30,326 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,326 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,330 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,334 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,338 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,341 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,341 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,341 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:30,343 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:30,346 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:30,350 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,350 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,355 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,360 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,363 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,367 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,367 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,367 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:30,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:30,372 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:30,375 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,375 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,380 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,384 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,387 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,392 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,392 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,392 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:30,393 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:30,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:30,401 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,401 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,405 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,409 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,412 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,416 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,416 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,416 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:30,418 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:30,422 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,425 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,426 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,431 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,434 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,438 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,442 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,442 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,442 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:30,443 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,446 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,450 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,450 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,455 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,485 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,513 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,528 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,529 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,529 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:30,530 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,534 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,535 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,535 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,551 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,575 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,580 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 05:13:30,581 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 05:13:30,582 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:30,583 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,584 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,584 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,585 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,586 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,587 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,588 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,588 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,589 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,589 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:30,590 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,591 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:30,591 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,592 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,600 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,609 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,616 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,624 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:30,625 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:30,626 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:30,631 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:30,632 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:30,633 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:30,633 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,634 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,634 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,635 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,636 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,637 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,637 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:30,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:30,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:30,642 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-30 05:13:30,643 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,643 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,644 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,645 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,646 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,646 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,646 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:30,649 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:30,653 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:30,657 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,657 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,662 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,667 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,676 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,680 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,680 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,681 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:30,682 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:30,686 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:30,690 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,690 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,695 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,699 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,703 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,707 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,707 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,707 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:30,709 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:30,713 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:30,717 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,717 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,724 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,728 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,747 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,751 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,751 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,752 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:30,753 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:30,757 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:30,761 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,761 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,766 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,770 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,773 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,777 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,777 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,777 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:30,779 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:30,782 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:30,786 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,786 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,790 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,794 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,798 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,802 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,802 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,803 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:30,804 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:30,807 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:30,811 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,811 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,816 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,820 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,824 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,828 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,828 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,828 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:30,829 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:30,833 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:30,837 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,837 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,842 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,846 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,850 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,854 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,854 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,854 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:30,855 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:30,859 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:30,862 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,863 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,867 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,872 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,875 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,879 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,879 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,879 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:30,881 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:30,884 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:30,888 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,888 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,893 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,897 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,900 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,904 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,904 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,904 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:30,906 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:30,909 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,913 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,913 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,919 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,923 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,926 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,930 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,930 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,930 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:30,931 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:30,934 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,938 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,938 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,943 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,947 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,954 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,955 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,955 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:30,956 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:30,959 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,960 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,960 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:30,965 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,969 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,973 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,976 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 05:13:30,976 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 05:13:30,977 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:30,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:30,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,979 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,979 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,980 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,980 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,981 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,982 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:30,982 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:30,982 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:30,983 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:30,983 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:30,983 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:30,984 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:30,992 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,002 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,013 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,020 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,021 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:31,021 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:31,026 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:31,026 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,027 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:31,027 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,027 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,028 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,029 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,029 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,030 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,030 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:31,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,031 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,034 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-30 05:13:31,035 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,035 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,036 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,037 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,037 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,038 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,038 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:31,041 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,044 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,048 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,048 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,052 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,056 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,060 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,064 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,064 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,064 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:31,065 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,069 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,073 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,073 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,078 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,082 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,086 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,089 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,090 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,090 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:31,091 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,095 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,099 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,099 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,105 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,109 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,114 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,118 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,118 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,118 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:31,120 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,123 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,127 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,127 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,133 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,137 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,142 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,146 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,146 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,146 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:31,148 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,151 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,155 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,156 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,161 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,166 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,170 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,175 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,175 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,175 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:31,177 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:31,184 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,184 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,189 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,194 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,199 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,203 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,203 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,203 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:31,205 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:31,208 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:31,212 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,212 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,222 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,227 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,232 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,232 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,232 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:31,234 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:31,237 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:31,241 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,241 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,247 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,251 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,256 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,260 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,261 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,261 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:31,262 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:31,266 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:31,270 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,270 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,275 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,280 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,284 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,288 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,289 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,289 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:31,291 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:31,294 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:31,298 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,299 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,312 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,316 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,320 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,323 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,324 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,324 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:31,325 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:31,329 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:31,333 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,333 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,341 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,344 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,348 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,351 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,352 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,352 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:31,353 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:31,357 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:31,358 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,358 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,363 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,367 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,370 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,374 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 05:13:31,374 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 05:13:31,374 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:31,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:31,376 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:31,376 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,376 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,377 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,378 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,379 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,379 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,380 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,380 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:31,380 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:31,380 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:31,381 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,381 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,389 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,395 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,402 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,408 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,409 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:31,409 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:31,414 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:31,415 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,415 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:31,416 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,416 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,417 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,417 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,418 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,418 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,418 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:31,419 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,419 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,423 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-30 05:13:31,423 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,423 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,424 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,425 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,425 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,426 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,426 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:31,429 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,432 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,435 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,436 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,440 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,444 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,447 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,455 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,456 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,456 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:31,458 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,462 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,466 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,467 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,473 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,477 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,481 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,486 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,486 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,486 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:31,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,491 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,496 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,496 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,502 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,506 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,510 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,514 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,514 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,515 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:31,516 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,520 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,525 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,525 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,530 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,535 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,539 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,543 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,543 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,544 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:31,546 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,550 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,554 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,554 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,578 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,581 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,585 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,589 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,589 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,589 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:31,591 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,594 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:31,598 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,599 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,604 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,607 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,611 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,615 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,615 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,615 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:31,616 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:31,620 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:31,624 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,624 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,629 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,633 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,637 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,640 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,641 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,641 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:31,642 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:31,646 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:31,650 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,650 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,655 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,658 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,662 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,666 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,666 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,666 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:31,667 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:31,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:31,680 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,681 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,688 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,697 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,703 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,708 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,709 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,709 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:31,711 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:31,716 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:31,723 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,723 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,728 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,733 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,736 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,740 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,740 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,741 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:31,742 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:31,746 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:31,751 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,751 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,760 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,764 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,767 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,771 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,771 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,771 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:31,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:31,776 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:31,777 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,777 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,790 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,794 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,798 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,801 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 05:13:31,802 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 05:13:31,802 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:31,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:31,804 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:31,804 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,804 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,805 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,805 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,806 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,807 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,807 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,807 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:31,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:31,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:31,808 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,809 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,817 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,823 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,830 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,835 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:31,836 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:31,836 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:31,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:31,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,843 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:31,843 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,844 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,844 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,845 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,846 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,846 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,846 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:31,846 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:31,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,851 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-30 05:13:31,851 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:31,851 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,852 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,853 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,853 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:31,854 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:31,854 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:31,856 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:31,860 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,863 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,864 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,869 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,872 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,876 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,880 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,880 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:31,880 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:31,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:31,885 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,889 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,889 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,894 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,898 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,901 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,905 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,905 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:31,905 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:31,907 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:31,910 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,914 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,914 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,919 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,922 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,926 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,929 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,930 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:31,930 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:31,931 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:31,935 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,938 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,939 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,943 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,947 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,954 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,954 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:31,955 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:31,956 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:31,959 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,963 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,963 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,968 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,974 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,977 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,981 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:31,981 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:31,981 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:31,983 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:31,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:31,990 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:31,990 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:31,995 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,001 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,005 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,024 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,025 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,025 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:32,026 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:32,030 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,034 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,034 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,043 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,047 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,050 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,055 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,055 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,055 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:32,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,060 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,064 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,065 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,069 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,073 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,077 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,081 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,081 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,081 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:32,082 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,090 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,091 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,097 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,103 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,107 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,111 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,111 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,111 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:32,113 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,117 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,121 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,121 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,126 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,130 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,134 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,139 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,139 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,139 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:32,141 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,144 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,149 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,149 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,154 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,158 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,162 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,165 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,166 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,166 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:32,167 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,171 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,171 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,171 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,176 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,180 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,184 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,187 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 05:13:32,187 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 05:13:32,188 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:32,189 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,190 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,190 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,190 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,191 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,192 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,192 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,193 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,193 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,193 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:32,194 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,194 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,194 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,194 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,201 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,208 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,214 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,220 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,221 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:32,221 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:32,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,227 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:32,227 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,228 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,228 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,229 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,230 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,230 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,230 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:32,230 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,231 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,235 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-30 05:13:32,235 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,236 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,236 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,237 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,238 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,238 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,238 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:32,241 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,244 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:32,249 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,249 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,264 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,267 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,271 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,274 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,275 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,275 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:32,276 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:32,280 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:32,283 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,284 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,289 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,292 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,296 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,299 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,300 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,300 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:32,301 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:32,305 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:32,308 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,308 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,313 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,317 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,320 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,324 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,324 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,324 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:32,326 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:32,329 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:32,333 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,333 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,338 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,342 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,345 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,349 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,349 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,350 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:32,351 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:32,354 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:32,358 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,358 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,363 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,367 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,370 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,374 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,374 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,374 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:32,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:32,379 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:32,383 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,383 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,388 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,392 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,395 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,399 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,399 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,399 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:32,400 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:32,404 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,408 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,408 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,412 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,416 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,420 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,424 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,424 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,424 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:32,425 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,429 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,432 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,433 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,437 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,441 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,445 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,448 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,448 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,448 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:32,450 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,453 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,457 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,457 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,462 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,465 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,469 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,472 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,473 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,473 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:32,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,478 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,482 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,482 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,486 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,490 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,494 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,500 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,500 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,500 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:32,502 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,505 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,509 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,510 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,527 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,531 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,535 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,538 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,539 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,539 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:32,540 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,544 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,544 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,544 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,549 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,553 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,557 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,560 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 05:13:32,560 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 05:13:32,560 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:32,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,562 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,562 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,562 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,563 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,565 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,565 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,566 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,566 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:32,566 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,567 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,567 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,575 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,581 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,587 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,593 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,593 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:32,593 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:32,599 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,600 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,600 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:32,600 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,601 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,602 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,602 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,603 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,603 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,603 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:32,603 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,604 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,608 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-30 05:13:32,608 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,609 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,609 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,610 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,611 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,611 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,611 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:32,614 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,617 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:32,621 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,621 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,626 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,630 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,634 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,638 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,639 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,639 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:32,640 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:32,644 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:32,647 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,647 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,652 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,656 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,660 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,663 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,663 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,663 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:32,665 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:32,668 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:32,672 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,672 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,677 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,681 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,685 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,688 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,688 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,689 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:32,690 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:32,693 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:32,697 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,697 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,702 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,706 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,710 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,714 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,714 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,714 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:32,716 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:32,719 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:32,723 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,723 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,728 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,747 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,751 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,755 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,756 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,756 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:32,757 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:32,761 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:32,765 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,765 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,770 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,774 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,778 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,782 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,782 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,782 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:32,784 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:32,788 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,791 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,792 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,796 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,801 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,804 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,809 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,809 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,809 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:32,811 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:32,814 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,818 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,818 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,823 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,827 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,830 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,834 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,834 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,834 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:32,835 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:32,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,843 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,843 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,848 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,852 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,855 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,859 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,859 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,859 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:32,861 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:32,864 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,868 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,868 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,874 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,878 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,885 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,885 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,886 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:32,887 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:32,890 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,894 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,894 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,899 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,903 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,906 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,910 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,910 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,910 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:32,912 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:32,915 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,915 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,916 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,920 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,924 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,928 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,931 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 05:13:32,931 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 05:13:32,932 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:32,933 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:32,934 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,934 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,934 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,935 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,936 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,936 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,937 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,937 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,937 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:32,938 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:32,938 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,939 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,939 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,946 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,952 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,957 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,963 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:32,963 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:32,963 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:32,968 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:32,969 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,969 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:32,969 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,970 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,971 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,971 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,972 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,972 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,972 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:32,973 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:32,973 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,977 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-30 05:13:32,977 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:32,978 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,979 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,979 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,980 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:32,980 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:32,980 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:32,983 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:32,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:32,990 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:32,990 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:32,995 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,021 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,026 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,029 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,030 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,030 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:33,031 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:33,035 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,039 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,039 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,047 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,051 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,054 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,055 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,055 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:33,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,059 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,063 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,063 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,068 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,072 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,075 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,079 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,079 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,079 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:33,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,084 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,087 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,087 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,092 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,096 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,099 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,103 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,103 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,103 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:33,105 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,108 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,112 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,113 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,117 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,122 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,126 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,129 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,129 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,129 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:33,131 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,134 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,138 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,138 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,143 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,147 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,150 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,154 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,154 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,154 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:33,155 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,159 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,162 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,163 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,167 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,171 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,174 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,178 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,178 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,178 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:33,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,183 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,186 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,186 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,191 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,195 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,198 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,202 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,203 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,203 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:33,204 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,207 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:33,211 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,211 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,217 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,220 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,224 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,227 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,227 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,227 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:33,229 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:33,232 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:33,236 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,236 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,258 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,262 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,266 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,269 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,269 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,270 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:33,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:33,274 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:33,278 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,278 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,283 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,286 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,290 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,294 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,294 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,294 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:33,296 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:33,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:33,300 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,300 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,305 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,308 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,312 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,315 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 05:13:33,315 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 05:13:33,316 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:33,317 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:33,317 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:33,318 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,318 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,318 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,319 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,320 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,321 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,321 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,321 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:33,321 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:33,322 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:33,322 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,322 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,333 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,338 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,344 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,350 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,350 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:33,351 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:33,355 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:33,356 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:33,356 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:33,357 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,357 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,358 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,359 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,359 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,359 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,360 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:33,360 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:33,360 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:33,364 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-30 05:13:33,364 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,365 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,366 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,366 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,367 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,367 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,367 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:33,370 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:33,374 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:33,377 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,377 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,382 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,386 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,390 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,393 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,393 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,393 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:33,395 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:33,398 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,402 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,402 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,407 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,410 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,414 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,418 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,418 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,418 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:33,419 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,423 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,427 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,427 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,432 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,435 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,439 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,443 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,443 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,443 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:33,444 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,448 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,451 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,451 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,456 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,460 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,464 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,467 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,467 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,467 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:33,469 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,472 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,476 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,476 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,481 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,485 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,488 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,492 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,492 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,492 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:33,494 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,497 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,500 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,501 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,521 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,525 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,529 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,532 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,532 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,532 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:33,534 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,537 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,541 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,541 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,546 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,549 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,553 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,561 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,562 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,562 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:33,563 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,571 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,571 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,587 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,591 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,595 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,599 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,599 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,599 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:33,600 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,603 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:33,607 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,607 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,612 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,615 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,619 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,622 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,622 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,622 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:33,624 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:33,627 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:33,631 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,631 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,636 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,640 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,643 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,647 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,647 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,647 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:33,648 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:33,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:33,655 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,655 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,660 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,664 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,667 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,671 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,671 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,671 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:33,673 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:33,676 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:33,676 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,677 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,682 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,685 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,689 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,693 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 05:13:33,693 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 05:13:33,693 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:33,694 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:33,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:33,695 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,695 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,696 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,697 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,697 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,698 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,698 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,698 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:33,699 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:33,699 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:33,700 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,700 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,707 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,713 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,719 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,725 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:33,725 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:33,726 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:33,730 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:33,731 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:33,731 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:33,731 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,732 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,733 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,734 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,734 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,734 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,735 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:33,735 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:33,735 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:33,739 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-30 05:13:33,739 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:33,740 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,741 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,741 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,742 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:33,742 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:33,742 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:33,745 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:33,749 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:33,752 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,753 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,758 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,762 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,766 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,770 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,770 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,770 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:33,771 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:33,774 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,778 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,778 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,783 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,786 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,790 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,793 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,794 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,794 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:33,795 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:33,799 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,802 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,802 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,807 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,811 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,814 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,818 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,818 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,818 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:33,820 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:33,823 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,827 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,827 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,832 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,835 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,839 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,843 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,843 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,843 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:33,845 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:33,848 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,852 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,852 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,856 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,860 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,863 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,867 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,867 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,867 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:33,869 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:33,872 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,876 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,876 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,881 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,885 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,888 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,892 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,892 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,892 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:33,893 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:33,897 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,901 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,902 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,935 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,940 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,945 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,949 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,949 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,949 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:33,951 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:33,955 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,959 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,959 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,964 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,968 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,972 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,977 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,977 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:33,977 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:33,978 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:33,982 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:33,986 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:33,986 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:33,991 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,995 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:33,999 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,003 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,003 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:34,004 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:34,005 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:34,009 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,012 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,012 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,039 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,048 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,051 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,052 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:34,052 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:34,054 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,057 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,061 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,062 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,067 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,071 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,075 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,079 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,079 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:34,079 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:34,081 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,085 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,085 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,086 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,091 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,095 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,100 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,104 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 05:13:34,104 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 05:13:34,104 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:34,106 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,106 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,107 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,107 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,108 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,108 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,109 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,110 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,110 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,110 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:34,110 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,111 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,111 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,111 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,120 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,127 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,135 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,141 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,143 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:34,143 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:34,147 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,148 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,149 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:34,149 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,149 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,150 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,151 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,151 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,151 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,152 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:34,152 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,152 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,156 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-30 05:13:34,156 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,157 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,158 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,158 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,159 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,159 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,159 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:34,162 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,165 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,169 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,169 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,174 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,177 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,181 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,185 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,185 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,185 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:34,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,189 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,193 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,193 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,198 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,202 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,205 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,209 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,209 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,209 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:34,211 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:34,217 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,218 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,222 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,226 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,230 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,234 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,234 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,234 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:34,236 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:34,239 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:34,243 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,243 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,263 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,267 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,271 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,276 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,277 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,277 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:34,278 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:34,282 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:34,286 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,286 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,291 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,295 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,298 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,302 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,302 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,303 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:34,304 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:34,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:34,312 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,312 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,317 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,322 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,326 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,330 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,330 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,330 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:34,332 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:34,335 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:34,340 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,340 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,345 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,349 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,352 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,356 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,356 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,356 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:34,358 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:34,361 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:34,365 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,365 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,370 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,374 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,378 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,382 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,382 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,382 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:34,384 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:34,388 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:34,392 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,393 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,397 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,401 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,405 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,408 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,408 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,409 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:34,410 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:34,414 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,418 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,418 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,423 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,427 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,431 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,434 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,435 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,435 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:34,436 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,444 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,444 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,449 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,453 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,456 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,460 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,460 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,461 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:34,462 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,466 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,467 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,472 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,476 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,479 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,483 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 05:13:34,483 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 05:13:34,483 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:34,484 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,485 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,485 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,486 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,486 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,487 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,488 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,489 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,489 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,489 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:34,489 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,490 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,490 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,490 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,497 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,504 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,510 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,522 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,532 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:34,533 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:34,538 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,539 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,539 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:34,539 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,540 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,541 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,541 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,542 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,542 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,542 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:34,543 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,543 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,547 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-30 05:13:34,547 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,547 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,548 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,548 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,549 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,549 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,549 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:34,552 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,555 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,559 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,559 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,564 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,568 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,571 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,575 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,575 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,575 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:34,577 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,580 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,584 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,584 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,597 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,601 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,605 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,608 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,608 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,609 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:34,610 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,614 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:34,617 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,618 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,622 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,626 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,630 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,633 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,634 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,634 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:34,635 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:34,639 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:34,643 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,643 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,648 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,652 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,656 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,659 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,659 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,659 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:34,661 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:34,664 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:34,668 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,669 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,674 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,677 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,681 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,684 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,685 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,685 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:34,686 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:34,690 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:34,693 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,693 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,698 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,702 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,705 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,709 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,709 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,709 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:34,710 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:34,714 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:34,718 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,718 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,723 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,727 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,730 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,734 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,734 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,734 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:34,736 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:34,739 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:34,743 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,743 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,760 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,766 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,773 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,779 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,779 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,779 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:34,781 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:34,784 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:34,789 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,789 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,794 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,798 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,801 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,805 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,805 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,805 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:34,806 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:34,810 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,814 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,814 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,819 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,822 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,826 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,829 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,830 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,830 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:34,831 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:34,835 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,839 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,839 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,844 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,848 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,852 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,856 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,856 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,856 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:34,858 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:34,861 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,862 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,862 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,867 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,871 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,875 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,878 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 05:13:34,879 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 05:13:34,879 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:34,880 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:34,880 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,881 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,881 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,882 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,883 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,884 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,884 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,884 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:34,885 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:34,885 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,885 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,886 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,894 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,900 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,907 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,913 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:34,914 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:34,915 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:34,919 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:34,920 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,920 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:34,921 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,921 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,922 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,923 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,923 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,923 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,924 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:34,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:34,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,928 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-30 05:13:34,929 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:34,929 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,930 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,931 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,931 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:34,932 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:34,932 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:34,935 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:34,938 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,942 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,942 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,948 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,951 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,955 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,959 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,959 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:34,959 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:34,961 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:34,964 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,968 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,968 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,973 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,977 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,981 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,985 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:34,985 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:34,985 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:34,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:34,990 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:34,994 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:34,994 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:34,999 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,002 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,006 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,010 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,010 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,010 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:35,012 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:35,015 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:35,019 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,019 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,032 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,036 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,040 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,044 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,044 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,044 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:35,046 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:35,049 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:35,053 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,053 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,058 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,062 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,066 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,069 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,069 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,070 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:35,071 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:35,075 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:35,078 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,079 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,083 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,091 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,095 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,099 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,099 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,100 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:35,101 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:35,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:35,108 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,108 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,113 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,117 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,121 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,124 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,125 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,125 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:35,126 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:35,130 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:35,134 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,134 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,139 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,143 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,147 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,152 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,152 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,152 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:35,153 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:35,157 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:35,161 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,161 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,166 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,170 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,173 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,177 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,177 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,177 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:35,179 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:35,182 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:35,186 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,186 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,190 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,194 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,198 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,202 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,202 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,202 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:35,203 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:35,207 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:35,210 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,211 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,216 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,219 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,223 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,227 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,227 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,227 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:35,228 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:35,232 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:35,232 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,232 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,237 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,241 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,245 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,249 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 05:13:35,249 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 05:13:35,249 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:35,250 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:35,251 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:35,251 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,252 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,252 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,253 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,254 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,254 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,255 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:35,255 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:35,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:35,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:35,256 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,256 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,263 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,269 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,277 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,287 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,292 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:35,293 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:35,298 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:35,299 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:35,299 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 05:13:35,300 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,301 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,301 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,302 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,303 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,303 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:35,303 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 05:13:35,304 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 05:13:35,304 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:35,308 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-30 05:13:35,309 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,309 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,310 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,311 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,312 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,312 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:35,312 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 05:13:35,315 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 05:13:35,319 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:35,323 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,324 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,329 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,333 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,337 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,341 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,341 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,342 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 05:13:35,343 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 05:13:35,347 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:35,351 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,351 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,356 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,360 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,364 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,368 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,369 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,369 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 05:13:35,370 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 05:13:35,374 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:35,378 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,378 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,383 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,387 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,391 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,395 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,395 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,395 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 05:13:35,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 05:13:35,402 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:35,406 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,406 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,412 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,416 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,421 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,425 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,425 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,425 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 05:13:35,427 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 05:13:35,430 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:35,434 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,435 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,440 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,445 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,449 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,453 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,453 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,454 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 05:13:35,455 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 05:13:35,459 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:35,463 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,464 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,469 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,476 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,480 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,484 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,484 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,484 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 05:13:35,486 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 05:13:35,489 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:35,493 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,494 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,499 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,503 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,507 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,511 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,511 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,511 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 05:13:35,513 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 05:13:35,517 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:35,521 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,521 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,526 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,530 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,533 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,538 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,538 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,539 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 05:13:35,540 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 05:13:35,543 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:35,548 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,548 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,563 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,567 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,570 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,574 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,574 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,574 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 05:13:35,576 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 05:13:35,579 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:35,583 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,583 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,588 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,592 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,595 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,598 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,599 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,599 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 05:13:35,600 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 05:13:35,603 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:35,607 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,607 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,612 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,615 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,620 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,623 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,623 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,623 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 05:13:35,625 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 05:13:35,628 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:35,629 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,629 [flexgen.py:108 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 05:13:35,634 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,638 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,641 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,645 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 05:13:35,645 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 05:13:35,645 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 05:13:35,646 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 05:13:35,647 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:35,647 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,647 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,648 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,649 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,649 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,650 [flexgen.py:129 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 05:13:35,650 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 05:13:35,651 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 05:13:35,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 05:13:35,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 05:13:35,652 [flexgen.py:107 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 05:13:35,652 [flexgen.py:108 in new_forward] DEBUG - kwargs: {}
2023-10-30 05:13:35,659 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,665 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,670 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,676 [flexgen.py:129 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'torch.Tensor'>: torch.Size([1, 1, 50272])
2023-10-30 05:13:35,677 [flexgen.py:141 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 05:13:35,677 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 05:13:35,682 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-30 05:13:35,682 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:13:35,682 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-30 05:13:35,682 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:13:35,683 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-30 05:13:35,683 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:13:35,683 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-30 05:13:35,683 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-30 05:13:35,691 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-30 05:13:35,692 [flexgen.py:66 in layer_reset] DEBUG - lm_head from flexgen to old.
