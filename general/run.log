2023-10-11 12:43:48,710 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp0a4mmsrh
2023-10-11 12:43:48,711 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp0a4mmsrh/_remote_module_non_scriptable.py
2023-10-11 12:43:49,147 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-11 12:43:49,213 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:50,758 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 12:43:51,049 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 12:43:51,839 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:51,918 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:43:51,960 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,037 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:43:52,037 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:43:52,044 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:43:52,044 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:43:52,045 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:43:52,046 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:43:52,046 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:43:52,047 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:43:52,048 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:43:52,049 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:43:52,050 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:43:52,051 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:43:52,052 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:43:52,053 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:43:52,054 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:43:52,055 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:43:52,056 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:43:52,056 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:43:52,056 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:43:52,058 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:43:52,059 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:43:52,127 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:52,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:52,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,304 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:52,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:52,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:52,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,338 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:52,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,347 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:52,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:52,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:52,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,372 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:52,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,380 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:52,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:52,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:52,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:52,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,411 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:52,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:52,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:52,422 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:52,427 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:43:52,427 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:43:52,437 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:43:52,479 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:52,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,617 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:43:52,618 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:52,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:52,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:52,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:52,621 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:52,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:52,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,626 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:43:52,626 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:52,627 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:52,628 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:52,629 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:52,629 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:52,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:52,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,637 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,641 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,642 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,642 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:52,654 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:52,658 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:52,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:52,667 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:52,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,676 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:52,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:52,685 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:52,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:52,695 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:52,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,705 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,705 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,705 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:52,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:52,732 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:52,737 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:52,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:52,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,751 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,751 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:52,757 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:52,762 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:52,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:52,772 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,772 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:52,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,780 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,781 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:52,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:52,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:52,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:52,799 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:52,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,808 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:52,815 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:52,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:52,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:52,827 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:52,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,836 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,836 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:52,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:52,846 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:52,850 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:52,854 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:52,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,863 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,863 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:52,868 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:52,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:52,878 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:52,883 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,883 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:52,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,892 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:52,899 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:52,904 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:52,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:52,915 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:52,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,920 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,924 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,924 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:52,929 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:52,934 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:52,938 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:52,942 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:52,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,951 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,951 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:52,957 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:52,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:52,966 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:52,970 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,970 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:52,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,977 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,977 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,977 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:52,982 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:52,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:52,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:52,996 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,996 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:52,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:52,998 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,998 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,999 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,000 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,003 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:53,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,004 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:53,004 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,004 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,020 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,034 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,048 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,061 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:43:53,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,074 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,074 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,076 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,076 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,077 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,077 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,082 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 12:43:53,082 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,082 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,082 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,084 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,085 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,085 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,096 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,096 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,102 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,112 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,116 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,124 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:53,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:53,134 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:53,137 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:53,142 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:53,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,150 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,150 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:53,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:53,160 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:53,164 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:53,168 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:53,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,176 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,176 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:53,181 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:53,185 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:53,189 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:53,193 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:53,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,202 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,202 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:53,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:53,211 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:53,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:53,239 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:53,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,248 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,248 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:53,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:53,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:53,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:53,292 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,292 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:53,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,301 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,301 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:53,307 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:53,311 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:53,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:53,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:53,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,335 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,336 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:53,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:53,345 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:53,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:53,352 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:53,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,361 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,361 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,361 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:53,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:53,371 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:53,374 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:53,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:53,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,390 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,390 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,390 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:53,395 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:53,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:53,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:53,406 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:53,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,415 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:53,420 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:53,424 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:53,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:53,433 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,433 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:53,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:53,444 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:53,448 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:53,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:53,455 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,455 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:53,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,458 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,458 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,459 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,461 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,463 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,463 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,463 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,476 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,486 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,495 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:53,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,511 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,512 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,514 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,519 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 12:43:53,519 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,520 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,522 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,533 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,533 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,533 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,539 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,542 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,550 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,550 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,558 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,559 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,559 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:53,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:53,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:53,580 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:53,584 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,584 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:53,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,593 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,593 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:53,598 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:53,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:53,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:53,610 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:53,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,618 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,618 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:53,623 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:53,627 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:53,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:53,635 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,635 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:53,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,644 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,644 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:53,649 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:53,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:53,657 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:53,661 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:53,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,670 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,670 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,670 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:53,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:53,679 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:53,683 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:53,686 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:53,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:53,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:53,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:53,707 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:53,711 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:53,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,720 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,720 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:53,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:53,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:53,746 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:53,751 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:53,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,759 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,759 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,759 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:53,764 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:53,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:53,771 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:53,775 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,775 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:53,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,783 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,784 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,784 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:53,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:53,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:53,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:53,801 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:53,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,809 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,809 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:53,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:53,825 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:53,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:53,872 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:53,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,879 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,879 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,879 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:53,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:53,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:53,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:53,896 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:53,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,898 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,898 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,898 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,899 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,900 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,902 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,903 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,903 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,904 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,916 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,926 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,935 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,944 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:53,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,952 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,952 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,955 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,960 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 12:43:53,960 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,974 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,974 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,986 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,995 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:53,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,003 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,003 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,004 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,010 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,018 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,022 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,031 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,031 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,031 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,037 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,052 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,052 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,060 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,060 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,090 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,090 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,098 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,098 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,098 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,103 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,117 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,117 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,125 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,125 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,125 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,166 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,173 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,183 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,183 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,190 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,200 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,205 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,205 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,214 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,214 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,214 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,220 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,235 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,244 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,244 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:54,250 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:54,255 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:54,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:54,265 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:54,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,273 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:54,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:54,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:54,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:54,291 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:54,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,299 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,299 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,300 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:54,304 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:54,308 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:54,314 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:54,318 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:54,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,324 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,324 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,324 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:54,331 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:54,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:54,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:54,345 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:54,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,348 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,348 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:54,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:54,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:54,351 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:54,351 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:54,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,353 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,353 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,353 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:54,363 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:54,370 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:54,377 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:54,385 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:54,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:54,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,392 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:54,392 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:54,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:54,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:54,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:54,395 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:54,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,400 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 12:43:54,400 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:54,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:54,401 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:54,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:54,402 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:54,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,412 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,413 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,413 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:54,418 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:54,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:54,426 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:54,429 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:54,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,438 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,447 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,455 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,464 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,464 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,464 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,474 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,478 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,481 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,490 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,490 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,507 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,507 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,515 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,515 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,532 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,541 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,550 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,554 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,557 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,565 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,566 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,575 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,583 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,591 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,591 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,591 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,599 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,603 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,607 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,618 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,626 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,626 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:54,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:54,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:54,639 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:54,643 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:54,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,652 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:54,657 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:54,661 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:54,664 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:54,668 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:54,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,676 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:54,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:54,685 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:54,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:54,693 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,694 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:54,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,699 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,699 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:54,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:54,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:54,712 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:54,716 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:54,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,718 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,718 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,718 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:54,719 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:54,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:54,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:54,722 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:54,722 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,723 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,723 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,723 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:54,735 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:54,743 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:54,751 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:54,758 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:54,759 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:54,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,766 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:54,766 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,766 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:54,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:54,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:54,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:54,769 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:54,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,773 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 12:43:54,774 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:54,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:54,775 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:54,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:54,776 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,776 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:54,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,786 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,786 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:54,792 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:54,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:54,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:54,804 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:54,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,814 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,814 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,831 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,839 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,848 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,848 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,848 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,854 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,866 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,875 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,875 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,880 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,892 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,901 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,902 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,902 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,915 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,919 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,920 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,927 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,928 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,937 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,945 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,953 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,954 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,967 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,971 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,980 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,980 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,980 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,986 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,994 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,998 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,998 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,999 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,007 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,007 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,017 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,021 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,025 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,034 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,034 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,045 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,053 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,062 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,063 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,063 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,068 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,089 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,089 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,095 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,095 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,095 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,108 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,112 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,115 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,115 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,115 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,118 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,128 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,129 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,129 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,129 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,139 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,148 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,158 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,167 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,175 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,175 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,178 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,183 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 12:43:55,183 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,184 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,185 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,185 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,196 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,196 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,196 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,201 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,209 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,214 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,223 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,223 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,223 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:55,228 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:55,233 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:55,236 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:55,240 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:55,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,249 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,249 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,249 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:55,254 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:55,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:55,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:55,271 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:55,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,279 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,280 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:55,285 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:55,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:55,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:55,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:55,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,305 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,306 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,306 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:55,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:55,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:55,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:55,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:55,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,336 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,337 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,337 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:55,343 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:55,347 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:55,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:55,356 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:55,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,365 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,365 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,365 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:55,370 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:55,375 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:55,378 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:55,382 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:55,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,391 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,391 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:55,397 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:55,401 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:55,405 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:55,409 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:55,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,417 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,417 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,417 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,427 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,430 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,435 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,443 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,443 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,449 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,453 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,461 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,470 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,470 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,471 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,475 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,488 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,494 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,494 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,508 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,512 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,512 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,514 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,514 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,514 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,519 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,521 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,521 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,521 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,531 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,539 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,547 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,555 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,555 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,562 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,563 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,564 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,564 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,565 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,570 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 12:43:55,570 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,572 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,572 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,583 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,583 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,605 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,613 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,613 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,614 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:55,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:55,624 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:55,628 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:55,632 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:55,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,640 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,641 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:55,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:55,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:55,654 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:55,658 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,658 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:55,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,666 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:55,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:55,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:55,679 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:55,686 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:55,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:55,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:55,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:55,712 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:55,716 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:55,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,725 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,725 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,725 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:55,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:55,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:55,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:55,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:55,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,750 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,750 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:55,756 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:55,760 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:55,764 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:55,767 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:55,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,776 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:55,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:55,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:55,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:55,794 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:55,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,803 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,803 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,808 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,812 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,816 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,820 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,830 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,830 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,845 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,849 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,858 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,858 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,868 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,872 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,877 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,877 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,883 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,900 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,903 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,903 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,903 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,904 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,908 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,908 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,910 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,910 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,919 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,928 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,936 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,944 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,951 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,951 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,954 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,959 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 12:43:55,959 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,961 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,972 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,982 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,991 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:55,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,017 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,021 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,030 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,030 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,030 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,051 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,060 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,060 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,077 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,086 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,086 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,086 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,091 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,104 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,104 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,112 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,113 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,113 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,119 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,132 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,141 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,141 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,146 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,150 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,159 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,168 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,168 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,168 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:56,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:56,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:56,182 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:56,186 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:56,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,194 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,194 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:56,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:56,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:56,209 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:56,213 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,213 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:56,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,221 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,222 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,222 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:56,227 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:56,231 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:56,235 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:56,239 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:56,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,248 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,248 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:56,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:56,258 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:56,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:56,267 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:56,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,273 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,273 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,273 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:56,283 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:56,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:56,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:56,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:56,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,300 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,300 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,300 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:56,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:56,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:56,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:56,303 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:56,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,304 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,305 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,305 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:56,315 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:56,323 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:56,338 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:56,362 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:56,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:56,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,372 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:56,372 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:56,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:56,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:56,374 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:56,374 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:56,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,379 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 12:43:56,379 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:56,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:56,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:56,381 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:56,382 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,382 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:56,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,392 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,392 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:56,398 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:56,403 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:56,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:56,413 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:56,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,421 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,422 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,427 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,432 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,440 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,449 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,449 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,450 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,455 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,470 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,479 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,479 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,479 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,498 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,512 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,512 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,530 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,530 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,538 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,539 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,539 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,548 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,552 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,557 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,565 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,565 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,575 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,596 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,605 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,605 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,605 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:56,615 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:56,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:56,625 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:56,629 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:56,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,638 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,638 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,638 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:56,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:56,648 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:56,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:56,657 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:56,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,666 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:56,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:56,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:56,680 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:56,683 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:56,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,692 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,692 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,692 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:56,698 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:56,702 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:56,707 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:56,711 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:56,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,716 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,717 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:56,722 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:56,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:56,731 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:56,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:56,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,738 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,738 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:56,739 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:56,740 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:56,741 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:56,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:56,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,743 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,743 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,743 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:56,754 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:56,762 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:56,771 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:56,779 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:56,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:56,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,787 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:56,787 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:56,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:56,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:56,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:56,789 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:56,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,794 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 12:43:56,794 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:56,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:56,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:56,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:56,797 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:56,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,807 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,808 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:56,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:56,817 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:56,822 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:56,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:56,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,834 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,835 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,849 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,853 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,862 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,862 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,872 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,876 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,881 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,886 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,890 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,890 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,891 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,905 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,909 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,918 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,932 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,936 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,945 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,946 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,946 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,955 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,973 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,973 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,973 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,983 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,992 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,006 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,015 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,020 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,029 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,029 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,047 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,056 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,056 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,061 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,066 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,070 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,074 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,083 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,094 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,114 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,114 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,114 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:57,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:57,131 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:57,136 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:57,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,138 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,138 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,138 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:57,139 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:57,140 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:57,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:57,142 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:57,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,143 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,144 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,144 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:57,156 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:57,167 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:57,178 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:57,190 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:57,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:57,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,198 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:57,198 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:57,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:57,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:57,200 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:57,201 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,201 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:57,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,205 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 12:43:57,205 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:57,206 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:57,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:57,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:57,208 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:57,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,218 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,218 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:57,224 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:57,229 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:57,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:57,238 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:57,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,247 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,247 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,247 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:57,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:57,258 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:57,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:57,267 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,268 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:57,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,277 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,277 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,277 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:57,283 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:57,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:57,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:57,298 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:57,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,307 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,307 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,307 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:57,314 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:57,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:57,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:57,334 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:57,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,343 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,344 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,344 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:57,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:57,354 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:57,358 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:57,363 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:57,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,373 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,373 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:57,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:57,383 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:57,388 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:57,393 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:57,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,401 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,401 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:57,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:57,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:57,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:57,420 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:57,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,429 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,429 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,434 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,447 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,448 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,456 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,456 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,456 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,461 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,473 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,474 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,482 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,482 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,482 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,488 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,492 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,496 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,500 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,508 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,508 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,514 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,529 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,534 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,534 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,535 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,540 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:57,544 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:57,549 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:57,553 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:57,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,555 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,556 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,556 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:57,556 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:57,557 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:57,558 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:57,559 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,559 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:57,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,560 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,560 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,560 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:57,571 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:57,580 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:57,594 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:57,602 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:57,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:57,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,610 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:57,610 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,610 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:57,610 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:57,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:57,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:57,612 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:57,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,617 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 12:43:57,617 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,617 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:57,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:57,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:57,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:57,620 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:57,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,631 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,631 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:57,637 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:57,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:57,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:57,650 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:57,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,659 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,660 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,660 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:57,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:57,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:57,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:57,679 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:57,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,688 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,688 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:57,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:57,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:57,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:57,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:57,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,744 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,744 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,745 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:57,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:57,755 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:57,760 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:57,764 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,764 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:57,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,773 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,773 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:57,778 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:57,783 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:57,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:57,791 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:57,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,800 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,800 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:57,805 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:57,810 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:57,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:57,846 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,847 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:57,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,855 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,856 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,856 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:57,861 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:57,866 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:57,870 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:57,874 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:57,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,883 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,901 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,911 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,916 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,921 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,940 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,941 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,946 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,950 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,958 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,959 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,967 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,967 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,967 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,977 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,986 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,987 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,992 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,992 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,992 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,997 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,006 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,010 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:58,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,013 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,013 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,015 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,016 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,016 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,018 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,018 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,018 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,031 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,038 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,047 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,055 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,056 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,063 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,063 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,063 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,064 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,066 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,071 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 12:43:58,071 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,071 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,074 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,084 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,084 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,085 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,092 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,098 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,117 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,118 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,118 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,127 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,132 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,136 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,145 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,145 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,145 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:58,151 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:58,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:58,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:58,164 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,164 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:58,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,172 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,173 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:58,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:58,187 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:58,191 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:58,195 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:58,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,204 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,204 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,204 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:58,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:58,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:58,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:58,223 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,223 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:58,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,231 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,231 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,231 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:58,236 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:58,241 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:58,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:58,249 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,249 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:58,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,257 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,257 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,257 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:58,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:58,266 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:58,270 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:58,275 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:58,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,284 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,284 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:58,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:58,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:58,297 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:58,302 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:58,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,309 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,310 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:58,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:58,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:58,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:58,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:58,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,335 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,335 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:58,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:58,346 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:58,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:58,354 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,354 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:58,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,362 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,363 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,363 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:58,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:58,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:58,377 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:58,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:58,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,386 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,386 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,386 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:58,392 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,396 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,404 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,406 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,407 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,409 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,410 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,411 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,411 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,411 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,420 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,428 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,438 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,450 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,459 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,459 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,459 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,461 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,467 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 12:43:58,467 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,467 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,470 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,480 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,480 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,486 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,498 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,498 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,506 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,506 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,506 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,524 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,532 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,533 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,533 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:58,538 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:58,542 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:58,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:58,551 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:58,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,559 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,560 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,560 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:58,599 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:58,604 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:58,608 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:58,612 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:58,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,621 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,621 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:58,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:58,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:58,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:58,639 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:58,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,653 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:58,658 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:58,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:58,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:58,673 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:58,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,682 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,683 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,683 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:58,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:58,693 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:58,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:58,705 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:58,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,720 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,720 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:58,729 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:58,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:58,741 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:58,747 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:58,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,760 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,760 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,761 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:58,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:58,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:58,780 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:58,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:58,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,796 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,796 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:58,802 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:58,806 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:58,810 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:58,814 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,814 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:58,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,823 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,823 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:58,828 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:58,838 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:58,843 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:58,847 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:58,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,853 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,853 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,853 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:58,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,871 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,874 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,874 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,876 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,877 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,878 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,880 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,881 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,881 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,891 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,900 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,907 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,914 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,924 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,924 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,924 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,926 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,927 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,933 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 12:43:58,933 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,934 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,935 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,935 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,936 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,936 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,947 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,947 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,947 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,957 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,966 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:58,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,974 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,974 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,979 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,984 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,988 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,993 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:58,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,020 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,024 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,032 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,032 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,032 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,037 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,042 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,050 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,059 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,059 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,059 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,068 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,082 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,082 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,090 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,090 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,090 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,116 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,120 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,120 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,128 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,128 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,140 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,145 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,154 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,162 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,168 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,180 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,184 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,192 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,192 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,192 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,211 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,219 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,219 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,224 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,228 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,232 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,236 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,244 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,244 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:59,249 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:59,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:59,257 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:59,262 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:59,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,267 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,267 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,267 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:59,272 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:59,276 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:59,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:59,284 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:59,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,286 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,287 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:59,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:59,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:59,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:59,290 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:59,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,291 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,291 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,291 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:59,301 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:59,308 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:59,316 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:59,323 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:59,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:59,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,332 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:59,333 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,333 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:59,334 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:59,334 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:59,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:59,335 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:59,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,340 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 12:43:59,340 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:59,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:59,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:59,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:59,342 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:59,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,351 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,352 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:59,357 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:59,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:59,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:59,371 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:59,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,379 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,379 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:59,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:59,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:59,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:59,397 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:59,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,405 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,405 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,405 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,419 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,424 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,431 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,432 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,432 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,437 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,446 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,450 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,458 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,458 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,464 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,473 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,477 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,485 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,485 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,485 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,511 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,511 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,520 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,524 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,529 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,536 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,536 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,536 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,550 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,554 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,554 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,562 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,562 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,562 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,568 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,572 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,576 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,580 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,589 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,589 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,589 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,610 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,611 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,619 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,619 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:59,625 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:59,630 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:59,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:59,639 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:59,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,645 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,645 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:59,656 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:59,661 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:59,665 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:59,669 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:59,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,671 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,672 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:59,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:59,673 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:59,674 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:59,675 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:59,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,676 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,676 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:59,685 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:59,693 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:59,700 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:59,707 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:59,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:59,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,715 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:59,715 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,715 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:59,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:59,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:59,717 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:59,717 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:59,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,722 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 12:43:59,722 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,722 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:59,723 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:59,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:59,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:59,725 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,725 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:59,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,735 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,735 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:59,740 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:59,745 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:59,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:59,753 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:59,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,761 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,761 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,762 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:59,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:59,771 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:59,775 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:59,779 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:59,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,787 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,788 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,801 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,805 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,805 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,813 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,813 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,818 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,827 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,831 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,839 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,839 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,839 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,849 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,853 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,857 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,865 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,865 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,865 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,871 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,879 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,884 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,892 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,902 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,906 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,910 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,910 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,918 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,932 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,937 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,945 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,945 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,945 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,956 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,966 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,975 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,975 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,980 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,985 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,994 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,002 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:00,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:00,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:44:00,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:00,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,026 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,026 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,026 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:00,031 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:00,036 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:00,040 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:00,045 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:44:00,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:00,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,047 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,047 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:00,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:00,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:00,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:00,050 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:00,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,051 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,051 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,051 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:00,061 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:00,070 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:00,078 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:00,085 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:00,085 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:00,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,098 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:00,098 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:00,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:00,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:00,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:00,101 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:00,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,106 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 12:44:00,106 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:00,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:00,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:00,108 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:00,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:00,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,119 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,119 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,120 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:00,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:00,131 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:00,135 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:00,140 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:00,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,149 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,149 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:00,154 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:00,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:00,164 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:00,168 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:00,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,177 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,177 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:00,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:00,187 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:00,192 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:00,196 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:00,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,205 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,205 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:00,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:00,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:00,220 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:00,225 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:00,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,233 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,234 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:00,239 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:00,243 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:00,247 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:00,252 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:00,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,260 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,260 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:00,265 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:00,270 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:00,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:00,278 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:00,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,287 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,287 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:00,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:00,298 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:00,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:00,307 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:00,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,315 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,316 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,316 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:00,320 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:00,325 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:00,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:00,333 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:00,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,342 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:00,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:00,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:00,357 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:00,361 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:00,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,370 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,370 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,370 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:00,376 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:00,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:00,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:00,388 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:00,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,398 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,398 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,404 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:00,413 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:00,418 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:00,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,424 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,424 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:00,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:00,463 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:00,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:00,473 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:00,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,476 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,476 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:00,477 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:00,477 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:00,478 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:00,479 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:00,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,481 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,481 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,481 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:00,490 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:00,498 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:00,505 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:00,513 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:00,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:00,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,521 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:00,521 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,522 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:00,522 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:00,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:00,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:00,524 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:00,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,529 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 12:44:00,529 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,529 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:00,530 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:00,531 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:00,531 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:00,532 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:00,536 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,543 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,543 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:00,549 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:00,554 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:00,559 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:00,563 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:00,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,573 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,573 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,573 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:00,578 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:00,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:00,587 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:00,596 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:00,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,605 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,606 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:00,612 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:00,616 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:00,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:00,626 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:00,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,635 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,636 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,636 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:00,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:00,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:00,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:00,655 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,655 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:00,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,664 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,665 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,665 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:00,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:00,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:00,680 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:00,685 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:00,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:00,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:00,705 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:00,710 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:00,715 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,715 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:00,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,724 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,724 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:00,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:00,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:00,739 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:00,743 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:00,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,752 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,753 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,753 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:00,758 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:00,763 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:00,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:00,771 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:00,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,779 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,780 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,780 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:00,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:00,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:00,794 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:00,798 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:00,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,807 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,807 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,807 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:00,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:00,817 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:00,822 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:00,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,827 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:00,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,835 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,835 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,845 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,850 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,040 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,045 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:01,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,051 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,051 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,051 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,056 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,069 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:01,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,072 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,072 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,075 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,075 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,076 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,076 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,076 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,086 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,098 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,105 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,112 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,121 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,121 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,121 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,123 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,128 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 12:44:01,128 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,131 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,142 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,142 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,142 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,147 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,152 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,156 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,161 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,169 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,169 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,169 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,179 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,190 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,190 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,198 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,198 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,204 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,208 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,212 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,216 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,225 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,225 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:01,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:01,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:01,238 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:01,242 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:01,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,251 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,251 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,251 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:01,256 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:01,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:01,266 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:01,270 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:01,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,278 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,279 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,279 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:01,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:01,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:01,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:01,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:01,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,305 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,305 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:01,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:01,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:01,324 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:01,328 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:01,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,336 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,336 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:01,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:01,346 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:01,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:01,354 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:01,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,362 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,362 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:01,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:01,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:01,377 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:01,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:01,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,389 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,389 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:01,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:01,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:01,403 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:01,407 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,407 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:01,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,415 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:01,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:01,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,434 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,439 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,440 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,440 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,450 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,454 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,464 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,465 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,466 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,467 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,468 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,469 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,469 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,469 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,479 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,486 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,494 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,501 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,509 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,509 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,510 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,510 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,511 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,516 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 12:44:01,516 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,518 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,518 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,528 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,528 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,534 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,538 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,547 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,547 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,555 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,555 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,555 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,561 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,565 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,574 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,574 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,582 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,582 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,582 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,592 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,601 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,601 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,609 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,609 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,609 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:01,614 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:01,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:01,622 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:01,627 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,627 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:01,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,635 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,635 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:01,640 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:01,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:01,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:01,654 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,654 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:01,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,662 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,662 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,662 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:01,667 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:01,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:01,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:01,681 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:01,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,689 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,689 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:01,695 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:01,700 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:01,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:01,708 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:01,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,716 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:01,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:01,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:01,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:01,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:01,736 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,742 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,743 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:01,747 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:01,752 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:01,756 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:01,760 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:01,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,768 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,768 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:01,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:01,777 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:01,782 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:01,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:01,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,794 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,794 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:01,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:01,804 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,813 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,813 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,818 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,818 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,824 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,828 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,833 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,837 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,839 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,839 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,839 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,846 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,847 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,848 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,848 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,859 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,866 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,873 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,881 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,888 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,889 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,890 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,890 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,891 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,895 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 12:44:01,895 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,895 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,898 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,907 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,907 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,912 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,921 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,926 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,933 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,933 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,938 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,958 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,971 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,984 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,994 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,002 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,012 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,028 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,028 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,055 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,060 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,068 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,069 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,087 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,087 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,095 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,096 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,102 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:02,115 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:02,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,124 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:02,132 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:02,136 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:02,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:02,146 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:02,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,154 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,155 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:02,160 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:02,165 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:02,169 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:02,174 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,174 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:02,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,182 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,182 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,182 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:02,188 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:02,197 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:02,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:02,206 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:02,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,214 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,214 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,214 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:02,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:02,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:02,229 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:02,233 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,233 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:02,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,238 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,238 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,239 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:02,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:02,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:02,252 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:02,256 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:02,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,259 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,259 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:02,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:02,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:02,261 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:02,262 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:02,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,263 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,264 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,264 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:02,274 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:02,282 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:02,290 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:02,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:02,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:02,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,304 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:02,304 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,304 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:02,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:02,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:02,306 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:02,306 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,306 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:02,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,311 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 12:44:02,311 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,311 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:02,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:02,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:02,313 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:02,313 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,314 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:02,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,323 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,323 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:02,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:02,333 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:02,337 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:02,342 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:02,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,350 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,351 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,351 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:02,356 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:02,361 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:02,365 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:02,369 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:02,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,378 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,378 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:02,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:02,388 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:02,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:02,397 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:02,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,405 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,406 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,406 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,424 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,433 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,433 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,433 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,445 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,456 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,465 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,465 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,470 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,481 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,496 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,504 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,504 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,504 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,553 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,560 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:02,564 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:02,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,574 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,574 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,574 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:02,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:02,584 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:02,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:02,592 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,593 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:02,594 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,600 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,600 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:02,607 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:02,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:02,616 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:02,620 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:02,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,628 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,628 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,629 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:02,634 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:02,638 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:02,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:02,648 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:02,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,656 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,656 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,656 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:02,662 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:02,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:02,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:02,681 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:02,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,686 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,686 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,686 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:02,715 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:02,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:02,725 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:02,730 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:02,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,733 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,733 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,733 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:02,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:02,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:02,736 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:02,737 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:02,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,739 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,739 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,739 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:02,749 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:02,757 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:02,766 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:02,774 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:02,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:02,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,782 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:02,783 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,783 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:02,784 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:02,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:02,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:02,787 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:02,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,792 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 12:44:02,793 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:02,794 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:02,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:02,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:02,796 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:02,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,805 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,806 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,806 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:02,811 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:02,816 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:02,820 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:02,824 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:02,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,832 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,832 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,832 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:02,838 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:02,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:02,847 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:02,851 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:02,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,859 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,859 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,859 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:02,864 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:02,869 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:02,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:02,878 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:02,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,886 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,886 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,886 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,905 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,905 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,913 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,913 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,913 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,919 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,940 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,940 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,958 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,967 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,975 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,975 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,976 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,005 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,014 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,014 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,019 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,024 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,033 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,041 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,041 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,041 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,051 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,055 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,060 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,069 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,069 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,079 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,088 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,115 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,123 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,135 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,147 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,147 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,153 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,153 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,154 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,179 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,184 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,194 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,196 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,197 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,197 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,200 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,200 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,202 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,202 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,202 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:03,214 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:03,223 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:03,234 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:03,244 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:03,244 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:03,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,252 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:03,252 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,252 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:03,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:03,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:03,254 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:03,255 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,255 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:03,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,261 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 12:44:03,261 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,261 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:03,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:03,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:03,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:03,264 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,264 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:03,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,274 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:03,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:03,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:03,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:03,294 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:03,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,302 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,302 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:03,308 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:03,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:03,317 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:03,321 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:03,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,329 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,329 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:03,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:03,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:03,344 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:03,348 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,348 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:03,350 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,356 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,356 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,356 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:03,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:03,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:03,371 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:03,375 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:03,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,383 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,384 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:03,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:03,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:03,398 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:03,403 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:03,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,411 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,411 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:03,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:03,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:03,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:03,430 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:03,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,438 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:03,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:03,453 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:03,457 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,470 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,470 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,470 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,489 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,497 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,497 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,498 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,507 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,517 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,525 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,525 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,530 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,535 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,540 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,544 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,552 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,553 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,553 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,558 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,567 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,572 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,577 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,577 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,577 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,584 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,597 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,599 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,599 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,600 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,600 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,603 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,604 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,604 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,604 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:03,613 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:03,621 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:03,628 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:03,636 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:03,636 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:03,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,643 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:03,643 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:03,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:03,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:03,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:03,645 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:03,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,650 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 12:44:03,650 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:03,651 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:03,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:03,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:03,653 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:03,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,663 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,663 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:03,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:03,673 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:03,677 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:03,682 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,682 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:03,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,690 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,690 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,690 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:03,700 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:03,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:03,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:03,713 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:03,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,721 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,721 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:03,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:03,731 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:03,736 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:03,740 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:03,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,749 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,749 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:03,755 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:03,759 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:03,763 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:03,768 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:03,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,776 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:03,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:03,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:03,791 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:03,795 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:03,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,803 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,804 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:03,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:03,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:03,818 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:03,822 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,822 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:03,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,830 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,830 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:03,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:03,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:03,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,849 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,856 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,857 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,857 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,866 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,871 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,875 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,898 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,902 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,910 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,910 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,915 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,922 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,939 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,940 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,940 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,945 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,956 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,964 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,964 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,969 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,969 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,970 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,975 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,979 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,983 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,987 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,987 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,989 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,989 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,992 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,993 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,994 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,994 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,003 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,012 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,019 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,028 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,045 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,046 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,048 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,052 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 12:44:04,052 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,053 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,053 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,055 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,065 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,065 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,070 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,079 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,083 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,091 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,091 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,091 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,097 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,101 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,111 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,119 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,120 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,120 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,125 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,134 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,138 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,147 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,147 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,147 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,153 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,157 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,166 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,175 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,175 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,181 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,186 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,190 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:04,194 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:04,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,203 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,203 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,203 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:04,208 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:04,213 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:04,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:04,222 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:04,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,230 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,230 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:04,237 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:04,242 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:04,246 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:04,250 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,250 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:04,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,258 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,259 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:04,264 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:04,268 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:04,273 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:04,279 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:04,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,286 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,287 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:04,292 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:04,298 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:04,303 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:04,307 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:04,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,315 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,315 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:04,321 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:04,325 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:04,330 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:04,334 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:04,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,342 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,343 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:04,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:04,353 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:04,358 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:04,362 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:04,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,367 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,367 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:04,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:04,378 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:04,382 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:04,387 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:04,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,389 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,389 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:04,390 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:04,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:04,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:04,392 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,392 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:04,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,393 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,394 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,394 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,403 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,411 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,419 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,427 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,434 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,434 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,435 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,435 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,437 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,441 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 12:44:04,442 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,444 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,453 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,454 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,454 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,462 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,466 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,471 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,476 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,484 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,484 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,500 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,504 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,513 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,513 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,533 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,533 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,540 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,551 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,555 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,560 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,560 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,568 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,568 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,568 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,574 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,578 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:04,588 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,588 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:04,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,595 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,596 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,596 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:04,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:04,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:04,612 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:04,616 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:04,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,624 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,624 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,624 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:04,630 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:04,634 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:04,639 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:04,644 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,644 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:04,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,653 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:04,659 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:04,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:04,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:04,672 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:04,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,681 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,681 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:04,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:04,693 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:04,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:04,709 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,710 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:04,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,717 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,718 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,718 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:04,723 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:04,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:04,732 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:04,736 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:04,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,743 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,744 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:04,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:04,754 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:04,758 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:04,762 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,762 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:04,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,767 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,767 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:04,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:04,777 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:04,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:04,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:04,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,788 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,788 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:04,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:04,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:04,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:04,791 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:04,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,792 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,792 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,792 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,802 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,810 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,818 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,833 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,833 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,833 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,834 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,836 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,840 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 12:44:04,840 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,842 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,843 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,852 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,852 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,852 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,859 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,872 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,872 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,879 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,880 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,880 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,885 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,894 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,898 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,907 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,908 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,908 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,933 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,933 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,941 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,941 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,942 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,948 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,968 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,973 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,981 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,981 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,998 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:05,003 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:05,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:05,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:05,012 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,012 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,012 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:05,018 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:05,023 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:05,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:05,039 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:05,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:05,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:05,048 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,049 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:05,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:05,059 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:05,064 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:05,069 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:05,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:05,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:05,077 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,078 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:05,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:05,088 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:05,093 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:05,097 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,098 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:05,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:05,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:05,106 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,106 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:05,112 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:05,116 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:05,121 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:05,125 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,125 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:05,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:05,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:05,133 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,133 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,133 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:05,139 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:05,144 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:05,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:05,153 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:05,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:05,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:05,161 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:05,167 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:05,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:05,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:05,182 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:05,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:05,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:05,187 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,188 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,188 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:05,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:05,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:05,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:05,214 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:05,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:05,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:05,216 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,217 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:05,217 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:05,217 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:05,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:05,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:05,220 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:05,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:05,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:05,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:05,221 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,221 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:05,221 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:05,231 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:05,240 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:05,248 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:05,256 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:05,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( ( (
,,,,,,,,,,,,,,,,, and and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious???,...   ...                  
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?ooooooooooooooooooo's,, and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?oooooo   ...                  
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( to
:::,,,,,,,,,,,,,,,, and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious??                             
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,265 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?oooooooooooooooooo's's, and and and and and and and and and
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,265 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?ooooo                         
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,274 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - lm_head from flexgen to old.
