2023-10-07 11:45:39,468 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpch6i6ux_
2023-10-07 11:45:39,469 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpch6i6ux_/_remote_module_non_scriptable.py
2023-10-07 11:45:39,961 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-07 11:45:40,053 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:45:41,814 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-07 11:45:42,160 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-07 11:45:42,160 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-07 11:45:42,160 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-07 11:45:42,160 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-07 11:45:43,033 [flexgen_init.py:40 in policy_init] DEBUG - Got empty CausalLM: 'bigscience/bloom-560m' on meta device.
2023-10-07 11:45:43,050 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.word_embeddings, [0. 0. 1.], size_todo: 302313472
2023-10-07 11:45:43,051 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.word_embeddings_layernorm, [0.00000000e+00 3.98593761e-06 9.99996014e-01], size_todo: 302311424
2023-10-07 11:45:43,051 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.03116083 0.96883917], size_todo: 289715200
2023-10-07 11:45:43,052 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.05953522 0.94046478], size_todo: 277118976
2023-10-07 11:45:43,053 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.08548396 0.91451604], size_todo: 264522752
2023-10-07 11:45:43,054 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.10930533 0.89069467], size_todo: 251926528
2023-10-07 11:45:43,055 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.13125066 0.86874934], size_todo: 239330304
2023-10-07 11:45:43,055 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.15153316 0.84846684], size_todo: 226734080
2023-10-07 11:45:43,056 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.17033494 0.82966506], size_todo: 214137856
2023-10-07 11:45:43,057 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.7, [0.         0.18781242 0.81218758], size_todo: 201541632
2023-10-07 11:45:43,058 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.19277305 0.80722695], size_todo: 188945408
2023-10-07 11:45:43,059 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.20836231 0.79163769], size_todo: 176349184
2023-10-07 11:45:43,060 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.10, [0.         0.21235237 0.78764763], size_todo: 163752960
2023-10-07 11:45:43,060 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.11, [0.        0.2263748 0.7736252], size_todo: 151156736
2023-10-07 11:45:43,061 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.22958653 0.77041347], size_todo: 138560512
2023-10-07 11:45:43,062 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.24229253 0.75770747], size_todo: 125964288
2023-10-07 11:45:43,063 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.24487307 0.75512693], size_todo: 113368064
2023-10-07 11:45:43,064 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.25646083 0.74353917], size_todo: 100771840
2023-10-07 11:45:43,064 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.25852448 0.74147552], size_todo: 88175616
2023-10-07 11:45:43,065 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.26915308 0.73084692], size_todo: 75579392
2023-10-07 11:45:43,066 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27078978 0.72921022], size_todo: 62983168
2023-10-07 11:45:43,067 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.28058853 0.71941147], size_todo: 50386944
2023-10-07 11:45:43,068 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.20, [0.        0.2818699 0.7181301], size_todo: 37790720
2023-10-07 11:45:43,069 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.21, [0.         0.29094504 0.70905496], size_todo: 25194496
2023-10-07 11:45:43,069 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.22, [0.        0.2919287 0.7080713], size_todo: 12598272
2023-10-07 11:45:43,070 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.23, [0.         0.30036843 0.69963157], size_todo: 2048
2023-10-07 11:45:43,071 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-07 11:45:43,071 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-07 11:45:43,071 [flexgen_init.py:220 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-07 11:45:43,075 [flexgen_init.py:226 in get_policy_weight_map] INFO - CausalLM bigscience/bloom-560m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.31 GiB (30.04%), Disk Mem 0.73 Gib (69.96%)
2023-10-07 11:45:43,113 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:45:43,239 [flexgen_init.py:123 in check_disk] INFO - [], []
2023-10-07 11:45:43,278 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:45:43,403 [flexgen_init.py:123 in check_disk] INFO - [], []
2023-10-07 11:45:43,404 [flexgen_init.py:73 in policy_init] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/bigscience.bloom-560m'
2023-10-07 11:45:43,506 [flexgen_init.py:85 in policy_init] INFO - model has been loaded by policy.
2023-10-07 11:45:43,506 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.word_embeddings to test forward
2023-10-07 11:45:43,506 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.word_embeddings_layernorm to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-07 11:45:43,507 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-07 11:45:43,508 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.20 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.21 to test forward
2023-10-07 11:45:43,509 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.22 to test forward
2023-10-07 11:45:43,510 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.23 to test forward
2023-10-07 11:45:43,510 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-07 11:45:43,510 [flexgen_forward.py:38 in to_test_forward] DEBUG - lm_head to test forward
2023-10-07 11:45:43,553 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-07 11:45:44,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:44,308 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.word_embeddings forward pass:
2023-10-07 11:45:44,308 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:44,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:44,310 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.word_embeddings_layernorm forward pass:
2023-10-07 11:45:44,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:44,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:44,318 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.0 forward pass:
2023-10-07 11:45:44,327 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:44,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:44,336 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.1 forward pass:
2023-10-07 11:45:44,339 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:44,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:44,352 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.2 forward pass:
2023-10-07 11:45:44,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:44,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:44,368 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.3 forward pass:
2023-10-07 11:45:44,373 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:44,376 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:44,383 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.4 forward pass:
2023-10-07 11:45:44,387 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:44,388 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:44,395 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.5 forward pass:
2023-10-07 11:45:44,399 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:44,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:44,407 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.6 forward pass:
2023-10-07 11:45:44,411 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:44,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:44,419 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.7 forward pass:
2023-10-07 11:45:44,426 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:44,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:44,436 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.8 forward pass:
2023-10-07 11:45:44,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:44,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:44,447 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.9 forward pass:
2023-10-07 11:45:44,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:44,452 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:44,458 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.10 forward pass:
2023-10-07 11:45:44,462 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:44,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:44,470 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.11 forward pass:
2023-10-07 11:45:44,474 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:44,475 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:44,482 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.12 forward pass:
2023-10-07 11:45:44,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:44,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:44,493 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.13 forward pass:
2023-10-07 11:45:44,497 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:44,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:44,505 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.14 forward pass:
2023-10-07 11:45:44,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:44,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:44,516 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.15 forward pass:
2023-10-07 11:45:44,520 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:44,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:44,528 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.16 forward pass:
2023-10-07 11:45:44,532 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:44,534 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:44,540 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.17 forward pass:
2023-10-07 11:45:44,544 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:44,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:44,552 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.18 forward pass:
2023-10-07 11:45:44,556 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:44,558 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:44,564 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.19 forward pass:
2023-10-07 11:45:44,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:44,570 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:44,576 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.20 forward pass:
2023-10-07 11:45:44,580 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:44,582 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:44,588 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.21 forward pass:
2023-10-07 11:45:44,592 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:44,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:44,600 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.22 forward pass:
2023-10-07 11:45:44,604 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:44,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:44,613 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.23 forward pass:
2023-10-07 11:45:44,616 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:44,618 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:44,619 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.ln_f forward pass:
2023-10-07 11:45:44,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:44,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:44,621 [flexgen_forward.py:47 in new_forward] DEBUG - lm_head forward pass:
2023-10-07 11:45:44,670 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:44,702 [flexgen_test.py:31 in test_hf_gen] INFO - 0.
2023-10-07 11:45:44,702 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:45:44,824 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.word_embeddings from test to old.
2023-10-07 11:45:44,824 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.word_embeddings_layernorm from test to old.
2023-10-07 11:45:44,824 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.0 from test to old.
2023-10-07 11:45:44,824 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.1 from test to old.
2023-10-07 11:45:44,824 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.2 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.3 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.4 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.5 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.6 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.7 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.8 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.9 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.10 from test to old.
2023-10-07 11:45:44,825 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.11 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.12 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.13 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.14 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.15 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.16 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.17 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.18 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.19 from test to old.
2023-10-07 11:45:44,826 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.20 from test to old.
2023-10-07 11:45:44,827 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.21 from test to old.
2023-10-07 11:45:44,827 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.22 from test to old.
2023-10-07 11:45:44,827 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.23 from test to old.
2023-10-07 11:45:44,827 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.ln_f from test to old.
2023-10-07 11:45:44,827 [flexgen_forward.py:27 in to_old_forward] DEBUG - lm_head from test to old.
2023-10-07 11:45:44,827 [flexgen_init.py:105 in policy_init] INFO - layer order: ['transformer.word_embeddings', 'transformer.word_embeddings_layernorm', 'transformer.h.0', 'transformer.h.1', 'transformer.h.2', 'transformer.h.3', 'transformer.h.4', 'transformer.h.5', 'transformer.h.6', 'transformer.h.7', 'transformer.h.8', 'transformer.h.9', 'transformer.h.10', 'transformer.h.11', 'transformer.h.12', 'transformer.h.13', 'transformer.h.14', 'transformer.h.15', 'transformer.h.16', 'transformer.h.17', 'transformer.h.18', 'transformer.h.19', 'transformer.h.20', 'transformer.h.21', 'transformer.h.22', 'transformer.h.23', 'transformer.ln_f', 'lm_head']
2023-10-07 11:45:44,827 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.word_embeddings to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.word_embeddings_layernorm to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-07 11:45:44,828 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-07 11:45:44,829 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.20 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.21 to flexgen forward
2023-10-07 11:45:44,830 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.22 to flexgen forward
2023-10-07 11:45:44,831 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.23 to flexgen forward
2023-10-07 11:45:44,831 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-07 11:45:44,831 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-07 11:45:44,869 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-07 11:45:45,509 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:45,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:45,512 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]),)
2023-10-07 11:45:45,512 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:45,512 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]),)
2023-10-07 11:45:45,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:45,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:45,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:45,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:45,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:45,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 1024])
2023-10-07 11:45:45,513 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 1024])
2023-10-07 11:45:45,513 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:45,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:45,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:45,522 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,522 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:45,522 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,522 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:45,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:45,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:45,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:45,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:45,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 1024])
2023-10-07 11:45:45,523 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 1024])
2023-10-07 11:45:45,523 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:45,524 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:45,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:45,537 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,537 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,538 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,538 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:45,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:45,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:45,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:45,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,561 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,561 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:45,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:45,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:45,576 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,576 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,576 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,576 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:45,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:45,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:45,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:45,601 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,601 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,602 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:45,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:45,610 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:45,617 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,617 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,617 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,617 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:45,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:45,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:45,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:45,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,639 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,639 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:45,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:45,646 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:45,654 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,654 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:45,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:45,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:45,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:45,675 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,675 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,676 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:45,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:45,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:45,690 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,690 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,691 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,691 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:45,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:45,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:45,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:45,710 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,711 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,711 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:45,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:45,719 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:45,726 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,726 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,726 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,726 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:45,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:45,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:45,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:45,749 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,750 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,750 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:45,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:45,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:45,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,765 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,766 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:45,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:45,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:45,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:45,786 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,787 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:45,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:45,794 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:45,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,801 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:45,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:45,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:45,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:45,824 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,825 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,825 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:45,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:45,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:45,841 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,841 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,841 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,841 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:45,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:45,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:45,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:45,864 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,864 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,865 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:45,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:45,872 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:45,879 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,879 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,879 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:45,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:45,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:45,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:45,907 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,908 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,908 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:45,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:45,916 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:45,923 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,923 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,923 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,923 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:45,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:45,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:45,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:45,951 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,951 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,952 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:45,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:45,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:45,967 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:45,967 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:45,968 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:45,968 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:45,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:45,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:45,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:45,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:45,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:45,995 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:45,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:45,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:46,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:46,010 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,010 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,010 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:46,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:46,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:46,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:46,039 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,040 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,040 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:46,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:46,048 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:46,055 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,055 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,055 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:46,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:46,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:46,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:46,083 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,083 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,084 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:46,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:46,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:46,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,098 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:46,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:46,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:46,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:46,125 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,126 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:46,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:46,134 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:46,141 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,141 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,141 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,141 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:46,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:46,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:46,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:46,168 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,169 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,169 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:46,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:46,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:46,183 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,183 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,183 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,183 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:46,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:46,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:46,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:46,203 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,204 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,204 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:46,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:46,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:46,219 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,219 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,219 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,219 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:46,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:46,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:46,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:46,238 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,239 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,239 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:46,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:46,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:46,254 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,254 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,254 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,254 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:46,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:46,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:46,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:46,274 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,275 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,275 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:46,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:46,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:46,289 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,289 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,289 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,290 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:46,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:46,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:46,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:46,309 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:46,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:46,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:46,323 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,324 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,324 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:46,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:46,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:46,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:46,345 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,346 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,346 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:46,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:46,354 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:46,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,361 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,361 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:46,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:46,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:46,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:46,385 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,385 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:46,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:46,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:46,400 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,400 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,400 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:46,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:46,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:46,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:46,420 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,421 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,421 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:46,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:46,429 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:46,430 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,431 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': None, 'attention_mask': torch.Size([8, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 10])}
2023-10-07 11:45:46,431 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,431 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': None, 'attention_mask': torch.Size([2, 1, 10, 10]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 10])}
2023-10-07 11:45:46,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:46,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:46,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:46,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:46,451 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 1024]), (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])))
2023-10-07 11:45:46,451 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 1024]), (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])))
2023-10-07 11:45:46,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:46,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:46,454 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:46,454 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,455 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:46,455 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,455 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:46,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:46,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:46,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:46,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:46,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 1024])
2023-10-07 11:45:46,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 1024])
2023-10-07 11:45:46,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:46,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:46,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:46,458 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 1024]),)
2023-10-07 11:45:46,458 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:46,458 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 1024]),)
2023-10-07 11:45:46,459 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:46,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:46,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:46,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:46,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:46,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 250880])
2023-10-07 11:45:46,717 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 250880])
2023-10-07 11:45:46,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:46,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:46,754 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:46,755 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:46,756 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:46,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:46,757 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:46,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:46,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:46,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:46,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:46,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:46,758 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:46,758 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:46,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:46,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:46,766 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,767 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:46,767 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,767 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:46,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:46,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:46,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:46,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:46,768 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:46,768 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:46,768 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:46,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:46,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:46,782 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,782 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,782 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,782 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:46,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:46,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:46,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:46,799 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,799 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,800 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:46,801 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:46,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:46,814 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,814 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,814 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,815 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:46,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:46,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:46,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:46,828 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,829 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:46,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:46,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:46,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:46,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:46,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:46,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:46,858 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,859 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:46,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:46,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:46,873 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,873 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,874 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,874 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:46,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:46,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:46,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:46,887 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,888 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,888 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:46,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:46,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:46,904 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,904 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,904 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,904 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:46,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:46,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:46,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:46,919 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,919 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,919 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:46,921 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:46,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:46,933 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,933 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,933 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,934 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:46,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:46,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:46,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:46,947 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,948 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,948 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:46,949 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:46,955 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:46,962 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,962 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,962 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,962 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:46,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:46,970 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:46,973 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:46,977 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:46,977 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:46,977 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:46,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:46,985 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:46,991 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:46,991 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:46,991 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:46,991 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:46,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:46,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:47,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:47,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:47,007 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,008 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,008 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:47,009 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:47,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:47,022 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,023 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,023 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,023 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:47,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:47,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:47,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:47,073 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,074 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,075 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:47,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:47,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:47,090 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,092 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:47,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:47,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:47,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:47,113 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,114 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,114 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:47,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:47,122 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:47,130 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,130 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,131 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,131 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:47,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:47,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:47,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:47,151 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,151 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:47,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:47,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:47,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:47,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:47,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:47,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:47,185 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,185 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,186 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:47,188 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:47,194 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:47,201 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,201 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,201 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,202 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:47,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:47,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:47,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:47,216 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,216 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,217 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:47,218 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:47,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:47,231 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,232 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,232 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,232 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:47,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:47,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:47,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:47,247 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,248 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,248 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:47,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:47,255 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:47,262 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,263 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,263 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,263 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:47,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:47,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:47,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:47,278 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,279 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:47,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:47,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:47,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,294 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,294 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:47,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:47,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:47,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:47,312 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,312 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,312 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:47,314 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:47,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:47,327 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,327 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,328 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,328 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:47,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:47,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:47,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:47,365 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,366 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,366 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:47,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:47,375 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:47,381 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,381 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,381 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:47,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:47,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:47,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:47,396 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,397 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,397 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:47,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:47,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:47,411 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,411 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,411 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,412 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:47,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:47,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:47,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:47,427 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,428 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,428 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:47,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:47,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:47,442 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,443 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,443 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:47,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:47,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:47,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:47,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,459 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:47,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:47,467 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:47,474 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,474 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,474 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:47,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:47,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:47,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:47,489 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,490 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,490 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:47,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:47,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:47,504 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,505 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:47,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:47,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:47,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:47,521 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,522 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,522 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:47,523 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:47,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:47,536 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,536 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,536 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:47,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:47,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:47,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:47,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:47,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:47,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:47,562 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,562 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 10]), torch.Size([128, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 11])}
2023-10-07 11:45:47,562 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,562 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 10]), torch.Size([32, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 11])}
2023-10-07 11:45:47,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:47,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:47,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:47,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:47,577 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])))
2023-10-07 11:45:47,578 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])))
2023-10-07 11:45:47,578 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:47,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:47,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:47,581 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,581 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:47,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,582 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:47,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:47,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:47,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:47,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:47,583 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:47,583 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:47,583 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:47,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:47,584 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:47,585 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,585 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:47,585 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,585 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:47,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:47,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:47,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:47,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:47,760 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:47,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:47,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:47,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:47,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:47,816 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:47,817 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:47,817 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:47,817 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:47,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:47,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:47,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:47,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:47,818 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:47,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:47,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:47,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:47,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:47,828 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,828 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:47,828 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,828 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:47,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:47,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:47,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:47,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:47,829 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:47,829 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:47,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:47,830 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:47,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:47,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:47,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:47,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:47,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:47,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:47,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:47,862 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:47,862 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:47,862 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:47,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:47,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:47,877 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,877 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:47,877 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,877 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:47,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:47,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:47,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:47,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:47,896 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:47,896 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:47,896 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:47,898 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:47,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:47,911 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,911 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:47,911 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,911 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:47,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:47,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:47,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:47,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:47,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:47,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:47,929 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:47,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:47,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:47,944 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:47,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:47,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:47,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:47,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:47,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:47,961 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:47,962 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:47,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:47,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:47,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:47,976 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:47,976 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:47,977 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:47,977 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:47,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:47,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:47,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:47,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:47,995 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:47,995 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:47,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:47,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:48,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:48,010 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,010 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,010 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:48,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:48,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:48,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:48,028 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,029 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,029 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:48,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:48,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:48,044 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,044 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,044 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,044 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:48,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:48,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:48,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:48,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,063 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:48,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:48,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:48,078 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,078 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,078 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,078 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:48,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:48,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:48,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:48,096 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,097 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,097 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:48,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:48,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:48,115 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,116 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,116 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,116 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:48,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:48,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:48,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:48,133 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,133 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,133 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:48,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:48,141 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:48,148 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,148 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,148 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,149 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:48,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:48,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:48,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:48,164 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,164 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,164 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:48,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:48,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:48,179 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,179 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,179 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,179 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,179 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:48,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:48,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:48,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:48,194 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,195 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:48,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:48,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:48,210 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,210 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,210 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,210 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:48,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:48,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:48,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:48,225 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,226 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,226 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:48,227 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:48,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:48,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:48,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:48,250 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:48,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:48,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,258 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,258 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:48,261 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:48,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:48,279 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,279 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,279 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,279 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:48,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:48,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:48,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:48,293 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,294 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,294 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:48,295 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:48,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:48,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,310 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,310 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:48,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:48,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:48,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:48,324 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,324 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,324 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:48,326 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:48,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:48,341 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,341 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,341 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,341 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:48,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:48,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:48,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:48,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,356 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:48,358 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:48,365 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:48,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,372 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:48,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:48,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:48,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:48,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,388 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,388 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:48,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:48,397 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:48,404 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,405 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:48,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:48,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:48,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:48,418 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,419 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,419 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:48,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:48,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:48,433 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,433 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,434 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,434 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,434 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:48,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:48,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:48,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:48,449 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,449 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,449 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:48,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:48,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:48,465 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,466 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,466 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,466 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:48,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:48,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:48,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:48,479 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,480 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,480 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:48,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:48,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:48,495 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,496 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,496 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,496 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:48,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:48,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:48,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:48,510 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,511 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,511 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:48,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:48,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:48,527 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,527 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,527 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:48,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:48,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:48,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:48,541 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,542 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,542 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:48,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:48,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:48,558 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,558 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,558 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,558 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:48,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:48,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:48,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:48,580 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,580 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,581 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:48,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:48,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:48,591 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,591 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 11]), torch.Size([128, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 12])}
2023-10-07 11:45:48,591 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,591 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 11]), torch.Size([32, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 12])}
2023-10-07 11:45:48,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:48,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:48,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:48,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:48,607 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])))
2023-10-07 11:45:48,608 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])))
2023-10-07 11:45:48,608 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:48,610 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:48,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:48,612 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:48,613 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,613 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:48,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:48,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:48,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:48,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:48,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:48,615 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:48,615 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:48,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:48,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:48,618 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,618 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:48,618 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,619 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:48,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:48,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:48,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:48,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:48,786 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:48,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:48,788 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:48,818 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:48,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:48,821 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:48,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:48,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:48,821 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:48,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:48,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:48,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:48,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:48,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:48,822 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:48,823 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:48,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:48,824 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:48,831 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,831 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:48,831 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,831 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:48,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:48,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:48,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:48,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:48,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:48,833 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:48,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:48,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:48,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:48,846 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,846 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:48,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:48,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:48,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:48,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:48,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:48,910 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:48,911 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:48,911 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:48,912 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:48,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:48,927 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,927 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:48,927 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:48,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:48,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:48,936 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:48,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:48,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:48,945 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:48,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:48,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:48,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:48,960 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,960 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:48,960 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,960 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:48,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:48,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:48,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:48,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:48,977 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:48,978 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:48,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:48,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:48,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:48,992 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:48,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:48,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:48,993 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:48,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:49,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:49,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:49,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:49,022 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,023 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,023 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:49,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:49,032 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:49,039 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,039 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,040 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,040 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:49,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:49,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:49,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:49,057 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,058 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,058 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:49,059 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:49,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:49,073 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,073 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,073 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,074 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:49,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:49,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:49,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:49,089 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,089 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:49,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:49,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:49,111 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,112 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,112 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,113 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:49,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:49,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:49,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:49,137 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:49,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:49,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:49,151 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,151 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,152 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:49,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:49,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:49,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:49,170 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,171 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,171 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:49,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:49,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:49,185 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,185 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,185 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:49,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:49,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:49,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:49,200 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,201 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,201 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:49,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:49,209 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:49,216 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,216 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,216 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,216 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:49,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:49,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:49,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:49,232 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,232 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,232 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:49,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:49,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:49,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,247 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:49,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:49,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:49,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:49,261 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,262 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:49,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:49,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:49,275 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,275 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,276 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,276 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:49,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:49,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:49,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:49,289 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,290 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,290 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:49,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:49,297 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:49,303 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,304 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,304 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,304 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:49,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:49,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:49,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:49,316 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,317 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,317 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:49,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:49,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:49,330 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,330 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:49,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:49,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:49,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:49,343 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,343 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:49,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:49,351 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:49,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,357 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,358 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,358 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:49,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:49,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:49,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:49,370 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,370 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,371 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:49,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:49,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:49,384 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,384 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,384 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,385 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:49,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:49,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:49,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:49,398 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,398 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,399 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:49,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:49,406 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:49,413 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,413 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,413 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,413 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:49,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:49,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:49,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:49,432 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,432 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,432 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:49,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:49,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:49,447 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,447 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,447 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,447 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:49,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:49,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:49,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:49,466 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,466 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,466 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:49,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:49,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:49,481 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,482 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,482 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,482 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:49,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:49,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:49,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:49,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,500 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,500 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:49,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:49,509 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:49,515 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,515 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,515 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,516 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:49,521 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:49,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:49,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:49,533 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,534 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,534 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:49,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:49,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:49,549 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,549 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,549 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:49,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:49,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:49,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:49,567 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,567 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:49,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:49,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:49,582 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,582 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:49,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:49,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:49,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:49,597 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,597 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,597 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:49,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:49,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:49,612 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,613 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,613 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:49,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:49,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:49,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:49,626 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,626 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,626 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:49,628 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:49,635 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:49,636 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 12]), torch.Size([128, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 13])}
2023-10-07 11:45:49,636 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,636 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 12]), torch.Size([32, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 13])}
2023-10-07 11:45:49,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:49,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:49,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:49,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:49,650 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])))
2023-10-07 11:45:49,650 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])))
2023-10-07 11:45:49,651 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:49,652 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:49,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:49,654 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,654 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:49,655 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,655 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:49,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:49,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:49,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:49,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:49,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:49,656 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:49,656 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:49,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:49,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:49,658 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,658 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:49,658 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,658 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:49,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:49,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:49,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:49,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:49,813 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:49,814 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:49,815 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:49,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:49,847 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:49,849 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:49,849 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:49,849 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:49,849 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:49,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:49,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:49,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:49,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:49,851 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:49,851 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:49,851 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:49,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:49,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:49,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:49,860 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:49,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:49,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:49,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:49,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:49,861 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:49,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:49,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:49,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:49,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:49,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:49,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:49,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:49,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:49,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:49,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:49,890 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:49,891 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:49,891 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:49,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:49,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:49,906 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,906 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:49,906 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,906 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:49,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:49,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:49,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:49,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:49,921 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:49,922 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:49,922 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:49,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:49,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:49,937 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,937 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:49,937 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,937 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:49,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:49,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:49,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:49,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:49,952 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:49,953 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:49,953 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:49,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:49,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:49,967 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,967 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:49,968 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,968 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:49,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:49,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:49,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:49,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:49,982 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:49,982 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:49,983 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:49,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:49,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:49,998 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:49,998 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:49,998 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:49,998 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:49,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:50,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:50,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:50,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:50,012 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,012 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,012 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:50,014 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:50,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:50,027 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,028 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,028 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:50,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:50,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:50,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:50,041 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,041 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,042 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:50,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:50,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:50,056 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,056 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,057 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,057 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:50,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:50,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:50,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:50,070 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,070 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,071 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:50,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:50,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:50,085 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,085 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,085 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,085 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:50,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:50,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:50,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:50,101 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,101 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:50,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:50,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:50,116 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,117 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,117 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,117 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:50,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:50,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:50,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:50,131 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,131 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:50,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:50,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:50,146 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,146 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,147 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,147 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:50,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:50,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:50,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:50,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,161 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:50,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:50,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:50,177 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,177 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,177 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:50,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:50,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:50,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:50,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,191 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,191 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:50,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:50,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:50,205 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,206 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,206 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,206 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:50,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:50,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:50,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:50,221 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,222 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,222 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:50,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:50,230 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:50,238 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,238 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,238 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,238 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:50,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:50,250 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:50,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:50,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:50,259 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:50,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:50,272 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,272 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,272 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,273 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:50,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:50,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:50,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:50,286 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,287 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,287 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:50,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:50,295 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:50,302 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,302 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,302 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,302 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:50,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:50,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:50,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:50,318 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,319 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,319 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:50,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:50,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:50,334 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,334 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,334 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,334 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:50,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:50,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:50,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:50,351 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,351 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,351 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:50,353 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:50,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:50,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,367 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,367 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,367 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:50,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:50,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:50,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:50,382 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,383 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,383 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:50,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:50,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:50,398 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,398 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,398 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,398 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:50,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:50,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:50,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:50,416 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,416 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,416 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:50,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:50,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:50,431 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,432 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,432 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:50,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:50,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:50,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:50,447 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,448 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,448 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:50,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:50,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:50,463 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,463 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:50,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:50,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:50,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:50,479 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,480 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,480 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:50,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:50,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:50,494 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,495 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,495 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:50,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:50,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:50,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:50,510 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,511 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,511 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:50,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:50,519 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:50,526 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,526 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,526 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,526 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:50,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:50,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:50,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:50,541 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,542 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,542 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:50,543 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:50,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:50,557 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,557 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,557 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,557 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:50,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:50,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:50,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:50,572 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:50,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:50,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:50,582 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,582 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 13]), torch.Size([128, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 14])}
2023-10-07 11:45:50,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,582 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 13]), torch.Size([32, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 14])}
2023-10-07 11:45:50,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:50,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:50,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:50,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:50,598 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])))
2023-10-07 11:45:50,598 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])))
2023-10-07 11:45:50,599 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:50,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:50,601 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:50,602 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,602 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:50,602 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,602 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:50,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:50,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:50,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:50,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:50,603 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:50,603 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:50,604 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:50,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:50,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:50,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,606 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:50,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,606 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:50,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:50,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:50,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:50,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:50,774 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:50,776 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:50,776 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:50,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:50,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:50,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:50,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:50,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:50,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:50,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:50,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:50,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:50,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:50,814 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:50,814 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:50,814 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:50,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:50,816 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:50,823 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,824 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:50,824 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,824 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:50,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:50,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:50,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:50,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:50,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:50,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:50,828 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:50,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:50,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:50,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:50,843 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:50,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:50,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:50,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:50,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:50,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:50,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:50,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:50,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:50,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:50,876 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,877 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:50,877 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,877 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:50,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:50,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:50,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:50,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:50,893 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:50,894 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:50,894 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:50,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:50,903 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:50,915 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,915 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:50,915 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,916 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:50,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:50,921 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:50,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:50,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:50,934 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:50,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:50,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:50,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:50,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:50,953 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,953 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:50,953 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,953 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:50,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:50,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:50,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:50,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:50,969 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:50,969 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:50,970 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:50,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:50,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:50,985 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:50,985 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:50,985 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:50,986 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:50,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:50,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:50,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:50,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:51,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,004 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,004 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:51,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:51,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:51,028 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,028 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,029 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,029 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:51,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:51,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:51,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:51,046 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,047 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,047 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:51,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:51,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:51,063 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,063 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,063 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,064 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:51,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:51,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:51,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:51,080 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,081 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,081 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:51,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:51,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:51,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,096 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,096 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,096 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:51,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:51,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:51,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:51,116 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,117 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,117 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:51,119 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:51,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:51,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,139 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:51,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:51,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:51,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:51,155 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,156 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,156 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:51,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:51,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:51,172 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,172 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,172 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,172 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:51,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:51,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:51,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:51,192 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,193 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:51,194 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:51,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:51,214 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,214 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,215 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,215 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:51,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:51,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:51,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:51,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,239 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,240 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:51,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:51,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:51,256 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,256 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,257 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,257 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:51,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:51,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:51,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:51,275 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,275 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,275 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:51,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:51,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:51,289 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,289 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,290 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,290 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:51,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:51,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:51,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:51,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:51,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:51,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:51,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,324 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,324 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:51,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:51,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:51,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:51,341 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,342 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,342 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:51,343 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:51,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:51,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,357 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,357 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,358 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:51,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:51,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:51,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:51,375 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,375 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,376 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:51,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:51,384 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:51,390 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,391 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,391 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,391 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:51,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:51,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:51,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:51,409 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,410 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,410 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:51,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:51,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:51,425 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,426 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:51,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:51,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:51,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:51,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,444 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,445 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:51,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:51,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:51,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,459 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,460 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,460 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:51,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:51,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:51,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:51,475 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,476 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:51,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:51,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:51,490 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:51,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:51,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:51,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:51,506 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,506 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,507 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:51,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:51,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:51,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,522 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,522 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,522 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:51,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:51,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:51,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:51,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,605 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:51,607 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:51,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:51,627 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,628 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,629 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,629 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:51,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:51,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:51,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:51,658 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,659 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,660 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:51,662 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:51,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:51,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,674 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,674 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:51,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:51,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:51,686 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:51,689 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,691 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,691 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:51,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:51,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:51,706 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,706 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,706 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:51,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:51,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:51,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:51,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,733 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,734 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:51,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:51,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:51,744 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,744 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 14]), torch.Size([128, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 15])}
2023-10-07 11:45:51,744 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,744 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 14]), torch.Size([32, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 15])}
2023-10-07 11:45:51,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:51,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:51,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:51,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:51,759 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])))
2023-10-07 11:45:51,760 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])))
2023-10-07 11:45:51,760 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:51,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:51,762 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:51,763 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,763 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:51,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,763 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:51,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:51,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:51,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:51,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:51,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:51,765 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:51,765 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:51,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:51,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:51,767 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,767 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:51,767 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,767 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:51,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:51,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:51,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:51,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:51,933 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:51,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:51,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:51,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:51,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:51,965 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:51,965 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:51,966 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:51,966 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:51,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:51,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:51,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:51,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:51,967 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:51,968 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:51,968 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:51,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:51,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:51,976 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,976 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:51,976 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,976 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:51,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:51,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:51,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:51,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:51,977 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:51,977 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:51,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:51,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:51,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:51,990 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:51,990 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:51,990 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:51,990 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:51,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:51,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:51,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:52,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:52,008 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,009 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,009 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:52,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:52,021 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:52,032 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,033 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,033 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,033 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:52,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:52,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:52,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:52,050 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,050 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:52,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:52,058 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:52,065 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,065 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,065 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,065 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:52,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:52,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:52,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:52,079 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,079 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,080 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:52,081 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:52,087 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:52,093 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,093 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,093 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,094 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:52,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:52,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:52,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:52,110 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,111 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,111 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:52,112 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:52,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:52,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,126 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:52,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:52,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:52,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:52,142 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,143 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,143 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:52,144 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:52,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:52,158 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,158 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,158 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,158 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:52,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:52,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:52,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:52,174 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,174 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,175 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:52,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:52,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:52,188 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,189 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,189 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:52,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:52,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:52,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:52,207 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,208 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,208 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:52,209 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:52,215 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:52,222 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,222 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,223 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,223 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:52,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:52,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:52,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:52,241 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,241 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,241 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:52,243 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:52,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:52,256 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,256 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,256 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:52,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:52,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:52,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:52,271 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,272 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,272 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:52,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:52,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:52,285 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,286 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,286 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,286 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:52,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:52,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:52,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:52,301 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,302 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,302 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:52,304 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:52,310 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:52,320 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,320 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,321 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,321 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:52,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:52,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:52,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:52,360 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,361 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,361 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:52,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:52,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:52,376 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,376 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,377 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:52,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:52,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:52,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:52,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,392 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:52,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:52,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:52,407 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,408 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,408 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:52,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:52,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:52,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:52,425 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,426 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,426 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:52,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:52,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:52,441 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,441 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,441 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,441 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:52,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:52,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:52,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:52,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:52,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:52,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:52,471 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,471 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,472 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,472 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:52,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:52,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:52,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:52,486 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,486 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,486 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:52,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:52,495 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:52,502 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,502 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,502 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,502 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:52,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:52,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:52,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:52,519 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,520 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,520 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:52,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:52,528 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:52,535 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,536 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,536 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:52,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:52,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:52,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:52,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,552 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:52,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:52,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:52,567 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,567 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,568 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,568 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:52,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:52,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:52,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:52,583 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,583 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,583 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:52,585 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:52,591 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:52,599 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,599 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,599 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,599 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:52,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:52,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:52,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:52,617 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,618 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,618 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:52,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:52,626 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:52,633 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,633 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,633 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,633 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:52,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:52,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:52,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:52,648 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,648 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:52,650 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:52,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:52,663 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,663 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,664 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,664 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:52,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:52,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:52,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:52,679 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,680 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,680 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:52,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:52,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:52,694 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,694 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,695 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,695 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:52,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:52,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:52,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:52,711 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,712 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,712 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:52,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:52,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:52,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,727 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,727 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:52,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:52,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:52,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:52,742 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,742 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,742 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:52,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:52,750 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:52,752 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,752 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 15]), torch.Size([128, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 16])}
2023-10-07 11:45:52,752 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,752 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 15]), torch.Size([32, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 16])}
2023-10-07 11:45:52,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:52,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:52,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:52,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:52,766 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])))
2023-10-07 11:45:52,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])))
2023-10-07 11:45:52,767 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:52,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:52,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:52,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:52,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,771 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:52,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:52,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:52,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:52,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:52,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:52,772 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:52,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:52,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:52,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:52,774 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,774 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:52,774 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,774 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:52,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:52,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:52,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:52,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:52,944 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:52,946 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:52,946 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:52,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:52,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:52,980 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:52,981 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:52,981 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:52,982 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:52,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:52,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:52,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:52,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:52,985 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:52,985 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:52,985 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:52,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:52,987 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:52,994 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:52,994 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:52,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:52,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:52,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:52,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:52,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:52,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:52,996 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:52,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:52,996 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:52,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:53,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:53,010 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,010 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,010 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:53,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:53,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:53,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:53,028 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,029 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,029 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:53,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:53,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:53,044 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,044 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,044 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,045 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:53,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:53,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:53,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:53,060 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,061 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,061 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:53,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:53,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:53,075 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,075 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,076 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,076 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:53,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:53,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:53,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:53,091 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,092 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:53,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:53,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:53,109 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,110 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,110 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,110 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:53,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:53,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:53,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:53,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:53,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:53,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:53,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:53,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:53,179 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:53,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:53,192 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,194 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:53,195 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:53,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:53,208 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,208 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,209 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,209 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:53,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:53,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:53,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:53,227 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,228 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:53,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:53,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:53,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,243 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,243 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:53,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:53,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:53,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:53,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,260 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,260 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:53,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:53,268 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:53,274 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,274 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,274 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,274 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:53,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:53,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:53,285 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:53,288 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,289 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,289 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:53,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:53,297 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:53,304 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,304 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,304 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,304 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:53,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:53,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:53,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:53,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,324 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,324 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:53,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:53,332 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:53,338 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,338 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,339 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,339 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:53,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:53,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:53,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:53,357 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,358 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,358 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:53,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:53,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:53,373 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,373 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:53,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:53,383 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:53,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:53,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,392 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:53,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:53,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:53,407 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,407 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:53,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:53,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:53,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:53,428 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:53,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:53,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:53,444 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,445 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,445 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,445 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:53,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:53,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:53,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:53,463 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:53,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:53,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:53,479 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,479 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:53,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:53,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:53,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:53,498 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,498 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,498 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:53,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:53,506 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:53,513 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,513 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,513 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,513 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:53,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:53,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:53,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:53,541 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,542 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,542 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:53,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:53,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:53,557 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,557 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,557 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,558 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:53,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:53,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:53,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:53,575 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,576 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,576 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:53,577 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:53,584 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:53,591 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,591 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,591 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,591 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:53,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:53,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:53,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:53,612 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,613 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,614 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:53,615 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:53,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:53,628 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,628 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,628 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,628 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:53,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:53,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:53,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:53,646 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,647 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,647 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:53,648 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:53,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:53,661 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,661 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,661 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,662 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,662 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:53,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:53,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:53,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:53,694 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,694 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,695 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:53,697 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:53,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:53,711 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,711 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,711 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,711 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:53,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:53,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:53,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:53,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:53,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:53,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:53,749 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,749 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,749 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,749 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:53,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:53,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:53,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:53,766 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,767 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:53,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:53,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:53,781 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,782 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,782 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,782 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:53,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:53,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:53,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:53,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,797 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:53,799 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:53,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:53,812 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,813 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,813 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:53,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:53,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:53,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:53,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:53,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:53,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:53,839 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,840 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 16]), torch.Size([128, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 17])}
2023-10-07 11:45:53,840 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,840 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 16]), torch.Size([32, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 17])}
2023-10-07 11:45:53,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:53,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:53,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:53,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:53,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])))
2023-10-07 11:45:53,856 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])))
2023-10-07 11:45:53,856 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:53,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:53,859 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:53,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:53,860 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:53,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:53,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:53,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:53,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:53,861 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:53,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:53,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:53,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:53,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:53,863 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:53,863 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:53,863 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:53,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:53,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:53,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:53,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:54,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:54,052 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:54,054 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:54,054 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:54,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:54,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:54,085 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:54,085 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:54,085 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:54,086 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:54,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:54,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:54,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:54,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:54,087 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:54,087 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:54,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:54,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:54,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:54,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,095 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:54,095 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,095 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:54,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:54,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:54,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:54,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:54,096 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:54,096 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:54,097 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:54,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:54,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:54,110 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,110 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,110 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,110 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:54,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:54,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:54,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:54,128 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,129 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,129 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:54,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:54,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:54,143 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,143 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:54,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:54,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:54,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:54,162 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:54,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:54,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:54,175 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,176 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,176 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:54,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:54,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:54,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:54,192 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,193 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:54,195 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:54,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:54,208 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,208 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,208 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,208 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:54,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:54,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:54,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:54,227 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,228 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:54,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:54,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:54,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:54,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:54,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:54,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:54,261 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:54,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:54,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:54,275 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,276 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,276 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,276 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:54,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:54,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:54,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:54,294 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,295 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,295 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:54,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:54,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:54,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:54,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:54,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:54,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:54,325 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,325 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,325 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:54,327 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:54,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:54,339 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,340 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,340 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,340 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:54,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:54,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:54,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:54,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,356 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:54,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:54,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:54,370 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,371 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,371 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,371 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:54,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:54,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:54,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:54,385 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,385 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:54,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:54,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:54,399 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,399 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,400 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:54,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:54,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:54,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:54,420 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,420 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,420 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:54,422 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:54,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:54,435 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,435 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,435 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,435 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:54,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:54,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:54,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:54,450 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,451 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:54,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:54,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:54,466 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,466 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,466 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,467 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:54,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:54,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:54,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:54,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:54,484 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:54,490 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:54,497 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,497 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,497 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,498 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:54,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:54,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:54,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:54,515 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,516 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,516 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:54,518 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:54,524 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:54,531 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,531 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,531 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,531 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:54,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:54,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:54,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:54,549 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,550 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,550 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:54,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:54,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:54,565 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,565 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,565 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,565 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:54,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:54,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:54,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:54,583 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,583 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,584 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:54,586 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:54,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:54,598 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,598 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,599 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,599 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:54,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:54,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:54,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:54,618 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,618 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:54,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:54,626 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:54,633 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,633 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,633 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,633 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:54,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:54,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:54,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:54,652 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,652 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:54,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:54,660 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:54,667 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,667 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,667 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,667 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:54,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:54,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:54,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:54,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,686 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,686 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:54,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:54,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:54,701 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,701 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,701 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,701 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:54,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:54,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:54,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:54,719 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,720 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,720 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:54,722 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:54,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:54,735 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,735 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,735 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,735 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:54,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:54,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:54,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:54,754 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,755 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,755 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:54,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:54,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:54,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:54,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:54,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:54,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:54,788 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,788 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,789 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:54,790 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:54,797 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:54,803 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,803 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,803 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,803 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:54,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:54,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:54,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:54,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:54,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:54,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:54,832 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,832 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,833 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:54,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:54,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:54,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:54,847 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,847 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,848 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:54,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:54,856 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:54,857 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,857 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 17]), torch.Size([128, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 18])}
2023-10-07 11:45:54,857 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,858 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 17]), torch.Size([32, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 18])}
2023-10-07 11:45:54,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:54,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:54,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:54,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:54,870 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])))
2023-10-07 11:45:54,871 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])))
2023-10-07 11:45:54,871 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:54,872 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:54,874 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:54,874 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:54,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,875 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:54,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:54,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:54,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:54,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:54,876 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:54,876 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:54,876 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:54,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:54,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:54,878 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:54,878 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:54,878 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:54,878 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:54,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:54,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:54,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:54,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:55,036 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:55,047 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:55,048 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:55,079 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:55,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:55,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:55,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:55,083 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:55,083 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:55,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:55,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:55,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:55,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:55,086 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:55,087 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:55,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:55,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:55,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:55,097 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,097 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:55,097 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,097 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:55,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:55,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:55,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:55,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:55,105 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:55,105 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:55,105 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:55,106 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:55,112 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:55,119 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,119 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,119 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,120 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:55,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:55,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:55,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:55,136 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:55,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:55,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:55,152 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,152 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,153 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,153 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:55,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:55,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:55,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:55,168 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,169 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,169 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:55,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:55,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:55,183 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,183 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,183 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,183 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:55,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:55,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:55,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:55,198 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,199 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:55,200 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:55,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:55,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,213 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,213 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,213 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:55,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:55,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:55,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:55,229 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,229 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,229 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:55,231 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:55,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:55,244 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,244 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,244 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,245 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:55,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:55,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:55,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:55,261 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,262 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:55,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:55,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:55,277 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,277 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,277 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,277 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:55,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:55,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:55,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:55,294 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,295 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,295 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:55,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:55,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:55,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:55,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:55,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:55,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:55,327 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:55,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:55,336 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:55,343 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,343 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,343 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,343 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:55,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:55,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:55,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:55,362 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,362 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,362 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:55,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:55,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:55,377 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,377 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,377 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:55,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:55,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:55,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:55,393 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,394 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,394 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:55,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:55,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:55,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,408 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:55,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:55,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:55,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:55,423 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,424 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,424 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:55,425 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:55,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:55,439 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,439 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,439 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,439 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:55,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:55,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:55,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:55,451 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,451 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:55,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:55,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:55,466 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,467 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,467 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,467 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:55,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:55,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:55,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:55,479 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,479 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,480 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:55,481 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:55,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:55,494 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,495 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,495 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:55,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:55,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:55,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:55,508 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,509 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,509 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:55,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:55,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:55,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,524 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:55,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:55,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:55,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:55,536 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,537 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,537 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:55,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:55,544 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:55,551 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,551 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,552 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,552 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:55,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:55,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:55,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:55,565 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,566 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:55,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:55,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:55,580 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,580 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,580 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,580 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:55,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:55,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:55,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:55,595 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,596 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:55,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:55,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:55,610 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,611 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,611 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,611 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:55,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:55,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:55,621 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:55,624 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,624 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,625 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:55,626 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:55,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:55,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,639 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:55,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:55,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:55,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:55,652 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,652 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:55,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:55,660 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:55,667 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,667 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,667 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,668 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:55,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:55,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:55,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:55,681 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,681 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,682 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:55,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:55,690 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:55,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,697 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:55,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:55,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:55,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:55,712 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,712 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,713 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:55,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:55,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:55,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,727 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,727 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:55,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:55,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:55,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:55,740 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,741 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,741 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:55,743 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:55,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:55,755 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,756 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:55,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:55,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:55,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:55,769 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,769 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,769 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:55,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:55,782 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:55,792 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,793 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:55,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:55,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:55,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:55,815 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,816 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,816 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:55,818 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:55,824 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:55,825 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,825 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 18]), torch.Size([128, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 19])}
2023-10-07 11:45:55,825 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,826 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 18]), torch.Size([32, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 19])}
2023-10-07 11:45:55,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:55,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:55,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:55,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:55,840 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])))
2023-10-07 11:45:55,840 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])))
2023-10-07 11:45:55,840 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:55,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:55,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:55,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:55,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:55,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:55,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:55,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:55,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:55,845 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:55,845 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:55,845 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:55,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:55,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:55,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:55,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:55,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:55,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:55,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:55,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:55,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:55,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:56,012 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:56,014 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:56,014 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:56,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:56,050 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:56,052 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:56,052 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:56,052 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:56,052 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:56,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:56,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:56,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:56,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:56,053 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:56,054 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:56,054 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:56,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:56,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:56,063 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,063 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:56,063 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,063 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:56,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:56,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:56,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:56,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:56,064 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:56,064 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:56,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:56,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:56,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:56,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,079 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,079 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:56,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:56,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:56,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:56,093 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,094 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,094 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:56,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:56,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:56,108 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,109 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,109 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,109 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:56,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:56,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:56,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:56,123 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,124 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,124 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:56,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:56,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:56,140 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,140 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:56,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:56,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:56,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:56,154 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,154 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,155 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:56,156 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:56,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:56,169 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,170 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,170 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:56,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:56,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:56,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:56,183 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,184 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,184 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:56,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:56,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:56,200 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,200 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,200 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,200 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:56,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:56,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:56,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:56,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:56,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:56,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:56,229 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,229 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,230 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:56,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:56,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:56,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:56,247 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,248 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,248 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:56,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:56,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:56,264 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,264 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,265 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:56,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:56,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:56,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:56,284 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,285 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,285 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:56,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:56,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:56,299 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,300 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,300 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,300 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:56,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:56,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:56,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:56,320 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,320 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,320 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:56,322 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:56,329 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:56,335 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,335 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,336 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,336 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:56,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:56,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:56,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:56,356 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:56,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:56,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:56,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:56,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:56,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:56,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:56,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,392 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:56,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:56,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:56,407 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,407 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:56,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:56,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:56,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:56,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,427 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,427 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:56,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:56,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:56,442 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,442 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,442 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:56,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:56,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:56,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:56,463 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:56,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:56,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:56,479 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,479 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,479 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,480 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:56,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:56,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:56,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:56,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,499 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,499 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:56,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:56,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:56,514 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,514 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,515 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,515 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:56,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:56,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:56,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:56,534 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,535 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,535 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:56,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:56,543 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:56,550 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,550 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,551 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,551 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:56,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:56,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:56,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:56,589 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,590 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,590 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:56,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:56,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:56,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,606 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,606 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:56,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:56,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:56,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:56,622 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,622 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:56,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:56,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:56,638 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,638 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,639 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:56,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:56,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:56,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:56,652 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,653 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,653 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:56,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:56,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:56,668 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,668 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,668 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:56,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:56,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:56,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:56,682 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,683 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:56,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:56,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:56,698 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,698 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,698 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,699 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:56,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:56,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:56,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:56,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,716 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:56,719 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:56,725 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:56,731 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,732 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,732 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:56,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:56,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:56,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:56,747 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,748 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,748 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:56,750 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:56,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:56,763 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,763 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,764 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:56,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:56,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:56,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:56,777 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,778 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:56,780 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:56,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:56,793 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,794 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:56,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:56,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:56,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:56,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,809 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:56,810 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:56,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:56,823 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,823 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,824 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,824 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:56,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:56,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:56,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:56,837 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,838 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,838 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:56,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:56,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:56,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 19]), torch.Size([128, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 20])}
2023-10-07 11:45:56,848 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,848 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 19]), torch.Size([32, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 20])}
2023-10-07 11:45:56,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:56,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:56,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:56,859 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:56,863 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])))
2023-10-07 11:45:56,863 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])))
2023-10-07 11:45:56,863 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:56,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:56,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:56,867 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,867 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:56,867 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,867 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:56,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:56,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:56,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:56,868 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:56,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:56,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:56,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:56,870 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:56,870 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:56,870 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:56,870 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:56,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:56,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:56,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:57,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:57,044 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:57,045 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:57,046 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:57,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:57,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:57,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:57,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:57,080 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:57,080 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:57,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:57,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:57,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:57,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:57,082 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:57,082 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:57,082 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:57,083 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:57,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:57,091 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:57,091 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,091 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:57,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:57,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:57,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:57,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:57,092 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:57,092 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:57,092 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:57,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:57,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:57,105 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,105 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,106 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,106 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:57,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:57,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:57,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:57,122 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,122 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,123 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:57,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:57,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:57,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,137 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,137 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:57,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:57,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:57,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:57,153 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:57,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:57,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:57,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:57,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:57,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:57,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:57,183 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,184 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,184 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:57,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:57,191 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:57,199 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,199 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,199 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,199 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:57,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:57,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:57,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:57,215 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,215 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:57,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:57,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:57,230 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,230 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,230 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:57,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:57,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:57,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:57,246 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,246 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,246 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:57,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:57,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:57,261 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,261 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,261 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,261 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:57,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:57,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:57,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:57,280 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,281 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,281 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:57,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:57,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:57,296 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,297 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,297 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:57,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:57,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:57,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:57,312 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,313 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,313 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:57,314 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:57,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:57,326 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,327 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,327 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:57,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:57,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:57,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:57,359 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,361 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,361 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:57,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:57,369 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:57,377 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,377 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,378 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:57,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:57,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:57,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:57,393 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,394 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,394 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:57,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:57,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:57,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,409 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:57,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:57,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:57,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:57,423 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,424 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,425 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:57,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:57,432 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:57,438 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,438 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,438 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:57,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:57,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:57,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:57,452 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,453 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,453 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:57,455 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:57,461 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:57,468 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,468 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,468 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,468 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:57,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:57,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:57,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:57,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,483 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:57,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:57,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:57,498 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:57,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:57,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:57,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:57,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:57,516 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:57,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:57,528 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,529 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:57,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:57,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:57,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:57,548 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,549 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,549 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:57,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:57,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:57,564 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,564 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,565 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,565 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:57,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:57,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:57,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:57,579 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,579 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,580 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:57,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:57,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:57,593 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,594 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,594 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,594 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:57,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:57,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:57,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:57,610 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,611 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,611 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:57,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:57,618 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:57,625 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,625 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,625 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,626 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:57,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:57,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:57,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:57,639 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,640 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,640 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:57,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:57,648 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:57,654 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,654 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:57,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:57,662 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:57,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:57,668 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,669 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,669 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:57,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:57,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:57,682 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,683 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,683 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,683 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:57,687 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:57,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:57,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:57,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,698 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:57,700 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:57,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:57,712 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,713 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:57,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:57,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:57,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:57,726 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,727 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,727 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:57,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:57,734 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:57,741 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,741 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,741 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,741 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:57,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:57,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:57,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:57,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:57,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:57,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:57,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,771 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,771 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:57,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:57,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:57,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:57,785 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,786 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,786 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:57,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:57,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:57,800 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,800 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:57,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:57,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:57,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:57,814 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,815 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,815 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:57,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:57,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:57,824 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,825 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 20]), torch.Size([128, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 21])}
2023-10-07 11:45:57,825 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,825 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 20]), torch.Size([32, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 21])}
2023-10-07 11:45:57,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:57,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:57,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:57,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:57,839 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])))
2023-10-07 11:45:57,840 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])))
2023-10-07 11:45:57,840 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:57,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:57,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:57,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:57,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:57,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:57,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:57,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:57,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:57,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:57,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:57,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:57,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:57,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:57,851 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:57,851 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:57,851 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:57,851 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:57,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:57,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:57,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:57,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:58,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:58,024 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:58,025 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:58,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:58,058 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:58,060 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:58,060 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:58,061 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:58,061 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:58,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:58,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:58,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:58,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:58,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:58,064 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:58,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:58,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:58,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:58,073 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,073 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:58,073 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,073 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:58,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:58,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:58,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:58,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:58,074 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:58,074 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:58,074 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:58,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:58,081 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:58,088 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,088 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,088 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,088 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:58,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:58,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:58,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:58,103 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,104 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,104 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:58,106 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:58,112 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:58,119 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,119 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,119 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,120 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:58,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:58,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:58,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:58,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,135 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,135 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:58,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:58,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:58,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:58,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:58,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:58,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:58,164 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,165 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,165 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:58,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:58,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:58,179 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,180 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,180 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,180 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:58,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:58,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:58,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:58,194 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,195 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:58,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:58,203 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:58,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,211 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,211 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,211 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:58,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:58,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:58,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:58,225 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,226 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,226 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:58,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:58,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:58,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,241 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,241 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:58,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:58,250 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:58,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:58,261 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,262 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:58,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:58,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:58,286 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,287 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,287 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,287 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:58,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:58,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:58,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:58,307 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,307 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,308 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:58,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:58,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:58,323 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,324 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,324 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:58,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:58,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:58,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:58,345 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,346 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,346 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:58,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:58,354 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:58,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,362 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:58,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:58,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:58,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:58,381 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,383 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:58,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:58,392 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:58,398 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,399 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,399 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,399 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:58,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:58,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:58,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:58,417 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,418 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,418 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:58,419 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:58,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:58,434 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,434 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,434 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,434 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:58,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:58,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:58,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:58,451 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,451 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:58,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:58,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:58,466 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,466 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,466 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,466 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:58,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:58,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:58,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:58,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,484 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:58,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:58,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:58,499 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,500 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,500 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:58,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:58,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:58,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:58,515 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,516 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,516 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:58,518 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:58,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:58,532 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,532 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,532 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,532 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:58,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:58,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:58,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:58,547 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,548 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,548 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:58,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:58,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:58,563 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,563 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,563 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:58,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:58,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:58,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:58,578 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,579 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,579 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:58,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:58,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:58,593 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,594 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,594 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,594 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:58,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:58,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:58,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:58,609 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,610 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,610 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:58,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:58,618 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:58,625 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,625 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,625 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,625 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:58,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:58,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:58,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:58,640 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,640 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,640 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:58,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:58,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:58,655 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,655 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,655 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,655 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:58,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:58,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:58,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:58,670 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,671 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,671 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:58,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:58,678 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:58,685 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,686 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,686 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,686 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,686 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:58,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:58,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:58,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:58,700 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,701 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,701 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:58,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:58,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:58,715 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,715 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,715 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,715 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:58,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:58,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:58,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:58,729 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,730 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,730 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:58,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:58,738 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:58,745 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,745 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,745 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:58,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:58,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:58,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:58,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,761 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,761 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:58,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:58,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:58,776 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,777 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,777 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,777 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:58,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:58,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:58,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:58,790 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,791 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:58,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:58,799 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:58,807 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,807 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,807 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,807 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:58,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:58,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:58,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:58,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,823 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,823 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:58,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:58,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:58,832 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,833 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 21]), torch.Size([128, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 22])}
2023-10-07 11:45:58,833 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 21]), torch.Size([32, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 22])}
2023-10-07 11:45:58,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:58,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:58,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:58,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:58,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])))
2023-10-07 11:45:58,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])))
2023-10-07 11:45:58,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:58,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:58,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:58,852 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,852 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:58,853 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:58,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:58,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:58,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:58,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:58,854 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:58,854 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:58,854 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:58,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:58,855 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:58,856 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:58,856 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:58,856 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:58,856 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:58,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:58,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:58,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:45:58,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:45:59,018 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:45:59,020 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:45:59,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:45:59,050 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:59,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:59,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:45:59,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:59,054 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:45:59,055 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:59,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:45:59,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:45:59,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:45:59,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:45:59,057 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:59,057 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:59,057 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:45:59,058 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:45:59,059 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:59,066 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,066 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:59,066 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,066 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:59,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:45:59,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:45:59,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:45:59,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:45:59,067 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:59,067 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:59,067 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:45:59,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:45:59,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:59,080 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,080 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,080 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,081 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:45:59,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:45:59,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:45:59,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:45:59,094 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,095 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,095 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:45:59,096 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:45:59,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:59,109 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,109 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,109 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,110 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:45:59,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:45:59,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:45:59,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:45:59,123 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,124 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,124 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:45:59,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:45:59,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:59,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,139 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:45:59,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:45:59,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:45:59,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:45:59,153 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,154 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:45:59,156 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:45:59,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:59,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:45:59,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:45:59,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:45:59,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:45:59,183 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,183 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,183 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:45:59,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:45:59,191 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:59,198 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,198 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,198 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,198 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:45:59,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:45:59,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:45:59,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:45:59,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:45:59,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:45:59,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:59,228 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,228 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,229 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,229 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:45:59,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:45:59,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:45:59,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:45:59,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:45:59,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:45:59,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:59,259 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,259 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,259 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,259 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:45:59,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:45:59,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:45:59,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:45:59,273 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,274 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,274 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:45:59,275 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:45:59,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:59,288 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,288 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,288 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,288 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:45:59,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:45:59,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:45:59,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:45:59,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:45:59,310 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:45:59,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:59,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,325 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:45:59,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:45:59,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:45:59,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:45:59,339 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,340 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,340 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:45:59,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:45:59,349 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:59,355 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,355 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,355 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,356 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:45:59,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:45:59,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:45:59,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:45:59,371 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,371 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,371 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:45:59,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:45:59,380 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:59,387 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,387 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,388 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,388 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:45:59,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:45:59,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:45:59,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:45:59,403 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,403 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,404 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:45:59,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:45:59,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:59,419 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,419 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,419 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,419 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:45:59,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:45:59,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:45:59,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:45:59,435 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,435 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,435 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:45:59,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:45:59,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:59,450 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,451 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,451 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,451 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:45:59,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:45:59,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:45:59,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:45:59,466 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,467 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,467 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:45:59,469 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:45:59,475 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:59,482 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,482 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,482 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,482 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:45:59,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:45:59,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:45:59,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:45:59,497 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,497 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,497 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:45:59,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:45:59,506 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:59,513 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,513 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,514 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,514 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:45:59,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:45:59,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:45:59,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:45:59,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,531 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,531 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:45:59,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:45:59,540 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:59,546 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,546 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,546 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:45:59,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:45:59,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:45:59,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:45:59,561 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,562 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,562 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:45:59,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:45:59,570 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:59,576 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,577 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,577 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,577 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:45:59,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:45:59,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:45:59,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:45:59,593 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,593 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,593 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:45:59,595 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:45:59,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:59,609 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,610 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,610 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:45:59,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:45:59,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:45:59,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:45:59,626 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,627 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,627 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:45:59,629 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:45:59,635 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:59,642 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,642 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,643 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,643 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:45:59,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:45:59,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:45:59,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:45:59,658 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,659 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,659 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:45:59,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:45:59,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:59,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,675 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:45:59,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:45:59,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:45:59,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:45:59,694 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,695 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,695 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:45:59,697 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:45:59,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:59,710 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,710 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,710 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,710 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:45:59,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:45:59,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:45:59,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:45:59,731 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,731 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,731 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:45:59,734 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:45:59,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:59,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,747 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,747 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,747 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:45:59,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:45:59,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:45:59,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:45:59,766 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,767 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:45:59,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:45:59,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:59,782 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,782 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,782 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,783 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:45:59,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:45:59,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:45:59,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:45:59,803 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,803 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,804 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:45:59,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:45:59,812 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:59,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 22]), torch.Size([128, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 23])}
2023-10-07 11:45:59,814 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 22]), torch.Size([32, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 23])}
2023-10-07 11:45:59,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:45:59,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:45:59,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:45:59,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:45:59,838 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])))
2023-10-07 11:45:59,839 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])))
2023-10-07 11:45:59,839 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:45:59,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:45:59,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:59,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:59,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,845 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:59,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:45:59,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:45:59,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:45:59,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:45:59,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:45:59,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:45:59,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:45:59,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:45:59,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:45:59,852 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:45:59,853 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:45:59,853 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:45:59,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:45:59,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:45:59,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:45:59,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:00,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:00,064 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:00,066 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:00,066 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:00,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:00,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:00,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:00,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:00,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:00,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:00,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:00,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:00,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:00,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:00,101 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:00,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:00,101 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:00,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:00,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:00,110 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,110 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:00,110 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,110 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:00,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:00,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:00,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:00,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:00,112 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:00,113 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:00,113 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:00,113 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:00,119 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:00,126 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,126 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,126 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,127 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:00,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:00,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:00,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:00,140 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,141 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,141 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:00,142 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:00,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:00,155 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,155 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,155 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,155 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:00,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:00,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:00,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:00,169 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,170 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,170 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:00,171 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:00,177 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:00,184 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,184 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,185 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:00,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:00,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:00,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:00,198 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,199 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:00,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:00,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:00,214 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,214 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,214 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:00,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:00,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:00,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:00,229 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,230 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,230 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:00,231 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:00,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:00,243 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,244 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,244 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,244 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:00,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:00,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:00,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:00,258 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,258 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,258 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:00,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:00,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:00,273 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,273 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,273 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,273 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:00,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:00,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:00,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:00,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,288 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,288 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:00,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:00,295 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:00,302 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,302 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,303 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,303 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:00,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:00,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:00,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:00,326 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,326 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,326 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:00,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:00,334 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:00,340 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,340 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,341 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,341 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:00,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:00,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:00,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:00,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,355 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,355 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:00,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:00,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:00,370 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,371 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,371 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,371 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:00,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:00,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:00,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:00,385 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,386 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,386 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:00,388 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:00,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:00,401 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,401 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,401 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,401 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:00,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:00,410 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:00,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:00,418 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,418 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,419 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:00,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:00,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:00,433 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,434 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,434 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,434 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,434 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:00,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:00,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:00,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:00,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,448 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,449 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:00,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:00,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:00,463 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,463 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,464 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,464 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:00,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:00,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:00,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:00,479 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,480 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,480 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:00,481 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:00,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:00,495 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,495 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,495 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:00,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:00,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:00,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:00,512 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,512 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,513 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:00,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:00,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:00,527 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,527 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,527 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:00,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:00,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:00,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:00,544 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,545 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,545 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:00,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:00,553 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:00,560 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,560 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,561 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,561 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:00,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:00,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:00,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:00,575 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,575 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,575 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:00,577 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:00,584 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:00,590 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,591 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,591 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,591 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:00,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:00,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:00,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:00,607 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,608 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,608 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:00,610 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:00,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:00,623 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,623 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,623 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,624 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:00,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:00,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:00,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:00,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,638 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,639 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:00,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:00,647 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:00,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:00,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:00,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:00,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:00,667 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,668 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,669 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:00,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:00,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:00,682 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,682 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,682 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,682 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:00,686 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:00,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:00,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:00,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,698 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:00,700 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:00,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:00,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:00,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:00,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:00,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:00,728 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,729 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,729 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:00,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:00,737 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:00,743 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,743 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,743 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,744 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:00,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:00,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:00,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:00,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,758 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,759 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:00,760 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:00,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:00,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,774 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,774 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,774 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:00,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:00,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:00,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:00,788 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,789 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,789 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:00,790 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:00,797 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:00,804 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,804 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,804 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,804 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:00,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:00,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:00,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:00,820 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,821 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:00,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:00,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:00,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 23]), torch.Size([128, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 24])}
2023-10-07 11:46:00,830 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,831 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 23]), torch.Size([32, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 24])}
2023-10-07 11:46:00,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:00,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:00,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:00,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:00,847 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])))
2023-10-07 11:46:00,847 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])))
2023-10-07 11:46:00,847 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:00,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:00,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:00,851 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,851 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:00,851 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,851 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:00,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:00,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:00,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:00,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:00,852 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:00,852 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:00,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:00,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:00,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:00,854 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:00,854 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:00,854 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:00,854 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:00,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:00,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:00,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:00,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:01,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:01,029 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:01,029 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:01,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:01,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:01,063 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:01,064 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:01,064 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:01,064 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:01,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:01,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:01,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:01,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:01,065 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:01,066 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:01,066 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:01,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:01,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:01,074 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,074 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:01,075 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,075 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:01,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:01,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:01,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:01,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:01,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:01,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:01,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:01,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:01,083 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:01,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,090 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,090 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:01,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:01,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:01,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:01,105 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,106 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,106 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:01,107 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:01,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:01,121 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,121 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,122 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,122 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:01,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:01,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:01,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:01,138 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,138 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,139 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:01,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:01,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:01,154 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,154 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,154 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,154 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:01,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:01,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:01,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:01,170 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,171 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,172 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:01,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:01,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:01,186 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,187 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:01,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:01,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:01,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:01,203 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,204 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,204 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:01,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:01,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:01,219 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,219 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,220 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,220 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:01,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:01,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:01,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:01,235 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,236 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:01,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:01,244 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:01,250 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,251 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,251 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:01,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:01,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:01,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:01,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,266 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,267 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:01,268 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:01,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:01,281 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,282 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,282 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:01,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:01,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:01,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:01,298 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,299 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,299 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:01,300 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:01,308 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:01,315 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,315 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,315 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,316 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:01,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:01,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:01,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:01,331 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,332 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,332 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:01,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:01,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:01,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,347 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,347 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:01,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:01,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:01,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:01,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,362 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,362 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:01,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:01,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:01,376 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,377 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:01,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:01,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:01,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:01,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,391 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:01,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:01,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:01,406 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,407 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:01,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:01,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:01,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:01,422 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,423 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,424 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:01,429 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:01,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:01,443 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,443 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,443 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:01,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:01,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:01,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:01,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,458 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,458 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:01,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:01,466 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:01,472 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,473 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:01,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:01,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:01,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:01,490 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,491 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,491 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:01,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:01,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:01,505 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,506 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,506 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,506 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:01,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:01,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:01,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:01,524 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,525 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,525 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:01,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:01,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:01,540 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,540 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,540 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:01,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:01,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:01,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:01,558 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,558 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,559 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:01,560 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:01,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:01,573 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,573 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,573 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,574 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:01,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:01,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:01,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:01,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,591 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,592 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:01,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:01,599 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:01,606 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,606 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,607 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,607 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:01,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:01,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:01,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:01,625 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,625 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,626 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:01,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:01,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:01,640 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,640 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,640 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,641 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:01,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:01,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:01,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:01,659 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,660 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,660 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:01,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:01,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:01,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:01,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:01,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:01,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:01,692 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,693 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,693 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:01,695 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:01,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:01,707 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,708 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,708 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,708 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:01,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:01,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:01,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:01,726 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,727 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,727 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:01,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:01,734 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:01,741 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,741 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,741 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,742 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:01,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:01,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:01,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:01,760 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,761 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,761 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:01,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:01,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:01,774 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,775 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:01,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:01,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:01,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:01,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,795 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:01,797 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:01,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:01,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:01,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:01,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:01,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:01,829 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:01,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:01,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:01,840 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,840 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 24]), torch.Size([128, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 25])}
2023-10-07 11:46:01,840 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,840 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 24]), torch.Size([32, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 25])}
2023-10-07 11:46:01,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:01,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:01,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:01,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:01,854 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])))
2023-10-07 11:46:01,855 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])))
2023-10-07 11:46:01,855 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:01,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:01,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:01,859 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,859 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:01,859 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,859 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:01,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:01,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:01,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:01,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:01,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:01,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:01,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:01,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:01,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:01,863 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:01,863 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:01,863 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:01,863 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:01,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:01,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:01,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:01,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:02,030 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:02,032 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:02,032 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:02,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:02,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:02,068 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:02,068 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:02,069 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:02,069 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:02,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:02,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:02,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:02,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:02,071 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:02,071 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:02,072 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:02,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:02,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:02,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,081 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:02,081 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,081 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:02,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:02,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:02,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:02,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:02,082 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:02,082 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:02,082 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:02,083 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:02,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:02,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,095 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,096 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,096 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:02,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:02,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:02,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:02,113 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,114 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,114 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:02,115 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:02,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:02,129 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,129 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,129 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,129 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:02,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:02,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:02,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:02,145 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,146 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,146 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:02,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:02,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:02,160 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,160 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,160 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,160 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:02,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:02,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:02,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:02,174 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,175 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,175 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:02,177 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:02,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:02,190 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,190 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:02,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:02,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:02,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:02,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,207 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,207 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:02,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:02,214 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:02,221 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,221 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,222 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,222 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:02,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:02,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:02,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:02,237 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,238 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,238 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:02,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:02,246 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:02,253 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,253 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,253 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,253 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:02,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:02,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:02,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:02,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,269 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:02,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:02,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:02,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,284 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,284 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:02,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:02,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:02,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:02,298 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,299 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,299 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:02,301 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:02,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:02,313 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,313 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,314 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,314 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:02,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:02,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:02,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:02,335 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:02,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:02,365 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:02,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:02,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:02,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:02,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:02,390 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,392 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:02,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:02,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:02,414 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,414 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,415 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,415 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:02,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:02,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:02,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:02,441 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,442 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,442 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:02,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:02,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:02,460 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,461 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:02,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:02,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:02,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:02,484 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,485 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:02,487 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:02,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:02,499 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,500 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,500 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,500 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:02,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:02,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:02,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:02,521 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,521 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,522 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:02,523 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:02,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:02,535 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,535 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,536 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,536 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:02,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:02,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:02,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:02,548 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,549 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,549 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:02,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:02,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:02,563 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,563 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,564 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:02,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:02,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:02,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:02,578 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,579 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,579 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:02,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:02,586 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:02,593 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,593 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,593 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:02,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:02,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:02,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:02,621 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,622 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:02,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:02,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:02,637 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,638 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,638 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:02,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:02,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:02,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:02,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:02,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:02,664 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:02,671 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,672 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,672 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,672 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:02,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:02,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:02,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:02,692 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,696 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,697 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:02,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:02,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:02,711 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,712 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,712 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,712 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:02,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:02,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:02,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:02,732 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,733 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,733 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:02,734 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:02,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:02,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,747 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,747 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,747 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:02,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:02,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:02,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:02,763 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,764 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:02,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:02,771 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:02,777 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,778 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,778 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,778 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:02,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:02,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:02,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:02,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,796 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,796 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:02,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:02,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:02,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:02,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:02,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:02,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:02,828 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,829 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:02,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:02,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:02,842 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,842 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,842 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,843 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:02,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:02,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:02,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:02,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:02,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:02,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:02,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,875 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:02,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:02,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:02,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:02,889 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,890 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,890 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:02,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:02,898 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:02,899 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,899 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 25]), torch.Size([128, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 26])}
2023-10-07 11:46:02,900 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,900 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 25]), torch.Size([32, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 26])}
2023-10-07 11:46:02,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:02,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:02,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:02,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:02,917 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])))
2023-10-07 11:46:02,918 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])))
2023-10-07 11:46:02,918 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:02,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:02,921 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:02,921 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,921 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:02,922 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,922 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:02,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:02,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:02,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:02,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:02,923 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:02,923 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:02,923 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:02,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:02,924 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:02,925 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:02,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:02,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:02,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:02,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:02,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:03,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:03,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:03,087 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:03,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:03,089 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:03,122 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:03,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:03,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:03,126 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:03,126 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:03,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:03,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:03,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:03,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:03,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:03,127 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:03,127 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:03,127 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:03,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:03,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:03,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,136 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:03,136 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,136 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:03,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:03,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:03,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:03,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:03,137 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:03,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:03,138 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:03,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:03,144 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:03,151 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,151 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:03,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:03,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:03,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:03,168 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,168 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,168 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:03,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:03,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:03,181 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,182 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,182 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,182 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:03,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:03,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:03,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:03,196 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,196 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,197 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:03,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:03,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:03,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,211 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,211 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,211 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:03,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:03,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:03,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:03,226 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,226 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,226 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:03,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:03,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:03,240 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,240 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,241 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,241 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:03,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:03,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:03,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:03,254 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,255 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,255 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:03,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:03,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:03,269 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,270 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,270 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:03,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:03,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:03,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:03,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,283 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:03,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:03,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:03,297 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,297 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,297 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,297 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:03,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:03,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:03,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:03,316 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:03,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:03,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:03,330 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,330 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:03,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:03,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:03,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:03,348 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,349 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,349 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:03,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:03,356 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:03,362 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,362 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,362 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:03,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:03,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:03,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:03,381 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,382 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:03,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:03,389 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:03,395 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,396 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,396 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,396 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:03,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:03,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:03,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:03,413 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,413 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,413 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:03,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:03,421 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:03,428 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,428 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,428 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,428 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:03,434 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:03,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:03,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:03,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,445 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:03,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:03,452 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:03,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,459 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,460 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,460 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:03,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:03,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:03,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:03,476 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,477 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,477 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:03,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:03,484 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:03,490 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,491 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,491 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,491 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:03,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:03,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:03,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:03,507 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,508 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:03,509 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:03,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:03,522 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,522 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,523 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,523 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:03,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:03,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:03,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:03,538 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,539 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,539 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:03,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:03,547 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:03,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,553 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,554 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:03,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:03,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:03,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:03,567 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,568 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:03,570 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:03,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:03,582 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,582 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:03,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:03,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:03,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:03,596 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,598 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,598 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:03,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:03,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:03,612 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,613 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,613 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,613 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:03,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:03,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:03,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:03,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,639 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,640 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:03,641 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:03,648 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:03,655 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,655 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,655 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,655 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:03,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:03,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:03,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:03,670 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,670 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,670 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:03,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:03,678 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:03,684 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,685 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,685 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,685 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:03,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:03,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:03,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:03,701 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,702 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:03,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:03,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:03,717 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,717 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,717 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:03,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:03,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:03,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:03,735 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,736 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,736 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:03,738 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:03,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:03,750 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,751 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,751 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,751 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:03,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:03,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:03,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:03,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,765 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,766 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:03,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:03,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:03,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:03,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:03,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:03,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:03,797 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,799 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,799 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:03,801 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:03,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:03,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,813 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:03,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:03,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:03,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:03,829 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:03,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:03,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:03,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,845 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,845 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:03,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:03,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:03,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:03,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:03,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:03,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:03,870 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,870 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 26]), torch.Size([128, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 27])}
2023-10-07 11:46:03,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 26]), torch.Size([32, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 27])}
2023-10-07 11:46:03,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:03,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:03,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:03,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:03,886 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])))
2023-10-07 11:46:03,887 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])))
2023-10-07 11:46:03,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:03,888 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:03,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:03,890 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,890 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:03,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:03,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:03,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:03,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:03,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:03,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:03,892 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:03,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:03,893 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:03,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:03,894 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:03,895 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:03,895 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:03,895 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:03,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:03,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:03,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:04,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:04,061 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:04,062 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:04,062 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:04,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:04,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:04,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:04,095 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:04,095 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:04,095 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:04,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:04,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:04,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:04,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:04,097 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:04,098 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:04,098 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:04,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:04,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:04,107 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,108 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:04,108 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,108 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:04,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:04,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:04,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:04,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:04,109 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:04,109 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:04,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:04,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:04,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:04,127 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,128 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,128 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,128 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:04,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:04,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:04,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:04,144 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,145 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,145 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:04,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:04,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:04,160 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,160 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,160 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,160 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:04,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:04,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:04,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:04,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,178 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:04,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:04,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:04,192 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,193 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,193 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,193 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:04,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:04,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:04,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:04,217 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,218 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,218 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:04,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:04,226 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:04,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,234 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,234 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:04,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:04,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:04,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:04,250 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,250 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,250 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:04,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:04,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:04,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,266 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,266 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,266 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:04,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:04,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:04,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:04,282 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,282 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,282 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:04,284 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:04,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:04,297 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:04,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:04,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:04,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:04,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,315 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,315 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:04,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:04,323 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:04,330 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,330 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:04,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:04,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:04,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:04,346 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,347 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,347 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:04,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:04,355 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:04,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,362 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,362 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:04,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:04,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:04,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:04,377 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,378 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,378 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:04,380 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:04,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:04,393 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,394 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,394 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,394 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:04,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:04,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:04,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:04,411 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,411 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,412 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:04,413 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:04,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:04,427 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,427 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,427 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,427 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:04,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:04,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:04,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:04,445 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,446 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:04,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:04,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:04,461 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,461 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:04,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:04,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:04,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:04,477 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,478 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,478 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:04,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:04,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:04,492 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,493 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,493 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,493 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:04,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:04,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:04,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:04,509 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,510 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:04,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:04,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:04,524 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,525 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,525 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,525 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:04,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:04,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:04,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:04,556 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,557 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,557 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:04,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:04,565 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:04,571 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,571 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,572 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,572 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:04,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:04,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:04,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:04,589 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,590 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,590 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:04,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:04,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:04,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,605 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,606 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:04,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:04,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:04,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:04,625 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,625 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,626 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:04,628 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:04,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:04,641 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,641 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,641 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,642 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:04,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:04,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:04,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:04,657 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,658 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:04,660 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:04,666 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:04,673 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,673 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,673 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,674 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:04,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:04,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:04,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:04,689 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,690 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,690 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:04,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:04,698 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:04,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,706 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,706 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:04,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:04,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:04,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:04,723 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,724 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,724 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:04,726 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:04,733 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:04,740 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,740 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,740 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,741 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:04,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:04,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:04,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:04,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,759 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,759 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:04,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:04,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:04,775 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,776 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:04,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:04,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:04,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:04,795 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,796 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,796 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:04,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:04,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:04,811 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:04,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:04,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:04,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:04,831 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:04,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:04,841 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:04,848 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,848 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,849 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,849 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:04,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:04,859 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:04,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:04,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,869 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,869 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:04,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:04,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:04,887 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,887 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,888 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,888 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:04,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:04,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:04,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:04,906 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,907 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,907 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:04,909 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:04,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:04,916 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,917 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 27]), torch.Size([128, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 28])}
2023-10-07 11:46:04,917 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,917 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 27]), torch.Size([32, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 28])}
2023-10-07 11:46:04,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:04,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:04,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:04,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:04,936 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])))
2023-10-07 11:46:04,937 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])))
2023-10-07 11:46:04,937 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:04,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:04,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:04,942 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,943 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:04,943 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,943 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:04,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:04,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:04,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:04,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:04,949 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:04,949 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:04,949 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:04,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:04,951 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:04,951 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:04,952 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:04,952 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:04,952 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:04,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:04,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:05,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:05,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:05,123 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:05,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:05,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:05,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:05,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:05,176 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:05,177 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:05,177 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:05,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:05,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:05,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:05,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:05,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:05,178 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:05,178 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:05,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:05,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:05,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:05,187 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,187 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:05,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,187 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:05,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:05,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:05,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:05,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:05,188 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:05,188 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:05,189 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:05,189 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:05,195 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:05,202 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,202 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,203 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,203 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:05,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:05,212 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:05,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:05,220 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,220 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,221 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:05,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:05,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:05,235 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,235 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,235 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:05,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:05,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:05,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:05,251 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,252 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,252 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:05,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:05,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:05,267 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,267 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,267 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,267 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:05,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:05,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:05,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:05,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,284 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:05,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:05,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:05,298 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:05,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:05,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:05,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:05,315 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:05,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:05,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:05,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:05,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:05,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:05,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:05,347 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,348 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,348 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:05,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:05,356 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:05,363 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,363 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,363 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,363 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:05,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:05,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:05,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:05,380 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,381 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,381 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:05,382 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:05,389 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:05,396 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,396 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,396 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,396 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:05,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:05,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:05,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:05,412 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,413 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,413 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:05,414 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:05,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:05,427 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,427 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,427 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,427 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:05,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:05,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:05,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:05,443 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,444 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,444 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:05,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:05,452 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:05,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,459 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,459 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,459 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:05,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:05,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:05,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:05,474 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,475 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:05,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:05,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:05,490 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,491 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:05,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:05,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:05,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:05,508 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,509 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,509 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:05,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:05,516 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:05,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,523 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:05,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:05,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:05,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:05,539 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,540 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:05,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:05,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:05,555 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,555 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,555 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,556 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:05,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:05,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:05,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:05,571 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,572 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,572 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:05,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:05,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:05,587 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,587 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,587 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:05,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:05,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:05,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:05,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,604 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:05,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:05,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:05,619 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,619 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,619 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,619 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:05,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:05,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:05,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:05,636 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,637 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,637 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:05,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:05,644 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:05,651 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,651 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,651 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,652 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:05,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:05,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:05,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:05,667 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,668 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,668 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:05,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:05,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:05,683 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,683 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,683 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,683 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:05,687 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:05,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:05,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:05,699 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,700 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,700 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:05,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:05,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:05,714 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,714 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,715 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,715 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:05,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:05,724 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:05,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:05,732 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,733 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,733 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:05,735 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:05,741 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:05,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,748 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,748 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,748 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:05,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:05,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:05,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:05,765 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,766 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,766 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:05,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:05,774 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:05,781 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,781 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:05,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:05,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:05,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:05,798 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,798 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,799 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:05,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:05,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:05,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,813 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:05,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:05,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:05,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:05,829 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:05,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:05,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:05,845 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,845 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,845 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,845 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:05,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:05,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:05,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:05,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:05,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:05,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:05,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,876 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:05,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:05,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:05,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:05,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:05,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:05,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:05,907 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,907 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,908 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,908 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:05,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:05,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:05,921 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:05,925 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,926 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,926 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:05,928 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:05,934 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:05,936 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,936 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 28]), torch.Size([128, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 29])}
2023-10-07 11:46:05,936 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,936 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 28]), torch.Size([32, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 29])}
2023-10-07 11:46:05,936 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:05,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:05,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:05,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:05,951 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])))
2023-10-07 11:46:05,952 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])))
2023-10-07 11:46:05,952 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:05,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:05,955 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:05,955 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,955 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:05,956 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,956 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:05,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:05,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:05,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:05,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:05,957 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:05,957 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:05,957 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:05,957 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:05,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:05,959 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:05,959 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:05,959 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:05,959 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:05,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:06,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:06,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:06,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:06,129 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:06,131 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:06,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:06,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:06,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:06,165 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:06,165 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:06,165 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:06,166 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:06,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:06,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:06,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:06,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:06,167 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:06,167 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:06,168 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:06,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:06,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:06,176 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,177 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:06,177 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:06,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:06,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:06,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:06,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:06,178 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:06,179 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:06,179 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:06,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:06,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:06,192 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,192 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,192 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,193 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:06,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:06,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:06,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:06,208 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,209 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,209 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:06,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:06,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:06,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,223 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,224 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,224 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:06,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:06,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:06,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:06,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,240 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,240 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:06,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:06,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:06,254 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,255 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,255 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:06,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:06,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:06,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:06,270 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,270 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:06,272 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:06,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:06,285 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,285 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,285 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,285 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:06,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:06,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:06,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:06,301 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,301 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,302 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:06,303 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:06,309 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:06,316 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,316 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,316 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,317 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:06,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:06,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:06,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:06,332 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,333 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,333 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:06,334 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:06,340 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:06,347 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,348 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,348 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,348 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:06,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:06,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:06,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:06,365 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,365 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,365 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:06,367 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:06,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:06,380 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,380 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,381 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:06,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:06,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:06,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:06,398 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,399 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,399 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:06,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:06,406 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:06,413 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,413 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,413 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,413 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:06,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:06,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:06,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:06,428 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:06,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:06,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:06,443 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,443 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,443 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,444 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:06,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:06,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:06,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:06,459 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,460 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,460 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:06,462 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:06,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:06,475 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,475 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,475 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,475 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:06,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:06,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:06,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:06,490 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,491 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,491 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:06,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:06,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:06,506 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,506 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,506 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,506 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:06,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:06,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:06,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:06,521 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,522 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,522 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:06,524 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:06,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:06,537 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,537 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,537 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,537 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:06,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:06,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:06,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:06,598 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,609 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,610 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:06,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:06,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:06,626 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,627 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,627 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,627 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:06,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:06,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:06,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:06,643 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,644 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,644 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:06,647 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:06,653 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:06,659 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,660 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,660 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,660 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:06,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:06,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:06,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:06,679 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,680 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,680 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:06,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:06,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:06,695 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,696 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:06,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:06,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:06,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:06,711 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,712 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,712 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:06,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:06,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:06,727 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,727 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,728 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:06,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:06,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:06,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:06,744 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,744 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,744 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:06,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:06,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:06,759 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,759 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,759 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,760 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:06,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:06,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:06,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:06,775 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,776 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,776 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:06,778 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:06,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:06,791 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,791 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,791 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,791 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:06,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:06,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:06,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:06,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,807 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:06,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:06,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:06,823 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,823 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,823 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,823 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:06,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:06,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:06,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:06,838 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,838 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,839 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:06,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:06,847 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:06,853 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,853 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,854 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,854 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:06,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:06,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:06,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:06,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,869 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,869 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:06,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:06,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:06,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,884 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,884 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,884 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:06,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:06,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:06,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:06,899 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,899 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,900 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:06,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:06,908 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:06,914 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,915 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,915 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,915 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:06,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:06,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:06,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:06,930 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,931 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:06,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:06,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:06,945 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,945 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,945 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:06,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:06,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:06,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:06,960 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,961 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,961 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:06,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:06,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:06,970 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,971 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 29]), torch.Size([128, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 30])}
2023-10-07 11:46:06,971 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,971 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 29]), torch.Size([32, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 30])}
2023-10-07 11:46:06,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:06,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:06,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:06,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:06,987 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])))
2023-10-07 11:46:06,987 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])))
2023-10-07 11:46:06,988 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:06,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:06,990 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:06,991 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,991 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:06,991 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,991 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:06,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:06,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:06,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:06,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:06,993 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:06,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:06,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:06,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:06,994 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:06,995 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:06,995 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:06,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:06,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:06,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:07,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:07,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:07,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:07,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:07,154 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:07,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:07,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:07,187 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:07,189 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:07,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:07,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:07,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:07,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:07,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:07,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:07,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:07,191 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:07,191 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:07,191 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:07,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:07,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:07,199 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,200 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:07,200 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,200 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:07,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:07,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:07,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:07,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:07,201 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:07,201 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:07,201 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:07,202 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:07,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:07,215 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,215 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,216 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,216 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:07,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:07,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:07,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:07,231 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,232 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,232 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:07,233 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:07,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:07,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:07,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:07,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:07,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:07,262 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,263 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,263 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:07,264 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:07,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:07,277 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,278 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,278 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,278 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:07,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:07,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:07,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:07,292 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,293 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,293 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:07,294 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:07,301 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:07,308 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,308 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,308 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,308 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:07,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:07,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:07,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:07,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,324 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,324 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:07,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:07,331 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:07,338 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,339 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,339 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,339 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:07,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:07,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:07,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:07,370 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,371 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,371 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:07,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:07,379 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:07,386 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,387 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,387 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,387 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:07,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:07,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:07,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:07,403 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,404 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,404 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:07,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:07,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:07,420 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,420 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,420 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,420 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:07,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:07,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:07,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:07,436 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,437 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,437 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:07,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:07,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:07,452 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,452 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,452 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,452 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:07,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:07,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:07,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:07,468 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,469 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,469 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:07,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:07,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:07,485 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,485 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,485 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,485 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:07,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:07,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:07,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:07,502 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,503 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,503 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:07,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:07,512 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:07,519 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,519 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,519 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,519 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:07,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:07,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:07,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:07,536 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,536 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,537 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:07,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:07,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:07,552 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,552 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,552 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,553 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:07,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:07,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:07,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:07,569 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,570 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,570 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:07,572 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:07,578 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:07,585 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,585 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,586 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,586 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:07,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:07,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:07,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:07,606 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,607 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,607 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:07,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:07,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:07,622 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,622 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,622 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,622 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:07,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:07,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:07,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:07,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,638 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,639 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:07,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:07,647 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:07,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:07,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:07,662 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:07,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:07,670 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,671 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,671 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:07,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:07,678 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:07,685 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,685 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,685 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,686 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,686 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:07,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:07,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:07,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:07,701 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,702 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:07,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:07,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:07,716 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,716 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,717 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:07,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:07,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:07,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:07,732 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,733 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,733 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:07,735 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:07,741 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:07,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,748 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,748 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,748 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:07,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:07,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:07,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:07,763 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,764 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:07,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:07,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:07,779 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,779 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,780 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,780 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:07,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:07,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:07,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:07,795 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,796 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,797 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:07,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:07,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:07,811 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:07,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:07,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:07,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:07,828 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,829 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:07,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:07,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:07,844 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,844 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:07,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:07,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:07,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:07,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,861 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:07,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:07,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:07,876 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,876 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:07,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:07,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:07,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:07,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,892 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:07,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:07,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:07,908 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,908 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,908 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,908 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:07,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:07,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:07,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:07,924 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,924 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,925 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:07,926 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:07,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:07,940 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:07,940 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:07,940 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:07,940 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:07,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:07,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:07,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:07,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:07,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:07,991 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:07,991 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:07,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:08,000 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:08,001 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,001 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 30]), torch.Size([128, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 31])}
2023-10-07 11:46:08,002 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,002 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 30]), torch.Size([32, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 31])}
2023-10-07 11:46:08,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:08,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:08,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:08,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:08,017 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])))
2023-10-07 11:46:08,018 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])))
2023-10-07 11:46:08,019 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:08,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:08,021 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:08,022 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,022 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:08,022 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,022 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:08,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:08,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:08,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:08,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:08,024 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:08,024 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:08,024 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:08,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:08,026 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:08,027 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:08,027 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:08,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:08,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:08,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:08,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:08,191 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:08,193 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:08,193 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:08,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:08,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:08,226 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:08,226 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:08,226 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:08,226 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:08,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:08,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:08,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:08,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:08,228 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:08,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:08,228 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:08,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:08,230 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:08,237 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,237 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:08,237 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,237 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:08,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:08,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:08,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:08,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:08,238 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:08,239 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:08,239 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:08,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:08,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:08,252 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,252 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:08,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:08,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:08,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:08,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,268 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,269 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:08,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:08,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:08,283 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,283 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,283 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,284 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:08,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:08,292 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:08,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:08,299 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,300 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,300 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:08,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:08,308 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:08,314 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,315 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,315 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,315 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:08,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:08,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:08,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:08,330 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,331 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,331 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:08,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:08,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:08,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:08,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:08,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:08,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:08,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:08,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:08,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:08,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,379 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,379 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:08,383 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:08,387 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:08,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:08,394 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,395 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,395 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:08,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:08,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:08,409 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,409 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,410 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,410 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,410 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:08,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:08,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:08,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:08,425 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,425 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,425 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:08,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:08,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:08,440 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,440 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,440 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,440 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:08,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:08,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:08,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:08,455 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:08,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:08,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:08,470 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,470 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,471 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,471 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:08,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:08,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:08,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:08,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,492 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:08,494 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:08,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:08,507 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,507 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,508 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,508 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:08,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:08,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:08,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:08,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,523 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,524 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:08,525 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:08,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:08,538 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,539 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,539 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,539 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:08,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:08,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:08,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:08,554 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,555 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:08,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:08,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:08,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,571 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,571 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:08,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:08,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:08,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:08,586 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,586 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,587 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:08,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:08,595 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:08,603 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,603 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,603 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,604 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:08,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:08,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:08,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:08,620 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,621 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,621 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:08,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:08,629 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:08,635 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,636 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,636 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:08,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:08,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:08,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:08,652 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,653 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:08,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:08,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:08,667 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,667 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,668 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,668 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:08,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:08,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:08,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:08,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:08,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:08,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:08,700 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,701 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,701 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,701 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:08,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:08,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:08,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:08,718 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:08,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:08,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:08,734 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,734 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,734 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,734 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:08,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:08,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:08,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:08,750 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,751 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,751 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:08,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:08,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:08,766 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,766 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,766 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:08,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:08,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:08,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:08,784 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,784 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,785 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:08,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:08,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:08,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,801 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:08,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:08,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:08,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:08,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:08,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:08,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:08,833 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,833 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,833 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:08,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:08,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:08,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:08,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,850 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:08,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:08,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:08,864 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,864 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,865 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,865 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:08,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:08,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:08,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:08,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,882 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:08,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:08,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:08,897 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,897 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,897 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,898 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:08,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:08,907 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:08,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:08,914 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,916 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,916 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:08,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:08,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:08,940 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,940 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,941 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,941 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:08,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:08,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:08,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:08,963 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:08,964 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:08,964 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:08,966 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:08,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:08,983 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:08,983 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:08,983 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:08,983 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:08,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:08,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:08,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:08,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:09,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:09,004 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:09,004 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:09,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:09,012 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:09,013 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,014 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 31]), torch.Size([128, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 32])}
2023-10-07 11:46:09,014 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,014 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 31]), torch.Size([32, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 32])}
2023-10-07 11:46:09,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:09,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:09,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:09,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:09,031 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])))
2023-10-07 11:46:09,032 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])))
2023-10-07 11:46:09,032 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:09,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:09,035 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:09,036 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,036 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:09,036 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,036 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:09,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:09,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:09,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:09,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:09,037 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:09,037 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:09,037 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:09,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:09,039 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:09,039 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,039 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:09,040 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,040 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:09,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:09,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:09,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:09,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:09,216 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:09,218 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:09,219 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:09,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:09,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:09,253 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:09,253 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:09,254 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:09,254 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:09,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:09,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:09,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:09,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:09,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:09,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:09,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:09,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:09,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:09,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,265 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:09,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,265 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:09,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:09,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:09,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:09,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:09,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:09,267 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:09,267 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:09,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:09,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:09,281 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,282 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,282 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:09,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:09,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:09,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:09,298 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,299 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,299 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:09,300 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:09,306 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:09,314 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,314 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,314 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,314 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:09,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:09,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:09,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:09,331 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,331 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,331 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:09,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:09,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:09,347 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,347 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,347 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,347 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:09,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:09,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:09,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:09,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:09,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:09,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:09,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,380 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,380 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:09,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:09,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:09,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:09,396 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,397 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,397 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:09,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:09,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:09,412 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,412 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,412 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,412 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:09,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:09,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:09,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:09,429 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,430 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,430 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:09,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:09,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:09,445 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,446 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,446 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,446 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:09,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:09,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:09,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:09,461 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,462 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,462 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:09,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:09,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:09,477 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,477 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,478 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,478 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:09,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:09,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:09,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:09,493 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,494 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,494 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:09,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:09,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:09,509 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,509 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,509 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,509 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:09,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:09,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:09,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:09,527 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,527 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,528 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:09,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:09,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:09,543 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,543 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,544 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,544 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:09,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:09,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:09,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:09,559 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,560 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,560 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:09,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:09,568 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:09,575 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,575 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,575 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,575 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:09,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:09,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:09,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:09,593 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,594 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,594 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:09,596 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:09,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:09,609 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,610 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,610 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,610 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:09,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:09,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:09,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:09,628 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,629 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,629 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:09,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:09,637 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:09,644 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,644 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,644 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,645 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:09,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:09,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:09,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:09,661 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,661 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,661 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:09,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:09,669 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:09,676 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,676 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,676 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,676 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:09,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:09,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:09,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:09,694 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,694 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,694 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:09,696 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:09,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:09,709 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,710 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,710 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,710 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:09,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:09,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:09,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:09,725 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,726 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,726 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:09,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:09,734 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:09,741 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,741 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,741 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,741 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:09,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:09,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:09,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:09,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,763 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,763 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:09,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:09,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:09,778 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,779 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,779 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,779 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:09,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:09,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:09,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:09,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,797 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:09,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:09,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:09,812 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,812 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:09,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:09,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:09,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:09,833 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,833 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:09,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:09,842 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:09,849 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,849 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,849 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,849 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:09,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:09,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:09,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:09,865 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,866 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,866 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:09,867 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:09,873 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:09,880 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,881 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,881 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,881 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:09,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:09,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:09,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:09,897 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,897 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,897 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:09,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:09,906 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:09,912 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,912 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,913 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,913 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:09,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:09,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:09,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:09,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:09,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:09,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:09,945 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,945 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,945 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,945 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:09,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:09,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:09,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:09,961 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,962 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:09,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:09,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:09,977 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:09,977 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:09,978 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:09,978 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:09,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:09,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:09,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:09,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:09,994 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:09,995 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:09,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:09,996 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:10,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:10,009 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,010 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:10,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,010 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:10,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:10,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:10,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:10,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:10,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:10,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:10,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:10,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:10,036 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:10,038 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,038 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 32]), torch.Size([128, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 33])}
2023-10-07 11:46:10,038 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,038 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 32]), torch.Size([32, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 33])}
2023-10-07 11:46:10,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:10,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:10,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:10,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:10,062 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])))
2023-10-07 11:46:10,063 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])))
2023-10-07 11:46:10,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:10,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:10,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:10,069 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,069 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:10,069 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,069 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:10,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:10,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:10,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:10,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:10,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:10,072 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:10,072 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:10,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:10,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:10,076 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,076 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:10,077 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,077 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:10,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:10,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:10,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:10,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:10,267 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:10,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:10,269 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:10,306 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:10,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:10,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:10,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:10,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:10,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:10,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:10,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:10,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:10,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:10,311 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:10,311 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:10,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:10,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:10,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:10,321 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,321 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:10,321 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,321 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:10,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:10,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:10,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:10,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:10,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:10,323 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:10,323 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:10,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:10,331 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:10,339 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,339 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,340 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,340 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:10,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:10,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:10,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:10,357 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,358 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:10,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:10,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:10,374 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,375 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,375 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,375 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:10,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:10,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:10,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:10,392 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,393 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,393 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:10,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:10,401 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:10,409 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,409 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,410 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,410 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:10,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:10,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:10,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:10,428 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:10,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:10,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:10,445 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,445 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,445 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,445 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:10,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:10,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:10,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:10,466 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,467 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,468 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:10,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:10,476 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:10,483 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,483 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,483 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,483 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:10,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:10,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:10,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:10,505 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,506 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,506 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:10,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:10,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:10,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,521 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,522 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,522 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:10,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:10,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:10,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:10,542 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,543 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,544 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:10,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:10,552 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:10,559 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,559 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,559 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,559 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:10,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:10,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:10,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:10,580 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,582 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,582 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:10,584 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:10,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:10,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,597 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,597 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,597 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:10,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:10,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:10,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:10,619 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,621 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,621 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:10,623 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:10,629 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:10,636 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,636 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,637 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:10,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:10,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:10,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:10,657 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,658 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:10,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:10,667 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:10,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,674 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:10,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:10,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:10,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:10,696 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,697 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,697 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:10,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:10,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:10,712 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,712 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,713 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:10,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:10,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:10,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:10,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,735 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:10,737 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:10,744 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:10,751 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,751 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,751 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,752 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:10,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:10,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:10,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:10,775 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,775 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,776 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:10,777 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:10,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:10,791 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,791 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,792 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,792 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:10,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:10,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:10,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:10,812 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,813 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,813 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:10,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:10,821 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:10,828 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,828 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,828 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,829 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:10,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:10,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:10,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:10,845 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,846 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,846 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:10,847 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:10,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:10,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,861 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,861 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,861 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:10,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:10,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:10,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:10,877 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,878 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,878 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:10,880 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:10,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:10,893 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,893 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,893 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,893 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:10,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:10,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:10,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:10,909 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,910 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:10,911 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:10,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:10,925 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:10,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:10,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:10,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:10,942 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,943 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,943 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:10,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:10,951 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:10,958 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,958 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,958 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,958 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:10,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:10,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:10,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:10,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:10,976 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:10,977 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:10,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:10,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:10,991 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:10,991 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:10,992 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:10,992 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:10,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:10,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:11,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:11,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:11,010 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,011 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,011 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:11,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:11,019 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:11,026 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,026 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:11,026 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,026 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:11,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:11,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:11,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:11,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:11,043 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,044 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,044 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:11,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:11,051 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:11,058 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,058 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:11,059 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,059 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:11,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:11,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:11,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:11,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:11,075 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:11,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:11,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:11,091 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:11,091 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:11,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:11,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:11,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:11,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:11,108 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,109 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:11,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:11,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:11,123 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,123 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:11,123 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,124 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:11,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:11,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:11,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:11,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:11,141 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,141 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,142 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:11,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:11,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:11,151 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,151 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 33]), torch.Size([128, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 34])}
2023-10-07 11:46:11,151 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,151 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 33]), torch.Size([32, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 34])}
2023-10-07 11:46:11,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:11,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:11,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:11,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:11,167 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])))
2023-10-07 11:46:11,168 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])))
2023-10-07 11:46:11,168 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:11,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:11,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:11,171 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,171 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:11,171 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,172 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:11,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:11,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:11,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:11,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:11,173 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:11,173 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:11,173 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:11,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:11,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:11,175 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:11,175 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,175 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:11,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:11,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:11,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:11,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:11,343 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:11,345 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:11,346 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:11,376 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:11,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:11,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:11,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:11,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:11,380 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:11,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:11,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:11,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:11,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:11,382 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:11,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:11,382 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:11,382 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:11,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:11,390 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,390 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:11,391 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,391 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:11,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:11,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:11,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:11,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:11,392 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:11,392 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:11,392 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:11,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:11,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:11,406 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,406 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,406 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:11,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:11,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:11,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:11,424 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,425 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,425 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:11,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:11,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:11,439 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,440 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,440 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,440 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:11,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:11,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:11,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:11,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,459 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,459 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:11,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:11,467 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:11,474 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,474 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,474 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:11,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:11,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:11,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:11,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,491 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:11,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:11,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:11,506 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,506 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,506 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,507 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:11,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:11,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:11,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:11,524 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,525 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,525 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:11,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:11,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:11,540 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,540 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,540 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:11,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:11,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:11,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:11,557 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,558 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,558 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:11,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:11,566 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:11,572 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,573 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,573 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,573 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:11,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:11,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:11,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:11,589 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,590 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,590 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:11,591 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:11,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:11,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,605 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,605 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,605 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:11,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:11,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:11,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:11,623 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,624 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,625 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:11,626 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:11,633 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:11,640 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,640 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,640 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,640 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:11,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:11,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:11,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:11,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:11,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:11,665 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:11,672 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,672 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,672 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,672 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:11,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:11,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:11,687 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:11,691 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,691 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,692 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:11,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:11,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:11,706 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,706 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,707 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:11,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:11,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:11,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:11,728 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,729 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,729 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:11,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:11,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:11,743 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,743 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,744 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,744 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:11,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:11,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:11,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:11,767 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,768 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,768 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:11,771 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:11,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:11,789 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,790 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,790 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,790 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:11,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:11,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:11,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:11,811 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,812 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,812 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:11,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:11,822 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:11,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,831 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,831 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:11,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:11,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:11,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:11,850 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,851 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:11,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:11,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:11,867 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,867 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,868 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,868 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:11,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:11,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:11,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:11,886 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,887 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:11,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:11,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:11,902 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,902 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,903 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,903 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:11,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:11,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:11,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:11,922 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,923 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,923 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:11,925 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:11,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:11,938 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,938 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,939 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,939 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:11,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:11,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:11,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:11,960 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,961 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,961 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:11,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:11,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:11,975 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:11,975 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:11,976 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:11,976 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:11,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:11,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:11,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:11,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:11,995 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:11,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:11,996 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:11,998 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:12,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:12,011 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,012 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,012 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,012 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:12,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:12,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:12,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:12,031 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,032 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,033 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:12,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:12,040 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:12,047 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,047 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,047 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,047 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:12,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:12,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:12,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:12,066 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,067 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,067 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:12,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:12,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:12,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,083 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:12,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:12,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:12,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:12,102 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,103 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,103 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:12,104 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:12,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:12,117 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,117 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,117 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,117 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:12,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:12,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:12,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:12,137 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,138 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,138 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:12,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:12,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:12,154 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,154 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,154 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,154 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:12,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:12,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:12,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:12,173 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,174 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,174 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:12,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:12,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:12,189 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:12,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:12,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:12,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:12,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,207 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,208 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:12,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:12,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:12,218 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,218 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 34]), torch.Size([128, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 35])}
2023-10-07 11:46:12,218 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,218 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 34]), torch.Size([32, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 35])}
2023-10-07 11:46:12,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:12,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:12,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:12,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:12,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])))
2023-10-07 11:46:12,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])))
2023-10-07 11:46:12,238 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:12,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:12,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:12,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,241 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:12,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:12,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:12,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:12,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:12,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:12,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:12,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:12,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:12,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:12,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:12,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,247 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:12,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:12,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:12,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:12,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:12,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:12,427 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:12,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:12,430 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:12,461 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:12,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:12,464 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:12,465 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:12,465 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:12,465 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:12,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:12,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:12,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:12,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:12,466 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:12,466 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:12,466 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:12,467 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:12,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:12,474 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,475 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:12,475 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,475 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:12,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:12,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:12,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:12,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:12,476 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:12,476 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:12,476 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:12,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:12,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:12,491 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,491 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,491 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,491 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:12,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:12,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:12,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:12,507 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,508 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:12,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:12,516 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:12,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,524 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:12,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:12,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:12,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:12,540 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,540 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:12,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:12,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:12,556 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,556 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,557 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,557 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:12,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:12,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:12,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:12,572 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:12,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:12,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:12,588 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,588 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,588 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,589 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:12,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:12,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:12,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:12,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,605 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,605 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:12,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:12,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:12,619 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,619 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,620 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,620 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:12,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:12,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:12,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:12,642 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,643 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,643 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:12,645 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:12,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:12,658 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,658 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,659 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:12,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:12,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:12,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:12,679 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,681 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,681 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:12,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:12,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:12,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,697 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,697 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,697 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:12,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:12,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:12,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:12,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,716 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:12,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:12,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:12,731 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,732 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,732 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:12,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:12,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:12,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:12,750 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,751 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,751 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:12,753 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:12,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:12,766 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,767 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,767 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,767 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:12,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:12,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:12,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:12,785 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,786 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,786 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:12,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:12,794 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:12,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,801 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:12,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:12,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:12,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:12,820 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,821 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:12,822 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:12,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:12,836 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,836 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,837 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,837 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:12,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:12,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:12,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:12,855 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,856 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,856 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:12,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:12,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:12,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,871 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:12,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:12,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:12,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:12,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:12,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:12,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:12,908 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,908 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,908 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,909 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:12,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:12,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:12,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:12,927 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,928 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,928 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:12,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:12,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:12,943 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,943 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:12,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:12,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:12,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:12,959 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,959 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,959 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:12,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:12,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:12,974 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:12,974 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:12,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:12,975 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:12,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:12,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:12,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:12,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:12,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:12,990 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:12,991 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:12,992 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:12,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:13,006 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,006 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,006 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,006 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:13,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:13,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:13,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:13,021 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,022 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,022 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:13,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:13,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:13,037 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,037 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,038 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,038 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:13,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:13,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:13,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:13,054 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,054 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,055 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:13,056 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:13,063 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:13,070 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,070 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,070 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,070 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:13,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:13,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:13,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:13,086 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,086 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,087 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:13,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:13,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:13,102 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,102 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,102 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,102 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:13,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:13,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:13,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:13,118 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,118 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:13,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:13,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:13,133 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,134 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,134 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,134 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:13,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:13,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:13,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:13,149 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,150 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,150 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:13,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:13,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:13,165 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,165 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,165 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,165 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:13,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:13,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:13,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:13,181 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,182 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,182 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:13,184 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:13,190 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:13,197 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,197 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,197 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,197 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,198 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:13,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:13,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:13,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:13,213 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:13,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:13,222 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:13,229 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,229 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,230 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:13,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:13,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:13,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:13,245 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,246 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,246 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:13,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:13,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:13,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 35]), torch.Size([128, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 36])}
2023-10-07 11:46:13,255 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,256 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 35]), torch.Size([32, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 36])}
2023-10-07 11:46:13,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:13,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:13,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:13,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:13,271 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])))
2023-10-07 11:46:13,271 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])))
2023-10-07 11:46:13,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:13,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:13,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:13,275 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,275 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:13,275 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,275 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:13,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:13,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:13,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:13,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:13,276 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:13,276 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:13,277 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:13,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:13,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:13,279 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,279 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:13,279 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,279 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:13,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:13,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:13,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:13,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:13,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:13,441 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:13,441 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:13,471 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:13,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:13,474 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:13,474 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:13,475 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:13,475 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:13,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:13,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:13,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:13,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:13,477 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:13,477 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:13,478 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:13,478 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:13,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:13,486 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:13,487 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:13,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:13,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:13,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:13,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:13,488 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:13,488 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:13,488 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:13,489 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:13,495 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:13,502 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,502 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,502 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,502 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:13,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:13,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:13,516 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:13,520 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,521 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,521 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:13,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:13,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:13,536 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,537 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,537 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:13,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:13,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:13,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:13,554 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,555 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:13,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:13,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:13,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,571 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,571 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:13,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:13,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:13,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:13,588 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,589 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,589 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:13,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:13,597 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:13,604 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,604 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,605 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,605 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:13,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:13,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:13,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:13,622 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,623 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:13,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:13,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:13,638 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,638 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,638 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:13,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:13,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:13,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:13,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:13,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:13,664 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:13,671 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,672 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,672 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,672 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:13,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:13,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:13,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:13,687 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,689 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,689 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:13,690 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:13,697 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:13,704 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,704 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,704 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,704 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:13,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:13,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:13,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:13,721 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,722 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,722 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:13,723 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:13,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:13,736 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,736 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,736 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,736 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:13,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:13,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:13,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:13,752 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,753 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,753 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:13,754 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:13,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:13,767 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,768 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,768 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,768 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:13,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:13,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:13,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:13,784 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,785 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,785 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:13,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:13,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:13,800 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,800 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:13,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:13,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:13,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:13,816 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,817 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,817 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:13,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:13,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:13,832 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,833 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,833 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,833 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:13,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:13,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:13,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:13,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:13,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:13,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:13,864 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,864 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,864 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,864 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:13,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:13,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:13,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:13,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:13,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:13,891 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:13,899 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,899 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,899 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,900 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:13,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:13,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:13,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:13,916 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,917 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,917 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:13,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:13,926 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:13,932 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,932 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,933 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,933 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:13,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:13,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:13,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:13,948 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,949 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,949 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:13,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:13,957 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:13,964 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,964 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,964 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,964 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:13,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:13,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:13,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:13,980 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:13,981 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:13,981 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:13,983 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:13,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:13,995 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:13,996 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:13,996 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:13,996 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:13,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:14,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:14,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:14,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:14,013 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,014 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,014 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:14,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:14,022 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:14,030 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,030 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,030 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,030 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:14,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:14,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:14,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:14,045 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,046 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,047 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:14,048 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:14,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:14,061 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,061 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,061 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,062 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:14,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:14,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:14,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:14,078 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,078 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,079 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:14,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:14,086 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:14,092 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,093 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,093 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,093 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:14,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:14,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:14,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:14,111 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,111 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,112 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:14,113 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:14,119 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:14,126 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,126 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,126 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:14,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:14,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:14,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:14,141 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,142 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,142 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:14,144 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:14,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:14,156 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,156 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,156 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,157 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:14,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:14,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:14,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:14,173 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,174 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,174 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:14,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:14,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:14,188 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,189 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,189 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,189 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:14,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:14,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:14,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:14,212 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,213 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,214 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:14,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:14,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:14,234 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,235 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:14,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:14,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:14,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:14,252 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,253 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,253 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:14,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:14,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:14,261 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,262 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 36]), torch.Size([128, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 37])}
2023-10-07 11:46:14,262 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,262 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 36]), torch.Size([32, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 37])}
2023-10-07 11:46:14,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:14,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:14,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:14,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:14,278 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])))
2023-10-07 11:46:14,279 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])))
2023-10-07 11:46:14,279 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:14,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:14,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:14,282 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,282 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:14,283 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,283 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:14,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:14,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:14,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:14,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:14,284 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:14,284 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:14,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:14,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:14,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:14,286 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,286 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:14,287 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,287 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:14,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:14,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:14,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:14,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:14,454 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:14,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:14,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:14,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:14,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:14,489 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:14,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:14,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:14,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:14,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:14,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:14,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:14,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:14,492 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:14,492 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:14,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:14,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:14,494 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:14,501 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,501 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:14,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,501 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:14,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:14,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:14,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:14,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:14,502 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:14,503 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:14,503 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:14,503 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:14,509 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:14,516 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,516 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,516 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,516 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:14,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:14,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:14,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:14,535 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,535 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,536 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:14,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:14,543 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:14,550 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,550 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,550 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,550 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:14,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:14,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:14,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:14,568 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,569 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,569 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:14,570 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:14,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:14,582 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,582 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,583 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:14,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:14,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:14,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:14,601 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,602 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,602 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:14,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:14,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:14,616 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,616 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,616 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,616 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:14,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:14,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:14,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:14,634 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:14,636 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:14,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:14,649 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,649 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,650 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,650 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:14,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:14,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:14,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:14,667 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,668 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,668 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:14,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:14,676 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:14,683 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,683 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,683 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,683 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:14,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:14,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:14,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:14,700 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,701 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,701 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:14,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:14,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:14,716 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,716 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,716 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,716 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:14,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:14,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:14,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:14,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,734 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:14,735 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:14,741 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:14,748 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,748 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,748 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,748 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:14,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:14,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:14,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:14,765 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,766 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,766 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:14,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:14,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:14,781 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,781 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:14,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:14,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:14,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:14,798 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,798 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,799 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:14,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:14,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:14,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,813 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,813 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:14,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:14,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:14,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:14,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,831 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:14,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:14,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:14,846 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,846 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,846 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,846 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:14,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:14,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:14,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:14,862 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,863 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,863 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:14,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:14,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:14,878 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,878 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,878 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:14,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:14,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:14,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:14,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,895 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,895 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:14,897 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:14,903 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:14,910 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,910 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,910 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,910 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:14,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:14,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:14,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:14,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:14,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:14,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:14,944 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:14,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:14,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:14,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:14,961 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,961 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:14,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:14,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:14,976 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:14,977 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:14,977 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:14,977 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:14,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:14,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:14,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:14,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:14,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:14,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:14,994 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:14,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:15,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:15,008 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,008 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,008 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,008 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:15,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:15,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:15,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:15,024 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,025 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,025 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:15,026 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:15,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:15,040 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,040 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,040 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,040 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:15,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:15,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:15,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:15,056 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,057 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,057 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:15,059 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:15,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:15,071 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,071 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:15,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:15,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:15,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:15,088 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,089 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:15,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:15,097 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:15,104 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,104 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,104 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,104 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:15,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:15,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:15,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:15,120 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,121 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,121 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:15,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:15,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:15,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,136 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,137 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:15,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:15,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:15,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:15,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:15,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:15,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:15,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:15,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:15,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:15,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:15,184 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,186 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,186 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:15,188 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:15,194 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:15,201 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,201 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,201 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,201 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:15,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:15,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:15,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:15,217 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,218 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,218 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:15,220 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:15,226 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:15,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,233 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,233 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,233 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:15,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:15,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:15,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:15,249 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,250 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,250 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:15,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:15,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:15,260 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,260 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 37]), torch.Size([128, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 38])}
2023-10-07 11:46:15,260 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,260 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 37]), torch.Size([32, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 38])}
2023-10-07 11:46:15,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:15,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:15,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:15,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:15,276 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])))
2023-10-07 11:46:15,277 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])))
2023-10-07 11:46:15,277 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:15,279 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:15,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:15,281 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:15,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,282 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:15,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:15,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:15,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:15,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:15,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:15,283 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:15,283 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:15,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:15,284 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:15,285 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,285 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:15,285 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,285 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:15,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:15,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:15,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:15,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:15,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:15,460 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:15,460 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:15,491 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:15,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:15,494 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:46:15,495 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:15,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:46:15,496 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:15,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0
2023-10-07 11:46:15,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1
2023-10-07 11:46:15,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2
2023-10-07 11:46:15,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3
2023-10-07 11:46:15,498 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:15,498 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:15,499 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-07 11:46:15,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-07 11:46:15,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:15,509 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,509 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:15,509 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,509 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:15,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0
2023-10-07 11:46:15,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1
2023-10-07 11:46:15,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2
2023-10-07 11:46:15,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3
2023-10-07 11:46:15,510 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:15,510 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:15,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-07 11:46:15,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-07 11:46:15,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:15,524 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,524 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-07 11:46:15,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-07 11:46:15,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-07 11:46:15,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-07 11:46:15,541 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,542 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,542 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-07 11:46:15,543 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-07 11:46:15,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:15,557 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,557 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,557 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,557 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-07 11:46:15,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-07 11:46:15,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-07 11:46:15,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-07 11:46:15,573 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,574 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,574 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-07 11:46:15,575 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-07 11:46:15,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:15,588 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,588 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,589 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,589 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-07 11:46:15,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-07 11:46:15,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-07 11:46:15,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-07 11:46:15,607 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,607 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,608 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-07 11:46:15,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-07 11:46:15,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:15,623 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,623 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,624 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,624 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-07 11:46:15,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-07 11:46:15,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-07 11:46:15,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-07 11:46:15,641 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,641 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,641 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-07 11:46:15,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-07 11:46:15,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:15,656 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,656 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,657 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,657 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-07 11:46:15,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-07 11:46:15,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-07 11:46:15,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-07 11:46:15,673 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,674 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,674 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-07 11:46:15,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-07 11:46:15,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:15,689 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,689 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,689 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-07 11:46:15,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-07 11:46:15,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-07 11:46:15,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-07 11:46:15,707 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,708 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,708 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-07 11:46:15,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-07 11:46:15,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:15,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,723 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,723 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,723 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-07 11:46:15,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-07 11:46:15,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-07 11:46:15,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-07 11:46:15,740 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,741 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,741 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-07 11:46:15,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-07 11:46:15,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:15,755 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,756 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-07 11:46:15,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-07 11:46:15,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-07 11:46:15,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-07 11:46:15,773 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,774 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,774 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-07 11:46:15,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-07 11:46:15,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:15,788 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,789 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,789 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,789 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-07 11:46:15,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-07 11:46:15,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-07 11:46:15,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-07 11:46:15,805 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,806 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,806 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-07 11:46:15,808 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-07 11:46:15,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:15,821 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,822 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,822 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-07 11:46:15,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-07 11:46:15,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-07 11:46:15,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-07 11:46:15,841 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,842 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,842 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-07 11:46:15,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-07 11:46:15,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:15,857 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,857 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,857 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,857 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-07 11:46:15,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-07 11:46:15,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-07 11:46:15,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-07 11:46:15,875 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,875 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-07 11:46:15,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-07 11:46:15,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:15,890 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,890 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-07 11:46:15,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-07 11:46:15,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-07 11:46:15,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-07 11:46:15,909 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,910 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-07 11:46:15,911 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-07 11:46:15,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:15,925 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-07 11:46:15,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-07 11:46:15,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-07 11:46:15,940 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-07 11:46:15,946 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,947 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,948 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-07 11:46:15,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-07 11:46:15,956 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:15,964 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:15,964 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:15,964 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:15,964 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:15,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-07 11:46:15,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-07 11:46:15,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-07 11:46:15,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-07 11:46:15,985 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:15,986 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:15,987 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-07 11:46:15,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-07 11:46:15,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:16,005 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,005 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,006 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,006 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-07 11:46:16,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-07 11:46:16,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-07 11:46:16,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-07 11:46:16,024 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,025 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,025 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-07 11:46:16,027 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-07 11:46:16,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:16,041 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,041 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,041 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,041 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-07 11:46:16,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-07 11:46:16,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-07 11:46:16,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-07 11:46:16,058 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,059 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,059 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-07 11:46:16,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-07 11:46:16,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:16,073 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,074 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,074 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,074 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-07 11:46:16,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-07 11:46:16,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-07 11:46:16,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-07 11:46:16,090 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,091 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-07 11:46:16,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-07 11:46:16,099 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:16,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,106 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,106 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-07 11:46:16,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-07 11:46:16,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-07 11:46:16,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-07 11:46:16,124 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,124 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,125 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-07 11:46:16,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-07 11:46:16,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:16,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-07 11:46:16,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-07 11:46:16,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-07 11:46:16,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-07 11:46:16,156 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,157 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,157 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-07 11:46:16,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-07 11:46:16,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:16,171 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,172 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,172 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,172 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-07 11:46:16,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-07 11:46:16,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-07 11:46:16,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-07 11:46:16,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,191 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,191 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-07 11:46:16,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-07 11:46:16,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:16,206 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,206 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,206 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,206 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 0
2023-10-07 11:46:16,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 1
2023-10-07 11:46:16,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 2
2023-10-07 11:46:16,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.20, batch: 3
2023-10-07 11:46:16,222 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,223 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,224 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-07 11:46:16,226 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-07 11:46:16,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:16,238 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,239 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,239 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,239 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 0
2023-10-07 11:46:16,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 1
2023-10-07 11:46:16,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 2
2023-10-07 11:46:16,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.21, batch: 3
2023-10-07 11:46:16,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-07 11:46:16,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-07 11:46:16,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:16,270 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,270 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,271 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,271 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 0
2023-10-07 11:46:16,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 1
2023-10-07 11:46:16,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 2
2023-10-07 11:46:16,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.22, batch: 3
2023-10-07 11:46:16,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,288 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,288 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-07 11:46:16,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-07 11:46:16,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:16,297 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,297 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'layer_past': (torch.Size([128, 64, 38]), torch.Size([128, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([128, 1, 39])}
2023-10-07 11:46:16,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'layer_past': (torch.Size([32, 64, 38]), torch.Size([32, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'head_mask': None, 'use_cache': True, 'output_attentions': False, 'alibi': torch.Size([32, 1, 39])}
2023-10-07 11:46:16,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 0
2023-10-07 11:46:16,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 1
2023-10-07 11:46:16,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 2
2023-10-07 11:46:16,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.23, batch: 3
2023-10-07 11:46:16,315 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([32, 64, 39]), torch.Size([32, 39, 64])))
2023-10-07 11:46:16,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([128, 64, 39]), torch.Size([128, 39, 64])))
2023-10-07 11:46:16,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-07 11:46:16,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-07 11:46:16,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:16,319 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,319 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:16,320 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,320 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:16,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-07 11:46:16,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-07 11:46:16,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-07 11:46:16,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-07 11:46:16,321 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-07 11:46:16,321 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-07 11:46:16,321 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-07 11:46:16,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:46:16,322 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-07 11:46:16,323 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-07 11:46:16,323 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:46:16,323 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-07 11:46:16,323 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:46:16,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:46:16,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:46:16,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:46:16,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:46:16,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 250880])
2023-10-07 11:46:16,501 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 250880])
2023-10-07 11:46:16,502 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:46:16,545 [flexgen_test.py:31 in test_hf_gen] INFO - Who are you? Are you conscious? As an individual, you may be conscious. While conscious may seem like an ambiguous term, we know one thing: we are unconscious. So why should
2023-10-07 11:46:16,545 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - Where is Deutschland? All you need to do to find an affordable car rental will be to search the price range in the Mercedes benz rental website, and the same is true
2023-10-07 11:46:16,546 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - How is Huawei Mate 60 Pro? If you want to buy one ofല്ലെ لاتف سونيك من Huawei، يمكنك استخدام جميع المميزات العديدة على سامسونغ Galaxy F20
2023-10-07 11:46:16,546 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - Who are you? Are you conscious? Doyou think this is crazy? Not possible until you are aware of your past mistakes, and you know how to undo them. You will not regret
2023-10-07 11:46:16,546 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - Where is Deutschland? Is a good question to ask, and the answer depends on your opinion and experience. Most people think the name is good for it is simply because it
2023-10-07 11:46:16,546 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - How is Huawei Mate 60 Pro? | How is Huawei Mate 30 Pro?: Lenovo's Mate Book 2 supports charging Qi but is not a full cell. This phone was the last model
2023-10-07 11:46:16,546 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,546 [flexgen_test.py:31 in test_hf_gen] INFO - Who are you? Are you conscious? Are you aware of your surroundings in terms of body or mind? Do you think about it? Are you aware of this or not? Or just don’t
2023-10-07 11:46:16,547 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,547 [flexgen_test.py:31 in test_hf_gen] INFO - Where is Deutschland? - the first step in your journey to a new level of luxury living is the understanding that you don’t have to invest in a home where your family can
2023-10-07 11:46:16,547 [flexgen_test.py:32 in test_hf_gen] INFO - ----------
2023-10-07 11:46:16,678 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.word_embeddings from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.word_embeddings_layernorm from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.0 from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.1 from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.2 from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.3 from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.4 from flexgen to old.
2023-10-07 11:46:16,679 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.5 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.6 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.7 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.8 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.9 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.10 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.11 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.12 from flexgen to old.
2023-10-07 11:46:16,680 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.13 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.14 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.15 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.16 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.17 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.18 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.19 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.20 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.21 from flexgen to old.
2023-10-07 11:46:16,681 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.22 from flexgen to old.
2023-10-07 11:46:16,682 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.23 from flexgen to old.
2023-10-07 11:46:16,682 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.ln_f from flexgen to old.
2023-10-07 11:46:16,682 [flexgen_forward.py:22 in to_old_forward] DEBUG - lm_head from flexgen to old.
