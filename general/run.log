2023-10-13 01:12:34,583 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpn6wan5lr
2023-10-13 01:12:34,583 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpn6wan5lr/_remote_module_non_scriptable.py
2023-10-13 01:12:35,356 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-13 01:12:35,437 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-13b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-13 01:12:39,230 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-13 01:12:40,335 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-13 01:12:40,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-13 01:12:40,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-13 01:12:40,336 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-13 01:12:42,043 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-13b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-13 01:12:42,309 [model.py:159 in is_on_disk] INFO - [], []
2023-10-13 01:12:42,350 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-13b/resolve/main/config.json HTTP/1.1" 200 0
2023-10-13 01:12:42,770 [model.py:159 in is_on_disk] INFO - [], []
2023-10-13 01:12:42,771 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-13b'
2023-10-13 01:12:42,794 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 12596080640
2023-10-13 01:12:42,794 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 12585584640
2023-10-13 01:12:42,796 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 12585574400
2023-10-13 01:12:42,797 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.18003639 0.81996361], size_todo: 12270935040
2023-10-13 01:12:42,798 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.23378988 0.76621012], size_todo: 11956295680
2023-10-13 01:12:42,799 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.25962997 0.74037003], size_todo: 11641656320
2023-10-13 01:12:42,800 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.27481753 0.72518247], size_todo: 11327016960
2023-10-13 01:12:42,801 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.         0.28481405 0.71518595], size_todo: 11012377600
2023-10-13 01:12:42,802 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.        0.2918925 0.7081075], size_todo: 10697738240
2023-10-13 01:12:42,803 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.29716785 0.70283215], size_todo: 10383098880
2023-10-13 01:12:42,804 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.30125122 0.69874878], size_todo: 10068459520
2023-10-13 01:12:42,805 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.30450561 0.69549439], size_todo: 9753820160
2023-10-13 01:12:42,806 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.30716019 0.69283981], size_todo: 9439180800
2023-10-13 01:12:42,806 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.30936679 0.69063321], size_todo: 9124541440
2023-10-13 01:12:42,807 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.31122999 0.68877001], size_todo: 8809902080
2023-10-13 01:12:42,808 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.12, [0.         0.31282417 0.68717583], size_todo: 8495262720
2023-10-13 01:12:42,809 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.13, [0.         0.31420366 0.68579634], size_todo: 8180623360
2023-10-13 01:12:42,810 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.14, [0.        0.3154091 0.6845909], size_todo: 7865984000
2023-10-13 01:12:42,811 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.15, [0.         0.31152735 0.68847265], size_todo: 7551344640
2023-10-13 01:12:42,812 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.16, [0.         0.31274766 0.68725234], size_todo: 7236705280
2023-10-13 01:12:42,813 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.17, [0.         0.30941891 0.69058109], size_todo: 6922065920
2023-10-13 01:12:42,814 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.18, [0.         0.31062249 0.68937751], size_todo: 6607426560
2023-10-13 01:12:42,815 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.19, [0.         0.30771494 0.69228506], size_todo: 6292787200
2023-10-13 01:12:42,815 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.20, [0.         0.30888634 0.69111366], size_todo: 5978147840
2023-10-13 01:12:42,816 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.21, [0.         0.30630924 0.69369076], size_todo: 5663508480
2023-10-13 01:12:42,817 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.22, [0.         0.30744135 0.69255865], size_todo: 5348869120
2023-10-13 01:12:42,818 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.23, [0.         0.30512979 0.69487021], size_todo: 5034229760
2023-10-13 01:12:42,819 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.24, [0.         0.30621994 0.69378006], size_todo: 4719590400
2023-10-13 01:12:42,820 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.25, [0.         0.30412605 0.69587395], size_todo: 4404951040
2023-10-13 01:12:42,821 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.26, [0.         0.30517395 0.69482605], size_todo: 4090311680
2023-10-13 01:12:42,822 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.27, [0.         0.30326146 0.69673854], size_todo: 3775672320
2023-10-13 01:12:42,823 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.28, [0.         0.30426812 0.69573188], size_todo: 3461032960
2023-10-13 01:12:42,824 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.29, [0.         0.30250897 0.69749103], size_todo: 3146393600
2023-10-13 01:12:42,825 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.30, [0.         0.30347605 0.69652395], size_todo: 2831754240
2023-10-13 01:12:42,825 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.31, [0.         0.30184811 0.69815189], size_todo: 2517114880
2023-10-13 01:12:42,826 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.32, [0.         0.30277757 0.69722243], size_todo: 2202475520
2023-10-13 01:12:42,827 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.33, [0.         0.30126309 0.69873691], size_todo: 1887836160
2023-10-13 01:12:42,828 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.34, [0.         0.30215702 0.69784298], size_todo: 1573196800
2023-10-13 01:12:42,829 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.35, [0.         0.30074158 0.69925842], size_todo: 1258557440
2023-10-13 01:12:42,830 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.36, [0.         0.30160205 0.69839795], size_todo: 943918080
2023-10-13 01:12:42,831 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.37, [0.         0.30027375 0.69972625], size_todo: 629278720
2023-10-13 01:12:42,832 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.38, [0.         0.30110278 0.69889722], size_todo: 314639360
2023-10-13 01:12:42,833 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.39, [0.         0.29985174 0.70014826], size_todo: 0
2023-10-13 01:12:42,833 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.29985174 0.70014826], size_todo: 0
2023-10-13 01:12:42,833 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-13 01:12:42,840 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-13b is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 7.18 GiB (29.99%), Disk Mem 16.76 Gib (70.01%)
2023-10-13 01:12:42,843 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-13 01:12:42,911 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-13 01:12:42,912 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.12 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.13 to test forward
2023-10-13 01:12:42,913 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.14 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.15 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.16 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.17 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.18 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.19 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.20 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.21 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.22 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.23 to test forward
2023-10-13 01:12:42,914 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.24 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.25 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.26 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.27 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.28 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.29 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.30 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.31 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.32 to test forward
2023-10-13 01:12:42,915 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.33 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.34 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.35 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.36 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.37 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.38 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.39 to test forward
2023-10-13 01:12:42,916 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-13 01:12:42,965 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-13b/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-13 01:12:43,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:12:43,210 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:12:43,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:12:43,234 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:12:43,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:12:44,691 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:12:44,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:12:46,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:12:46,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:12:47,483 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:12:47,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:12:48,879 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:12:48,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:12:50,283 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:12:50,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:12:51,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:12:51,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:12:53,043 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:12:53,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:12:54,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:12:54,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:12:55,935 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:12:55,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:12:57,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:12:57,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:12:58,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:12:58,686 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:13:00,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:13:00,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:13:01,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:13:01,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:13:02,827 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:13:02,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:13:04,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:13:04,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:13:05,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:13:05,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:13:06,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:13:06,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:13:08,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:13:08,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:13:09,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:13:09,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:13:11,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:13:11,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:13:12,546 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:13:12,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:13:13,928 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:13:13,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:13:15,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:13:15,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:13:16,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:13:16,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:13:18,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:13:18,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:13:19,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:13:19,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:13:21,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:13:21,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:13:22,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:13:22,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:13:23,864 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:13:23,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:13:25,331 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:13:25,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:13:26,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:13:26,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:13:28,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:13:28,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:13:29,586 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:13:29,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:13:30,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:13:30,973 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:13:32,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:13:32,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:13:33,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:13:33,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:13:35,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:13:35,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:13:36,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:13:36,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:13:37,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:13:37,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:13:39,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:13:39,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:13:39,339 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:13:39,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:13:40,389 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:13:40,448 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-13 01:13:40,448 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-13 01:13:40,456 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-13 01:13:40,456 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-13 01:13:40,457 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.12 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.13 from test to old.
2023-10-13 01:13:40,458 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.14 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.15 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.16 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.17 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.18 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.19 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.20 from test to old.
2023-10-13 01:13:40,459 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.21 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.22 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.23 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.24 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.25 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.26 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.27 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.28 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.29 from test to old.
2023-10-13 01:13:40,460 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.30 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.31 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.32 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.33 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.34 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.35 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.36 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.37 from test to old.
2023-10-13 01:13:40,461 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.38 from test to old.
2023-10-13 01:13:40,462 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.39 from test to old.
2023-10-13 01:13:40,462 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-13 01:13:40,462 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-13 01:13:40,462 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-13 01:13:40,462 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-13 01:13:40,462 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-13 01:13:40,462 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-13 01:13:40,463 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.12 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.13 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.14 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.15 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.16 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.17 to flexgen forward
2023-10-13 01:13:40,464 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.18 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.19 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.20 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.21 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.22 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.23 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.24 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.25 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.26 to flexgen forward
2023-10-13 01:13:40,465 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.27 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.28 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.29 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.30 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.31 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.32 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.33 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.34 to flexgen forward
2023-10-13 01:13:40,466 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.35 to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.36 to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.37 to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.38 to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.layers.39 to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-13 01:13:40,467 [forward.py:127 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-13 01:13:40,508 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-13b/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-13 01:13:40,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:13:40,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:13:40,656 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-13 01:13:40,656 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:13:40,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,664 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,665 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])
2023-10-13 01:13:40,665 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:13:40,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:13:40,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:13:40,670 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-13 01:13:40,671 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:13:40,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,677 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:13:40,677 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])
2023-10-13 01:13:40,677 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:13:40,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:13:40,686 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:13:40,689 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:13:40,690 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:13:45,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:45,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:45,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:45,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:45,614 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:13:45,614 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:13:45,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:13:45,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:13:45,641 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:13:45,641 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:13:50,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:50,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:50,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:50,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:50,531 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:13:50,531 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:13:50,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:13:50,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:13:50,560 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:13:50,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:13:55,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:55,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:55,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:55,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:13:55,457 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:13:55,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:13:55,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:13:55,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:13:55,488 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:13:55,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:00,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:00,144 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:00,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:00,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:00,368 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:00,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:14:00,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:14:00,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:14:00,396 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:00,396 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:04,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:05,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:05,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:05,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:05,285 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:05,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:14:05,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:14:05,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:14:05,326 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:05,327 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:10,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:10,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:10,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:10,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:10,453 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:10,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:14:10,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:14:10,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:14:10,486 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:10,486 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:15,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:15,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:15,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:15,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:15,346 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:15,347 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:14:15,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:14:15,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:14:15,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:15,378 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:19,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:19,992 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:20,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:20,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:20,201 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:20,202 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:14:20,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:14:20,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:14:20,228 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:20,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:24,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:25,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:25,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:25,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:25,273 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:25,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:14:25,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:14:25,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:14:25,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:25,306 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:29,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:29,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:30,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:30,166 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:30,167 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:30,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:14:30,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:14:30,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:14:30,193 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:30,193 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:34,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:34,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:35,007 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:35,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:35,113 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:35,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:14:35,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:14:35,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:14:35,139 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:35,139 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:39,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:39,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:39,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:40,009 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:40,010 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:40,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:14:40,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:14:40,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:14:40,039 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:40,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:44,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:44,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:45,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:45,138 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:45,138 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:45,138 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:14:45,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:14:45,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:14:45,166 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:45,166 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:49,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:49,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:50,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:50,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:50,173 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:50,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:14:50,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:14:50,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:14:50,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:50,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:54,903 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:55,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:55,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:55,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:14:55,229 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:14:55,229 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:14:55,257 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:14:55,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:14:55,264 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:14:55,264 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:14:59,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:00,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:00,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:00,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:00,312 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:00,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:15:00,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:15:00,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:15:00,351 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:00,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:04,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:05,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:05,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:05,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:05,276 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:05,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:15:05,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:15:05,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:15:05,310 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:05,310 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:09,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:09,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:10,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:10,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:10,161 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:10,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:15:10,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:15:10,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:15:10,192 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:10,192 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:14,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:14,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:14,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:15,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:15,063 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:15,063 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:15:15,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:15:15,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:15:15,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:15,100 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:19,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:19,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:19,810 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:19,911 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:19,912 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:19,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:15:19,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:15:19,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:15:19,944 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:19,945 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:24,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:24,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:24,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:24,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:24,780 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:24,781 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:15:24,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:15:24,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:15:24,809 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:24,810 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:29,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:29,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:29,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:29,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:29,736 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:29,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:15:29,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:15:29,763 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:15:29,767 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:29,767 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:34,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:34,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:34,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:34,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:34,623 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:34,623 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:15:34,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:15:34,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:15:34,661 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:34,661 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:39,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:39,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:39,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:39,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:39,472 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:39,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:15:39,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:15:39,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:15:39,507 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:39,507 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:44,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:44,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:44,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:44,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:44,454 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:44,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:15:44,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:15:44,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:15:44,483 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:44,484 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:48,966 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:49,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:49,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:49,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:49,303 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:49,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:15:49,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:15:49,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:15:49,335 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:49,335 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:53,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:53,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:54,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:54,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:54,207 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:54,207 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:15:54,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:15:54,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:15:54,234 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:54,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:15:58,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:58,906 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:59,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:59,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:15:59,152 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:15:59,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:15:59,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:15:59,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:15:59,186 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:15:59,186 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:03,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:03,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:03,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:04,089 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:04,089 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:04,089 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:16:04,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:16:04,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:16:04,122 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:04,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:08,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:08,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:08,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:09,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:09,071 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:09,071 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:16:09,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:16:09,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:16:09,109 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:09,109 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:13,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:13,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:13,906 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:14,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:14,013 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:14,014 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:16:14,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:16:14,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:16:14,048 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:14,048 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:18,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:18,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:18,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:18,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:18,946 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:18,946 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:16:18,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:16:18,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:16:18,979 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:18,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:23,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:23,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:23,910 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:24,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:24,034 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:24,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:16:24,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:16:24,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:16:24,066 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:24,066 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:28,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:28,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:28,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:29,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:29,068 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:29,068 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:16:29,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:16:29,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:16:29,109 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:29,109 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:33,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:33,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:33,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:34,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:34,038 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:34,038 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:16:34,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:16:34,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:16:34,067 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:34,067 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:38,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:38,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:38,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:38,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:38,940 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:38,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:16:38,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:16:38,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:16:38,970 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:38,971 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:43,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:43,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:43,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:43,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:43,865 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:43,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:16:43,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:16:43,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:16:43,900 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:43,900 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:48,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:48,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:48,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:48,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:48,927 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:48,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:16:48,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:16:48,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:16:48,956 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:48,956 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:53,590 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:53,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:53,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:53,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:53,919 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:53,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:16:53,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:16:53,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:16:53,946 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:53,946 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:16:58,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:58,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:58,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:58,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 9, 128])"))
2023-10-13 01:16:58,854 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"))
2023-10-13 01:16:58,855 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:16:58,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:16:58,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:16:58,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:58,881 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:16:58,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:16:58,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:16:58,886 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:16:58,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 5120])
2023-10-13 01:16:58,887 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])
2023-10-13 01:16:58,887 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:16:58,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:16:58,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:16:58,889 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 5120])",)
2023-10-13 01:16:58,889 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:16:59,953 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-13 01:17:00,052 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-13 01:17:00,147 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-13 01:17:00,245 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-13 01:17:00,261 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 9, 50272])
2023-10-13 01:17:00,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:17:00,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:17:00,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:17:00,304 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-13 01:17:00,304 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:17:00,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,306 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,307 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:17:00,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:17:00,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:17:00,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:17:00,311 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-13 01:17:00,312 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:17:00,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:17:00,316 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:17:00,316 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:17:00,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:17:00,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:17:00,326 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:00,327 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:01,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:01,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:01,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:01,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:01,914 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:01,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:17:01,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:17:01,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:17:01,945 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:01,945 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:03,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:03,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:03,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:03,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:03,589 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:03,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:17:03,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:17:03,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:17:03,627 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:03,628 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:04,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:05,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:05,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:05,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:05,220 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:05,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:17:05,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:17:05,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:17:05,253 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:05,254 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:06,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:06,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:06,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:06,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:06,944 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:06,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:17:06,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:17:06,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:17:06,975 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:06,975 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:08,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:08,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:08,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:08,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:08,561 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:08,561 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:17:08,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:17:08,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:17:08,592 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:08,592 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:09,921 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:10,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:10,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:10,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:10,155 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:10,155 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:17:10,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:17:10,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:17:10,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:10,187 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:11,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:11,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:11,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:11,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:11,760 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:11,761 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:17:11,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:17:11,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:17:11,790 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:11,791 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:13,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:13,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:13,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:13,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:13,405 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:13,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:17:13,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:17:13,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:17:13,434 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:13,435 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:14,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:14,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:14,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:14,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:14,999 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:14,999 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:17:15,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:17:15,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:17:15,030 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:15,030 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:16,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:16,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:16,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:16,601 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:16,601 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:16,602 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:17:16,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:17:16,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:17:16,641 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:16,641 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:17,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:18,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:18,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:18,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:18,237 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:18,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:17:18,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:17:18,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:17:18,266 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:18,266 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:19,679 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:19,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:19,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:19,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:19,896 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:19,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:17:19,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:17:19,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:17:19,926 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:19,926 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:21,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:21,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:21,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:21,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:21,491 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:21,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:17:21,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:17:21,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:17:21,522 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:21,522 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:22,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:22,970 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:23,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:23,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:23,122 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:23,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:17:23,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:17:23,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:17:23,152 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:23,152 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:24,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:24,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:24,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:24,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:24,794 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:24,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:17:24,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:17:24,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:17:24,835 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:24,836 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:26,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:26,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:26,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:26,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:26,463 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:26,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:17:26,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:17:26,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:17:26,500 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:26,500 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:27,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:27,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:28,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:28,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:28,117 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:28,117 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:17:28,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:17:28,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:17:28,148 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:28,148 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:29,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:29,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:29,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:29,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:29,751 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:29,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:17:29,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:17:29,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:17:29,794 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:29,794 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:31,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:31,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:31,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:31,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:31,421 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:31,421 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:17:31,454 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:17:31,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:17:31,461 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:31,462 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:32,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:32,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:33,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:33,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:33,121 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:33,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:17:33,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:17:33,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:17:33,163 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:33,163 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:34,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:34,602 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:34,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:34,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:34,757 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:34,758 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:17:34,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:17:34,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:17:34,790 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:34,790 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:36,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:36,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:36,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:36,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:36,549 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:36,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:17:36,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:17:36,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:17:36,586 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:36,587 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:37,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:38,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:38,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:38,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:38,207 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:38,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:17:38,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:17:38,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:17:38,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:38,245 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:39,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:39,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:39,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:39,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:39,922 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:39,922 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:17:39,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:17:39,962 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:17:39,966 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:39,966 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:41,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:41,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:41,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:41,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:41,530 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:41,531 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:17:41,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:17:41,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:17:41,570 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:41,570 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:42,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:43,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:43,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:43,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:43,184 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:43,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:17:43,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:17:43,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:17:43,216 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:43,216 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:44,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:44,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:44,751 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:44,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:44,831 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:44,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:17:44,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:17:44,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:17:44,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:44,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:46,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:46,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:46,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:46,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:46,618 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:46,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:17:46,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:17:46,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:17:46,655 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:46,655 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:48,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:48,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:48,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:48,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:48,247 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:48,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:17:48,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:17:48,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:17:48,279 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:48,279 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:49,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:49,733 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:49,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:49,886 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:49,886 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:49,887 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:17:49,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:17:49,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:17:49,920 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:49,921 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:51,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:51,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:51,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:51,579 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:51,580 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:51,581 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:17:51,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:17:51,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:17:51,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:51,622 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:52,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:53,073 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:53,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:53,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:53,218 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:53,218 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:17:53,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:17:53,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:17:53,252 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:53,252 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:54,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:54,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:54,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:54,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:54,846 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:54,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:17:54,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:17:54,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:17:54,880 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:54,880 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:56,239 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:56,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:56,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:56,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:56,488 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:56,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:17:56,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:17:56,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:17:56,519 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:56,520 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:57,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:58,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:58,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:58,198 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:58,199 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:58,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:17:58,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:17:58,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:17:58,230 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:58,230 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:17:59,637 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:59,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:59,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:59,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:17:59,917 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:17:59,917 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:17:59,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:17:59,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:17:59,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:17:59,962 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:01,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:01,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:01,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:01,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:01,605 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:18:01,606 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:18:01,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:18:01,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:18:01,636 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:01,637 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:03,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:03,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:03,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:03,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:03,271 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:18:03,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:18:03,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:18:03,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:18:03,304 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:03,304 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:04,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:04,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:04,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:04,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:04,902 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:18:04,903 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:18:04,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:18:04,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:18:04,939 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:04,939 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:06,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:06,425 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:06,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:06,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 9, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 10, 128])"))
2023-10-13 01:18:06,616 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"))
2023-10-13 01:18:06,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:18:06,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:18:06,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:18:06,655 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:06,655 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:18:06,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:06,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:06,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:06,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:06,661 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:18:06,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:18:06,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:18:06,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:18:06,662 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:06,662 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:18:07,664 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:18:07,734 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:18:07,805 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:18:07,876 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:18:07,881 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])
2023-10-13 01:18:07,882 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:18:07,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:18:07,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:18:07,930 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-13 01:18:07,930 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:18:07,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,933 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:18:07,933 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:18:07,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:18:07,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:18:07,937 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-13 01:18:07,937 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:18:07,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:18:07,942 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:18:07,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:18:07,945 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:18:07,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:18:07,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:07,952 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:09,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:09,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:09,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:09,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:09,545 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:09,546 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:18:09,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:18:09,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:18:09,582 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:09,582 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:10,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:11,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:11,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:11,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:11,265 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:11,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:18:11,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:18:11,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:18:11,305 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:11,305 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:12,703 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:12,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:12,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:12,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:12,968 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:12,968 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:18:12,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:18:12,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:18:12,999 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:12,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:14,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:14,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:14,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:14,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:14,575 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:14,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:18:14,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:18:14,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:18:14,605 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:14,605 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:15,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:16,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:16,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:16,171 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:16,172 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:16,172 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:18:16,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:18:16,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:18:16,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:16,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:17,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:17,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:17,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:17,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:17,786 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:17,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:18:17,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:18:17,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:18:17,817 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:17,817 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:19,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:19,230 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:19,394 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:19,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:19,471 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:19,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:18:19,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:18:19,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:18:19,499 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:19,500 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:20,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:20,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:20,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:21,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:21,035 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:21,035 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:18:21,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:18:21,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:18:21,065 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:21,065 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:22,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:22,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:22,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:22,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:22,617 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:22,617 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:18:22,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:18:22,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:18:22,650 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:22,650 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:23,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:24,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:24,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:24,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:24,275 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:24,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:18:24,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:18:24,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:18:24,315 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:24,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:25,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:25,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:25,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:25,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:25,995 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:25,996 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:18:26,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:18:26,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:18:26,025 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:26,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:27,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:27,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:27,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:27,584 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:27,585 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:27,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:18:27,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:18:27,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:18:27,617 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:27,617 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:28,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:29,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:29,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:29,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:29,284 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:29,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:18:29,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:18:29,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:18:29,315 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:29,315 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:30,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:30,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:30,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:30,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:30,868 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:30,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:18:30,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:18:30,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:18:30,909 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:30,909 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:32,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:32,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:32,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:32,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:32,488 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:32,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:18:32,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:18:32,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:18:32,533 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:32,533 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:33,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:33,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:34,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:34,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:34,161 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:34,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:18:34,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:18:34,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:18:34,204 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:34,204 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:35,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:35,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:35,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:35,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:35,826 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:35,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:18:35,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:18:35,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:18:35,859 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:35,859 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:37,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:37,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:37,417 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:37,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:37,488 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:37,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:18:37,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:18:37,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:18:37,536 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:37,536 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:38,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:39,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:39,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:39,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:39,192 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:39,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:18:39,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:18:39,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:18:39,223 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:39,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:40,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:40,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:40,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:40,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:40,777 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:40,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:18:40,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:18:40,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:18:40,822 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:40,822 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:42,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:42,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:42,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:42,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:42,375 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:42,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:18:42,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:18:42,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:18:42,407 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:42,407 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:43,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:43,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:43,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:44,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:44,073 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:44,073 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:18:44,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:18:44,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:18:44,116 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:44,117 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:45,493 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:45,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:45,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:45,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:45,713 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:45,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:18:45,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:18:45,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:18:45,745 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:45,745 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:47,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:47,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:47,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:47,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:47,413 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:47,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:18:47,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:18:47,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:18:47,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:47,448 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:48,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:48,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:48,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:49,044 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:49,044 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:49,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:18:49,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:18:49,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:18:49,084 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:49,084 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:50,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:50,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:50,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:50,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:50,615 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:50,615 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:18:50,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:18:50,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:18:50,648 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:50,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:52,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:52,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:52,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:52,230 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:52,230 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:52,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:18:52,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:18:52,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:18:52,263 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:52,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:53,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:53,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:53,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:53,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:53,897 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:53,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:18:53,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:18:53,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:18:53,945 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:53,946 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:55,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:55,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:55,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:55,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:55,523 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:55,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:18:55,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:18:55,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:18:55,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:55,564 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:56,966 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:57,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:57,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:57,198 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:57,199 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:57,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:18:57,225 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:18:57,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:18:57,231 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:57,232 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:18:58,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:58,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:58,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:58,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:18:58,884 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:18:58,885 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:18:58,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:18:58,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:18:58,916 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:18:58,916 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:00,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:00,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:00,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:00,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:00,459 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:00,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:19:00,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:19:00,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:19:00,505 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:00,505 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:01,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:02,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:02,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:02,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:02,221 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:02,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:19:02,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:19:02,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:19:02,252 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:02,252 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:03,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:03,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:03,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:03,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:03,847 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:03,847 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:19:03,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:19:03,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:19:03,894 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:03,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:05,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:05,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:05,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:05,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:05,513 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:05,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:19:05,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:19:05,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:19:05,547 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:05,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:06,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:06,981 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:07,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:07,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:07,147 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:07,147 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:19:07,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:19:07,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:19:07,195 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:07,195 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:08,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:08,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:08,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:08,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:08,902 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:08,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:19:08,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:19:08,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:19:08,943 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:08,944 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:10,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:10,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:10,469 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:10,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:10,549 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:10,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:19:10,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:19:10,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:19:10,584 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:10,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:11,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:11,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:12,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:12,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:12,180 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:12,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:19:12,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:19:12,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:19:12,210 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:12,210 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:13,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:13,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:13,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:13,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 10, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 11, 128])"))
2023-10-13 01:19:13,753 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"))
2023-10-13 01:19:13,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:19:13,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:19:13,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:19:13,781 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:13,781 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:19:13,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:13,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:13,786 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:13,786 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:13,787 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:19:13,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:19:13,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:19:13,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:19:13,788 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:13,788 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:19:14,783 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:19:14,877 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:19:14,961 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:19:15,056 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:19:15,087 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])
2023-10-13 01:19:15,088 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:19:15,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:19:15,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:19:15,131 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-13 01:19:15,131 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:19:15,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,134 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:19:15,134 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:19:15,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:19:15,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:19:15,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-13 01:19:15,139 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:19:15,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:19:15,143 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:19:15,143 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:19:15,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:19:15,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:19:15,152 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:15,153 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:16,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:16,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:16,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:16,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:16,657 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:16,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:19:16,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:19:16,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:19:16,691 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:16,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:18,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:18,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:18,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:18,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:18,288 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:18,288 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:19:18,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:19:18,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:19:18,321 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:18,321 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:19,652 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:19,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:19,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:19,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:19,866 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:19,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:19:19,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:19:19,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:19:19,897 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:19,897 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:21,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:21,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:21,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:21,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:21,539 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:21,539 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:19:21,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:19:21,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:19:21,577 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:21,577 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:22,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:23,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:23,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:23,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:23,161 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:23,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:19:23,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:19:23,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:19:23,198 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:23,198 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:24,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:24,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:24,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:24,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:24,802 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:24,802 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:19:24,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:19:24,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:19:24,836 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:24,836 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:26,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:26,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:26,322 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:26,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:26,403 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:26,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:19:26,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:19:26,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:19:26,448 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:26,448 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:27,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:27,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:27,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:28,012 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:28,012 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:28,013 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:19:28,040 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:19:28,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:19:28,047 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:28,047 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:29,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:29,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:29,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:29,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:29,902 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:29,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:19:29,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:19:29,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:19:29,931 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:29,931 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:31,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:31,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:31,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:31,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:31,626 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:31,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:19:31,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:19:31,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:19:31,666 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:31,667 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:32,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:33,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:33,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:33,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:33,221 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:33,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:19:33,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:19:33,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:19:33,254 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:33,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:34,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:34,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:34,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:34,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:34,823 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:34,823 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:19:34,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:19:34,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:19:34,857 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:34,857 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:36,192 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:36,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:36,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:36,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:36,409 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:36,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:19:36,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:19:36,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:19:36,444 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:36,444 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:37,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:37,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:37,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:38,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:38,028 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:38,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:19:38,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:19:38,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:19:38,062 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:38,062 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:39,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:39,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:39,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:39,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:39,665 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:39,665 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:19:39,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:19:39,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:19:39,699 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:39,699 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:41,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:41,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:41,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:41,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:41,313 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:41,313 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:19:41,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:19:41,350 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:19:41,353 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:41,353 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:42,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:42,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:42,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:42,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:42,983 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:42,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:19:43,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:19:43,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:19:43,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:43,015 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:44,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:44,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:44,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:44,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:44,617 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:44,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:19:44,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:19:44,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:19:44,659 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:44,659 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:46,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:46,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:46,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:46,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:46,332 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:46,332 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:19:46,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:19:46,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:19:46,364 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:46,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:47,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:47,816 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:47,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:47,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:47,983 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:47,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:19:48,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:19:48,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:19:48,020 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:48,020 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:49,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:49,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:49,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:49,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:49,720 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:49,720 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:19:49,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:19:49,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:19:49,758 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:49,758 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:51,144 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:51,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:51,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:51,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:51,426 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:51,426 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:19:51,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:19:51,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:19:51,458 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:51,459 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:52,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:52,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:53,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:53,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:53,151 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:53,151 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:19:53,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:19:53,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:19:53,182 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:53,182 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:54,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:54,637 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:54,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:54,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:54,795 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:54,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:19:54,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:19:54,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:19:54,831 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:54,831 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:56,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:56,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:56,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:56,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:56,396 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:56,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:19:56,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:19:56,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:19:56,437 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:56,437 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:57,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:57,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:57,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:58,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:58,063 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:58,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:19:58,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:19:58,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:19:58,111 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:58,111 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:19:59,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:59,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:59,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:59,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:19:59,796 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:19:59,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:19:59,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:19:59,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:19:59,831 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:19:59,831 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:01,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:01,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:01,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:01,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:01,476 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:01,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:20:01,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:20:01,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:20:01,512 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:01,513 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:03,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:03,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:03,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:03,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:03,374 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:03,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:20:03,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:20:03,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:20:03,419 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:03,419 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:04,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:05,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:05,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:05,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:05,185 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:05,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:20:05,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:20:05,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:20:05,221 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:05,221 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:06,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:06,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:06,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:07,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:07,021 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:07,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:20:07,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:20:07,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:20:07,052 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:07,052 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:08,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:08,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:08,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:08,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:08,668 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:08,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:20:08,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:20:08,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:20:08,702 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:08,703 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:10,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:10,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:10,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:10,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:10,397 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:10,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:20:10,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:20:10,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:20:10,428 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:10,428 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:11,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:11,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:11,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:12,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:12,004 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:12,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:20:12,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:20:12,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:20:12,046 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:12,046 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:13,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:13,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:13,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:13,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:13,620 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:13,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:20:13,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:20:13,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:20:13,652 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:13,652 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:14,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:15,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:15,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:15,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:15,190 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:15,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:20:15,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:20:15,225 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:20:15,228 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:15,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:16,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:16,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:16,910 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:16,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:16,991 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:16,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:20:17,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:20:17,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:20:17,037 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:17,037 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:18,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:18,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:18,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:18,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:18,617 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:18,617 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:20:18,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:20:18,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:20:18,655 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:18,655 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:20,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:20,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:20,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:20,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:20,257 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:20,257 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:20:20,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:20:20,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:20:20,296 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:20,297 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:21,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:21,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:21,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:21,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 11, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 12, 128])"))
2023-10-13 01:20:21,915 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"))
2023-10-13 01:20:21,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:20:21,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:20:21,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:20:21,951 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:21,952 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:20:21,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:21,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:21,956 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:21,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:21,957 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:20:21,958 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:20:21,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:20:21,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:20:21,959 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:21,959 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:20:22,934 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:20:23,011 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:20:23,083 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:20:23,157 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:20:23,161 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])
2023-10-13 01:20:23,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:20:23,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:20:23,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:20:23,204 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-13 01:20:23,204 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:20:23,205 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,205 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,207 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:20:23,207 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:20:23,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:20:23,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:20:23,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-13 01:20:23,212 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:20:23,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,215 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:20:23,216 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:20:23,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:20:23,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:20:23,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:20:23,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:23,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:24,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:24,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:24,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:24,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:24,779 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:24,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:20:24,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:20:24,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:20:24,815 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:24,815 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:26,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:26,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:26,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:26,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:26,401 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:26,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:20:26,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:20:26,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:20:26,433 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:26,434 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:27,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:27,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:27,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:28,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:28,050 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:28,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:20:28,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:20:28,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:20:28,094 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:28,094 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:29,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:29,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:29,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:29,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:29,719 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:29,719 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:20:29,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:20:29,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:20:29,757 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:29,758 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:31,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:31,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:31,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:31,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:31,365 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:31,366 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:20:31,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:20:31,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:20:31,410 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:31,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:32,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:32,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:32,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:32,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:32,957 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:32,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:20:32,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:20:32,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:20:32,993 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:32,993 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:34,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:34,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:34,554 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:34,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:34,623 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:34,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:20:34,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:20:34,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:20:34,656 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:34,656 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:36,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:36,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:36,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:36,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:36,346 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:36,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:20:36,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:20:36,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:20:36,380 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:36,380 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:37,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:37,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:38,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:38,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:38,083 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:38,083 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:20:38,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:20:38,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:20:38,127 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:38,128 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:39,437 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:39,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:39,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:39,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:39,678 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:39,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:20:39,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:20:39,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:20:39,712 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:39,712 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:41,008 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:41,084 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:41,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:41,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:41,237 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:41,238 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:20:41,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:20:41,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:20:41,282 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:41,282 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:42,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:42,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:42,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:42,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:42,828 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:42,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:20:42,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:20:42,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:20:42,861 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:42,861 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:44,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:44,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:44,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:44,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:44,509 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:44,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:20:44,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:20:44,538 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:20:44,542 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:44,542 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:45,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:45,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:46,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:46,114 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:46,114 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:46,114 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:20:46,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:20:46,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:20:46,150 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:46,150 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:47,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:47,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:47,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:47,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:47,692 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:47,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:20:47,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:20:47,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:20:47,725 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:47,725 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:49,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:49,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:49,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:49,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:49,367 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:49,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:20:49,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:20:49,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:20:49,414 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:49,414 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:50,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:50,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:51,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:51,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:51,078 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:51,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:20:51,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:20:51,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:20:51,108 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:51,109 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:52,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:52,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:52,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:52,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:52,660 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:52,660 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:20:52,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:20:52,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:20:52,694 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:52,694 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:54,073 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:54,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:54,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:54,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:54,313 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:54,313 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:20:54,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:20:54,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:20:54,363 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:54,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:55,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:55,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:55,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:55,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:55,945 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:55,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:20:55,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:20:55,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:20:55,981 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:55,981 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:57,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:57,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:57,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:57,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:57,630 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:57,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:20:57,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:20:57,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:20:57,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:57,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:20:58,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:59,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:59,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:59,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:20:59,220 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:20:59,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:20:59,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:20:59,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:20:59,255 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:20:59,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:00,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:00,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:00,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:00,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:00,936 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:00,936 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:21:00,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:21:00,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:21:00,967 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:00,968 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:02,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:02,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:02,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:02,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:02,557 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:02,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.23


2023-10-13 01:21:02,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:21:02,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:21:02,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:02,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:03,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:04,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:04,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:04,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.24, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:04,211 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:04,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.24


2023-10-13 01:21:04,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.25 to cpu
2023-10-13 01:21:04,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:21:04,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:04,245 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:05,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:05,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:06,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:06,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.25, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:06,101 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:06,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.25


2023-10-13 01:21:06,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.26 to cpu
2023-10-13 01:21:06,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:21:06,155 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:06,155 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:07,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:07,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:07,634 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:07,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.26, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:07,720 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:07,720 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.26


2023-10-13 01:21:07,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.27 to cpu
2023-10-13 01:21:07,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:21:07,758 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:07,758 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:09,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:09,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:09,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:09,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.27, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:09,335 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:09,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.27


2023-10-13 01:21:09,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.28 to cpu
2023-10-13 01:21:09,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:21:09,369 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:09,369 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:10,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:10,859 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:10,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:11,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.28, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:11,044 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:11,044 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.28


2023-10-13 01:21:11,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.29 to cpu
2023-10-13 01:21:11,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:21:11,078 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:11,078 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:12,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:12,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:12,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:12,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.29, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:12,673 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:12,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.29


2023-10-13 01:21:12,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.30 to cpu
2023-10-13 01:21:12,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:21:12,710 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:12,710 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:14,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:14,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:14,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:14,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.30, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:14,397 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:14,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.30


2023-10-13 01:21:14,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.31 to cpu
2023-10-13 01:21:14,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:21:14,430 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:14,431 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:15,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:15,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:15,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:16,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.31, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:16,039 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:16,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.31


2023-10-13 01:21:16,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.32 to cpu
2023-10-13 01:21:16,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:21:16,074 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:16,074 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:17,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:17,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:17,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:17,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.32, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:17,633 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:17,633 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.32


2023-10-13 01:21:17,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.33 to cpu
2023-10-13 01:21:17,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:21:17,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:17,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:19,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:19,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:19,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:19,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.33, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:19,282 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:19,282 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.33


2023-10-13 01:21:19,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.34 to cpu
2023-10-13 01:21:19,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:21:19,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:19,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:20,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:20,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:20,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:20,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.34, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:20,924 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:20,925 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.34


2023-10-13 01:21:20,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.35 to cpu
2023-10-13 01:21:20,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:21:20,957 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:20,957 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:22,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:22,367 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:22,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:22,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.35, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:22,526 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:22,526 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.35


2023-10-13 01:21:22,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.36 to cpu
2023-10-13 01:21:22,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:21:22,576 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:22,576 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:23,910 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:23,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:24,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:24,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.36, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:24,159 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:24,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.36


2023-10-13 01:21:24,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.37 to cpu
2023-10-13 01:21:24,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:21:24,192 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:24,192 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:25,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:25,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:25,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:25,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.37, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:25,766 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:25,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.37


2023-10-13 01:21:25,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.38 to cpu
2023-10-13 01:21:25,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:21:25,803 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:25,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:27,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:27,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:27,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:27,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.38, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:27,370 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:27,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.38


2023-10-13 01:21:27,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.39 to cpu
2023-10-13 01:21:27,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:21:27,403 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:27,403 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:28,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:28,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:28,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:29,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.39, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 12, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 13, 128])"))
2023-10-13 01:21:29,013 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"))
2023-10-13 01:21:29,013 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.39


2023-10-13 01:21:29,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-13 01:21:29,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:21:29,046 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:29,046 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:21:29,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:29,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:29,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:29,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:29,053 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:21:29,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-13 01:21:29,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-13 01:21:29,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:21:29,055 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:29,056 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:21:30,056 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:21:30,138 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:21:30,218 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:21:30,299 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-13 01:21:30,303 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([8, 1, 50272])
2023-10-13 01:21:30,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-13 01:21:30,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-13 01:21:30,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:21:30,339 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-13 01:21:30,340 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:21:30,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,342 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:21:30,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-13 01:21:30,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-13 01:21:30,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:21:30,347 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-13 01:21:30,347 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-13 01:21:30,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])
2023-10-13 01:21:30,352 [forward.py:120 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])
2023-10-13 01:21:30,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-13 01:21:30,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-13 01:21:30,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:21:30,362 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:30,362 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:31,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:31,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:31,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:31,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:31,943 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:31,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-13 01:21:31,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-13 01:21:31,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:21:31,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:31,988 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:33,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:33,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:33,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:33,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:33,507 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:33,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-13 01:21:33,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-13 01:21:33,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:21:33,552 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:33,552 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:34,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:34,938 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:35,012 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:35,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:35,085 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:35,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-13 01:21:35,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-13 01:21:35,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:21:35,130 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:35,130 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:36,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:36,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:36,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:36,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:36,877 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:36,877 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-13 01:21:36,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-13 01:21:36,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:21:36,910 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:36,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:38,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:38,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:38,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:38,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:38,498 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:38,499 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-13 01:21:38,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-13 01:21:38,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:21:38,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:38,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:39,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:40,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:40,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:40,178 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:40,179 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:40,179 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-13 01:21:40,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-13 01:21:40,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:21:40,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:40,211 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:41,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:41,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:41,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:41,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:41,740 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:41,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-13 01:21:41,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-13 01:21:41,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:21:41,772 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:41,772 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:43,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:43,165 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:43,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:43,325 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:43,326 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:43,326 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-13 01:21:43,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-13 01:21:43,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:21:43,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:43,370 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:44,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:44,764 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:44,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:44,933 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:44,934 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:44,934 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-13 01:21:44,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-13 01:21:44,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:21:44,967 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:44,967 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:46,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:46,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:46,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:46,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:46,510 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:46,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-13 01:21:46,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-13 01:21:46,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:21:46,555 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:46,555 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:47,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:48,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:48,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:48,252 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:48,253 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:48,253 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-13 01:21:48,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-13 01:21:48,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:21:48,296 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:48,296 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:49,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:49,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:49,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:49,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:49,918 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:49,918 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-13 01:21:49,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.12 to cpu
2023-10-13 01:21:49,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:21:49,964 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:49,964 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:51,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:51,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:51,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:51,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:51,661 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:51,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.12


2023-10-13 01:21:51,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.13 to cpu
2023-10-13 01:21:51,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:21:51,707 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:51,707 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:53,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:53,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:53,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:53,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:53,321 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:53,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.13


2023-10-13 01:21:53,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.14 to cpu
2023-10-13 01:21:53,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:21:53,362 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:53,362 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:54,679 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:54,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:54,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:54,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:54,919 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:54,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.14


2023-10-13 01:21:54,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.15 to cpu
2023-10-13 01:21:54,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:21:54,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:54,961 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:56,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:56,394 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:56,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:56,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:56,540 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:56,541 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.15


2023-10-13 01:21:56,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.16 to cpu
2023-10-13 01:21:56,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:21:56,575 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:56,575 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:57,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:58,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:58,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:58,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:58,178 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:58,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.16


2023-10-13 01:21:58,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.17 to cpu
2023-10-13 01:21:58,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:21:58,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:58,211 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:21:59,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:59,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:59,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:59,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:21:59,790 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:21:59,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.17


2023-10-13 01:21:59,820 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.18 to cpu
2023-10-13 01:21:59,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:21:59,826 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:21:59,826 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:01,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:01,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:01,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:01,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:01,446 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:22:01,446 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.18


2023-10-13 01:22:01,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.19 to cpu
2023-10-13 01:22:01,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:22:01,480 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:22:01,481 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:02,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:02,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:02,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:03,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:03,014 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:22:03,014 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.19


2023-10-13 01:22:03,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.20 to cpu
2023-10-13 01:22:03,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:22:03,049 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:22:03,049 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:04,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:04,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:04,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:04,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:04,609 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:22:04,609 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.20


2023-10-13 01:22:04,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.21 to cpu
2023-10-13 01:22:04,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:22:04,643 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:22:04,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:05,982 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:06,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:06,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:06,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:06,218 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:22:06,218 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.21


2023-10-13 01:22:06,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.22 to cpu
2023-10-13 01:22:06,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:22:06,263 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:22:06,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:07,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:07,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:07,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:07,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 5120])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])", "<class 'torch.Tensor'>: torch.Size([2, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 5120])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 40, 14, 128])"))
2023-10-13 01:22:07,977 [forward.py:120 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 14, 128])"))
2023-10-13 01:22:07,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.22


2023-10-13 01:22:08,006 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.23 to cpu
2023-10-13 01:22:08,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.24 to cpu
2023-10-13 01:22:08,013 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 5120])",)
2023-10-13 01:22:08,013 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 40, 13, 128])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-13 01:22:08,448 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-13 01:22:08,449 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-13 01:22:08,449 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-13 01:22:08,450 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.12 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.13 from flexgen to old.
2023-10-13 01:22:08,451 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.14 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.15 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.16 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.17 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.18 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.19 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.20 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.21 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.22 from flexgen to old.
2023-10-13 01:22:08,452 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.23 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.24 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.25 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.26 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.27 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.28 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.29 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.30 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.31 from flexgen to old.
2023-10-13 01:22:08,453 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.32 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.33 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.34 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.35 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.36 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.37 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.38 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.39 from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-13 01:22:08,454 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
